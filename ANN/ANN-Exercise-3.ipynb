{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-05T14:50:35.365495Z",
     "start_time": "2024-03-05T14:50:35.134625Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks and Learning Machines\n",
    "## Exercise 5 - Convolutional ANN and Transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-05T14:50:48.630517Z",
     "start_time": "2024-03-05T14:50:48.628144Z"
    }
   },
   "outputs": [],
   "source": [
    "epochs = 300\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "The goal of this exercise is for you to get a better understanding of what convolution is, how it is leveraged to increase the usability and performance of neural networks. The exercise will also teach you about transfer learning and the differences between fine-tuning/feature extraction. \n",
    "\n",
    "## Literature\n",
    "This exercise will rely on the following sections in the [course book](https://www.deeplearningbook.org/).\n",
    "\n",
    "- Chapter 9\n",
    "    - Most of it\n",
    "- Chapter 7\n",
    "    - Section 7.4 - Dataset augmentation\n",
    "- Chapter 15\n",
    "    - Section 15.2 - Transfer learning\n",
    "    \n",
    "## Examination\n",
    "Epochs are predefined to be 30. Feel free to increase/decrease this number depending on the hardware that you are working with. Just make sure that you use the same hyperparameters on tasks 2, 3 and 4. **Make sure you have all examination requirements in order before presenting.**\n",
    "\n",
    "### Task 1\n",
    "1. Implementation of same convolution.\n",
    "2. The resulting image using 3 different filters.\n",
    "\n",
    "### Task 2\n",
    "1. The given network trained, validated and tested on the given dataset. Don't forget to make the train/validation/test split of the dataset. This can be achieved programmatically using https://pytorch.org/docs/stable/data.html#torch.utils.data.random_split.\n",
    "2. Some type of regularization should be used. You should understand how the chosen regularization technique works.\n",
    "3. Report the training, validation and test accuracy. (Should beat randomly picking)\n",
    "4. Calculate and plot the multi-class [confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix).\n",
    "5. Add some augmentation techniques which fits well with the data. Does this increase or decrease the validation accuracy?\n",
    "\n",
    "### Task 3\n",
    "1. Fine-tune Resnet18 on the given dataset.\n",
    "2. Report the training, validation and test accuracy. (Should beat randomly picking)\n",
    "3. Calculate and plot the multi-class [confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix).\n",
    "4. Add some augmentation techniques which fits well with the data. Does this increase or decrease the validation accuracy?\n",
    "\n",
    "### Task 4\n",
    "1. Use Resnet18 as a feature extractor on the dataset.\n",
    "2. Report the training, validation and test accuracy. (Should beat randomly picking)\n",
    "3. Calculate and plot the multi-class [confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix).\n",
    "4. Add some augmentation techniques which fits well with the data. Does this increase or decrease the validation accuracy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolution in Neural Networks\n",
    "A convolutional neural network, CNN for short, is a type of ANN that consists of at least one convolutional layer. CNN's are often used where the input size may vary such as when we are dealing with image input. The architecture of CNNs was inspired by how the visual cortex functions in our brain.\n",
    "\n",
    "## Task 1: Implement convolution\n",
    "Implement 2d same convolution without using a built-in convolution function. This should function as described in [this blog post](https://jcbgamboa.github.io/2017/08/12/what-are-convolutions/). One of the great strengths of convolution is that it functions on any sized image, hence it is important that your implementation also does. Same convolution means that the dimensions of the output are the same as the dimensions of the input. This is achieved by padding the input.\n",
    "\n",
    "Once you have implemented a function that performs 2d convolution, use that to perform convolution over all channels in this image. Show the result using 3 different filters.\n",
    "\n",
    "To find the padding needed to get the input to be the same space as the output you can use the formula:\n",
    "\n",
    "$$ n_{out} = \\left \\lfloor\\frac{n_{in}+2p-k}{s} \\right \\rfloor+1 $$\n",
    "\n",
    "where $n_{out}$ is the number of output features, $n_{in}$ is the number of input features, $k$ is the kernel size, $p$ is the padding size and $s$ is the stride size. You can assume that the stride is always 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = [12, 30]\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "\n",
    "# Implement same convolution in the function below (kernel is a 2d numpy array an example of which can be found in the test)\n",
    "def conv(image, kernel, strides=1):\n",
    "    krh = kernel.shape[0] // 2\n",
    "    kch = kernel.shape[1] // 2\n",
    "    acc = np.zeros_like(image)\n",
    "    for ir in range(image.shape[0]):\n",
    "        for ic in range(image.shape[1]):\n",
    "            for kr in range(kernel.shape[0]):\n",
    "                irs = ir - krh + kr\n",
    "                if irs < 0 or irs >= image.shape[0]:\n",
    "                    continue\n",
    "                for kc in range(kernel.shape[1]):\n",
    "                    ics = ic - kch + kc\n",
    "                    if ics < 0 or ics >= image.shape[1]:\n",
    "                        continue\n",
    "                    v = kernel[kr, kc]\n",
    "                    acc[ir, ic] += v * image[irs, ic - kch + kc]\n",
    "    return acc\n",
    "\n",
    "# Our test, don't edit\n",
    "inp = np.array([[1,1,1,1],[1,1,2,1],[1,-3,-4,1],[1,1,1,1]])\n",
    "kernel = np.array([[0,1,0],[1,2,1],[0,1,0]]) # This is the second input of conv()\n",
    "\n",
    "# If all are TRUE the convolution is implemented correctly\n",
    "ans = np.array([[4, 5, 6, 4], [5, 3, 3, 6], [1, -7, -7, 0], [4, 1, 0, 4]])\n",
    "print(conv(inp, kernel) == ans)\n",
    "\n",
    "f, axarr = plt.subplots(4,1)\n",
    "\n",
    "# How to load images using opencv\n",
    "image_path = \"Vantar.jpg\" # add your file path here\n",
    "image = cv2.imread(image_path)\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) # gray scale so we dont have to deal with more than 1 channel\n",
    "\n",
    "# Define your 3 kernels\n",
    "kernel_1 = np.array([[1, 0, -1],  # Sobel edge detect\n",
    "                     [2, 0, -2],\n",
    "                     [1, 0, -1]])\n",
    "kernel_2 = np.array([[1, 2, 1],\n",
    "                     [0, 0, 0],\n",
    "                     [-1,-2,-1]])\n",
    "kernel_3 = np.array([[0, 1, 0],  # Laplacian 2nd derivate\n",
    "                     [1,-4, 1],\n",
    "                     [0, 1, 0]])\n",
    "\n",
    "# Perform the convolution (might take a couple of seconds depending on the implementation)\n",
    "output1 = conv(image, kernel_1)\n",
    "output2 = conv(image, kernel_2)\n",
    "output3 = conv(image, kernel_3)\n",
    "\n",
    "# plot the loaded image and the 3 convoluted images\n",
    "axarr[0].imshow(image, cmap=\"gray\")\n",
    "axarr[1].imshow(output1, cmap=\"gray\")\n",
    "axarr[2].imshow(output2, cmap=\"gray\")\n",
    "axarr[3].imshow(output3, cmap=\"gray\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computer Vision\n",
    "Computer vision (CV) is a task within the computer science field that aim is to extract high-level information from static images or video. Such high-level information can be, but is not limited to:\n",
    "* Object detection - Detect and classify objects within input images\n",
    "* Anomaly detection - Detect anomalies in the input images\n",
    "* Semantic segmentation - Classify each pixel in the input image into different classes\n",
    "* Object recognition - Classifying an entire image depending on what it contains\n",
    "\n",
    "CV has been studied for multiple decades where early solutions used handwritten feature extractors to extract information from the input. However, with the increase of computing power together with the rise of deep learning algorithms, the main method used to solve CV problems is convolutional neural networks.\n",
    "\n",
    "In this exercise, we will be taking a closer look at object recognition by first using a randomly initialized network and then utilizing transfer learning. **The dataset we will use for this exercise can be downloaded on canvas**. It is a subset of [this dataset](http://www.vision.caltech.edu/Image_Datasets/Caltech101/). Remember to split the data into separate training, validation and test set.\n",
    "\n",
    "## Task 2: Implement the missing code and train it on the given dataset.\n",
    "For task 2, implement the missing parts of the code below. The code should correctly train, validate and test the model. There are some comments guiding you through the process, however if something is unclear try to leverage the documentation for pytorch found [here](https://pytorch.org/docs/stable/index.html). You should also add some type of regularization into your model.\n",
    "\n",
    "Remember to check the examination requirements in the start of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-05T14:51:08.829435Z",
     "start_time": "2024-03-05T14:51:07.292283Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution device cuda:0\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "import torchvision.models as models\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available()\n",
    "                      else \"cpu\")\n",
    "print(\"Execution device\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-05T14:56:55.913048Z",
     "start_time": "2024-03-05T14:56:55.902617Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset ImageFolder\n",
      "    Number of datapoints: 84\n",
      "    Root location: 101_ObjectCategories_2classes\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               ToTensor()\n",
      "           )\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f2c437aa070>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(16 * 26 * 26, 120)\n",
    "        self.fc2 = nn.Linear(120, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Implement a train model function so you can re_use it in task 3 and 4.\n",
    "# Should return the best performing model after training\n",
    "def train_model(model, criterion, optimizer, train_loader, val_loader, num_epochs):\n",
    "    best_loss = float('inf')\n",
    "    best_model = copy.deepcopy(model)\n",
    "    for epoch in range(num_epochs):\n",
    "        # Validation\n",
    "        validation_loss = 0\n",
    "        for image, target in val_loader:\n",
    "            loss = criterion(model.forward(image), target)\n",
    "            validation_loss += loss\n",
    "\n",
    "        if validation_loss < best_loss:\n",
    "            best_loss = validation_loss\n",
    "            best_model = copy.deepcopy(model)\n",
    "            print(f\"Saving model... loss: {best_loss/len(val_loader):.4f}\")\n",
    "\n",
    "        # Training\n",
    "        training_loss = 0\n",
    "        for image, target in train_loader:\n",
    "            estimate = model.forward(image)\n",
    "            loss = criterion(estimate, target)\n",
    "            training_loss += loss\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    return best_model, best_loss\n",
    "\n",
    "def evaluate_model(model, criterion, eval_loader):\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    for image, target in eval_loader:\n",
    "        outv = model.forward(image)\n",
    "        guess = torch.argmax(outv)\n",
    "        correct += 1 if (guess == target[0]) else 0\n",
    "        print(guess, target[0])\n",
    "        loss = criterion(outv, target)\n",
    "        total_loss += loss\n",
    "    return correct / len(eval_loader), total_loss / len(eval_loader)\n",
    "\n",
    "# Hyperparams. Set these to reasonable values\n",
    "BATCH_SIZE = 1  # TODO\n",
    "LEARNING_RATE = 1e-3\n",
    "\n",
    "# Train augmentations\n",
    "transformations = transforms.Compose([\n",
    "    # Add training augmentations here, remember: we do not want to transform the validation images.\n",
    "    # For information about augmentation see: https://pytorch.org/vision/stable/transforms.html\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Load the full dataset, perform the training/validation/test split and then load the subsets into dataloaders.\n",
    "# Remember that the training images should be augmentated.\n",
    "DATA_DIR = \"101_ObjectCategories_2classes\" # Path to dataset\n",
    "\n",
    "labeled_images = datasets.ImageFolder(root=DATA_DIR, transform=transformations)\n",
    "print(labeled_images)\n",
    "\n",
    "# Split dataset; training, validation, test\n",
    "torch.manual_seed(1)\n",
    "parts = torch.utils.data.random_split(labeled_images, [0.64, 0.16, 0.20])\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-05T14:59:26.274451Z",
     "start_time": "2024-03-05T14:58:17.529772Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train <class 'torch.utils.data.dataloader.DataLoader'>\n",
      "Saving model... loss: 0.7317\n",
      "Saving model... loss: 0.6782\n",
      "Saving model... loss: 0.6325\n",
      "Saving model... loss: 0.6014\n",
      "Saving model... loss: 0.5251\n",
      "Saving model... loss: 0.5247\n",
      "Saving model... loss: 0.5244\n",
      "Saving model... loss: 0.4836\n",
      "Saving model... loss: 0.4748\n",
      "tensor(0) tensor(0)\n",
      "tensor(0) tensor(0)\n",
      "tensor(1) tensor(1)\n",
      "tensor(1) tensor(1)\n",
      "tensor(1) tensor(1)\n",
      "tensor(1) tensor(1)\n",
      "tensor(0) tensor(0)\n",
      "tensor(1) tensor(0)\n",
      "tensor(1) tensor(1)\n",
      "tensor(1) tensor(0)\n",
      "tensor(1) tensor(1)\n",
      "tensor(0) tensor(0)\n",
      "tensor(1) tensor(1)\n",
      "tensor(1) tensor(0)\n",
      "tensor(1) tensor(0)\n",
      "tensor(0) tensor(0)\n",
      "0.75 tensor(0.9493, grad_fn=<DivBackward0>)\n",
      "Test 75.0%, loss=0.9493464231491089\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(parts[0], batch_size=BATCH_SIZE, shuffle=True)\n",
    "validate_loader = DataLoader(parts[1], batch_size=BATCH_SIZE, shuffle=False)\n",
    "print(\"Train\", type(train_loader))\n",
    "\n",
    "test_loader = DataLoader(parts[2], batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Load our network\n",
    "model = Net()\n",
    "\n",
    "# Define our loss function\n",
    "_m = nn.Sigmoid()\n",
    "_loss = torch.nn.BCELoss()\n",
    "def criterion(y_est, y_true):\n",
    "    return _loss(_m(y_est),\n",
    "                 nn.functional.one_hot(y_true, num_classes=2).type(torch.Tensor)) # What function to use to calculate the loss given the prediction and labels\n",
    "\n",
    "# Define our optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE, weight_decay=5e-5) # Function for updating the parameters of the network based on loss\n",
    "\n",
    "# Train the model\n",
    "trained_model, loss = train_model(model, criterion, optimizer, train_loader, validate_loader, epochs)\n",
    "\n",
    "# Test the model\n",
    "correctness, test_loss = evaluate_model(trained_model, criterion, test_loader)\n",
    "print(correctness, test_loss)\n",
    "print(f\"Test {100 * float(correctness):.1f}%, loss={test_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer learning\n",
    "Transfer learning refers to the practice to use a model which has already been pre-trained on a large dataset to be able to solve task $T_1$, replace the output layer or a few of the upper layers within this model and retrain the model on a smaller dataset to be able to solve task $T_2$. Formally this can be described as the following:\n",
    "\n",
    "__Def 1:__ Let $D_s$ be the source domain and $T_s$ be the corresponding source task. Let $D_t$ be the target domain and $T_t$ be the corresponding target task. Let $f_t$ be the predictive function for $T_s$. Thus transfer learning aims to improve the learning of $f_t$ in $D_t$ using the already learned knowledge in $D_s$ and $T_s$ where $D_s \\neq D_t$ and $T_s \\neq T_t$.\n",
    "\n",
    "The benefit from using transfer learning is that we can train an accurate computer vision model with relatively small amounts of data and computing resources compared to the costly pretraining process of the full convolutional neural network (a few days using multiple GPUs). \n",
    "\n",
    "## Fine-tuning and Feature extraction\n",
    "There are two main ideas when it comes to transfer learning, fine-tuning and feature extraction. When using fine-tuning we allow all weights to be changed during the training phase. However, when we use the pre-trained model as a feature extractor we instead freeze earlier layers of the model, which means that the weights in those layers will not be updated during the training phase and we only update the weights in the upper layers that we have replaced. \n",
    "\n",
    "This works because low-level information extracted from the input image is universal between tasks, examples of such information is edge detection, shape detection and pattern detection. This is what the early layers are optimized to do, where later layers extract more abstract features relevant for the task. \n",
    "\n",
    "Most of the pre-trained models in PyTorch are trained on [ImageNet](http://www.image-net.org/). \n",
    "\n",
    "In this exercise, we use ResNet18 as our model. You should make yourself familiar with the Resnet18 architecture using, for example, [the paper](https://arxiv.org/abs/1512.03385).\n",
    "\n",
    "## Task 3: Fine-tuning\n",
    "In task 3 you should fine-tune Resnet18 to the small dataset which is provided above. Some code has been given to you. Remember to re-use functions (such as trained_model) from task 2 to decrease the implementation time.\n",
    "\n",
    "Remember to check the examination requirements at the start of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tune a model to the dataset\n",
    "# We use resnet18 as the model.\n",
    "model_ft = models.resnet18(pretrained=True)\n",
    "\n",
    "# Do the things required for fine-tuning before training the model\n",
    "\n",
    "# Train the model\n",
    "trained_model_ft = train_model(model_ft, criterion_ft, optimizer_ft, train_loader, val_loader, epochs)\n",
    "\n",
    "# Test the model\n",
    "tested_model = ...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Feature extraction\n",
    "In task 4, you should use Resnet18 as a feature extractor. Similarly to task 3, some code has been provided. Remember to re-use as much code as you can. \n",
    "\n",
    "Once again, check the examination requirements so you don't forget to implement some required functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a predefined model as a feature extractor\n",
    "\n",
    "# We use resnet18 as the model.\n",
    "model_fe = models.resnet18(pretrained=True)\n",
    "\n",
    "# Do the things required for fine-tuning before training the model\n",
    "\n",
    "# Train the model\n",
    "trained_model_fe = train_model(model_fe, criterion_fe, optimizer_fe, train_loader, val_loader, epochs)\n",
    "\n",
    "# Test the model\n",
    "tested_model = ...."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
