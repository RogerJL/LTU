{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/RogerJL/LTU/blob/main/ANN/ANN_Exercise_1_jax_works.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zqnwnWXQakfJ"
   },
   "source": [
    "# D7046E Exercise 1 (ANN1)\n",
    "\n",
    "This exercise has three taks where you will deepen your understanding of how artificial neural networks (ANNs) are implemented and trained. First, you will represent digits on an eight-segment display as vectors and hard-code perceptrons that classifies the digits. The purpose of this task is to better understand the basic computational units in ANNs and how inputs can be represented as feature vectors. Secondly, you will implement and train neural networks using [pytorch](https://pytorch.org/) on the seven-segment display data and the [MNIST](http://yann.lecun.com/exdb/mnist/) dataset which you are familiar with from Exercise 0. Finally, you will implement a neural network including the forward (inference) pass and the backward (learning) pass from scratch using numpy. After completing these steps you will know the central building blocks of ANNs.\n",
    "\n",
    "## Literature\n",
    "Before starting with the implementation you should familiarize yourself with two additional chapters in the [deep learning book](https://www.deeplearningbook.org/). This will help you understand the theory behind neural networks and what mathematical formulas are important for the task. The lectures has touched on most of these concepts. Below is a list of recommended sections from the book. If you feel familiar with the contents of these sections, feel free to skip it.\n",
    "\n",
    "* Chapter 6 - Deep feedforward networks\n",
    "    - Section 6.0 - Discusses what do we mean by feedfoward networks and terminology such as input layer, output layer and hidden layer.\n",
    "    - Section 6.2 - Discusses what gradient based learning is and what cost functions are.\n",
    "    - Section 6.5 - Explains back propagation. Important here are the formulas 6.49 - 6.52.\n",
    "* Chapter 8 - Optimization for Training Deep Models\n",
    "    - Section 8.1.3 - Presents differences between batch (deterministic) and mini-batch (stochastic) algorithms.\n",
    "    \n",
    "## Libraries\n",
    "\n",
    "Before starting with the implementations you need to import the following libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-02T15:34:41.013034800Z",
     "start_time": "2024-02-02T15:34:38.495033100Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "26MnPsL9akfK",
    "outputId": "65fc1a13-a7b6-4436-83e7-f08b492c085c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JAX devices [gpu(id=0)]\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "print(\"JAX devices\", jax.devices())\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "import copy\n",
    "import itertools\n",
    "\n",
    "\n",
    "# Download the dataset and create the dataloaders\n",
    "mnist_train = datasets.MNIST(\"./\", train=True, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "# Split training dataset\n",
    "parts = torch.utils.data.random_split(mnist_train, [0.8, 0.2])\n",
    "\n",
    "mnist_test = datasets.MNIST(\"./\", train=False, download=True, transform=transforms.ToTensor())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-02T15:34:41.024488200Z",
     "start_time": "2024-02-02T15:34:41.016034400Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F8A0h1Gs5HRY",
    "outputId": "1436f03e-757b-4eca-962a-fd11cd2d1099"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution device cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available()\n",
    "                      else \"cpu\")\n",
    "print(\"Execution device\", device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fwtudc8DakfL"
   },
   "source": [
    "## Task 1\n",
    "\n",
    "Classification of seven-segment display numbers with perceptrons. A [seven-segment display](https://en.wikipedia.org/wiki/Seven-segment_display) can be used to display digits by turning the different segments (A,B,C,D,E,F, G) on or off. Your task is to design ten different perceptrons (which together make a single-layer neural network with 10 outputs) that recognizes the ten different digits (0,1,2,3, ... ,9) represented by a seven-segment display. The input to each perceptron will be a seven-dimensional vector {A,B,C,D,E,F,G} where A is 1 if segment A is turned on and 0 otherwise (and the same for all the other segments).\n",
    "\n",
    "![Seven Segment Display](https://upload.wikimedia.org/wikipedia/commons/thumb/e/ed/7_Segment_Display_with_Labeled_Segments.svg/225px-7_Segment_Display_with_Labeled_Segments.svg.png)\n",
    "\n",
    "This means that for each digit you should create a vector representing the digit, as well as a perceptron that selectively gives a high output for that particular digit and a low output for other digits. For each digit (0 to 9) the corresponding perceptron shoud have the greatest output value, indicating that it can correctly classify the corresponding digit/vector and thereby distinguish that digit from other digits.\n",
    "\n",
    "For example, the digit 2 corresponds to the vector {1,1,0,1,1,0,1}. If the perceptron at index 2 (the third pereptron since we have a perceptron for 0 as well) gives the greatest output for this vector then this perceptron functions as desired. Similarly, other perceptrons should give the highest output for their corresponding vector.\n",
    "\n",
    "For example, the digit zero is represented as segments A-F being on and segment G being off. The zero perceptron should reasonably have positive activation for A-F, say at a value of 1, and negative activation for G, say -1.\n",
    "\n",
    "Why negative?\n",
    "\n",
    "If we instead set the activation for G to 0, then there is no distinction between the digits zero and eight. Thus, our perceptron for zero would have the same output regardless of whether the input is zero or eight. You can address this with biases, so that the output for our eight-perceptron is higher than the zero-perceptron for input eight, and vice versa, but it makes balancing weights and biases much more difficult.\n",
    "\n",
    "A good starting point is to consider which activations should be positive or negative, implement this, then check the output for all perceptrons for each digit and then contemplate the bias.\n",
    "\n",
    "\n",
    "### Task 1.1\n",
    "\n",
    "In this task you should use numpy rather than PyTorch to implement and understand the elementary arithmetic operations involved. After completing this exercise you should understand how an artificial neural network unit (like the perceptron) produces one scalar output from multiple input values, and how the trainable weights and biases determine the relation between input and output values of a neural network unit and layer.\n",
    "\n",
    "Complete the input vectors, the weight vectors, and biases. Then update the prediction calculation (forward pass) to include bias in the calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-02T15:34:41.032488Z",
     "start_time": "2024-02-02T15:34:41.025488300Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QR8eEBuRakfL",
    "outputId": "54374827-8b17-48ac-f4d2-4db883373719"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Digit 5 corresponds to the vector [1 0 1 1 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "## First we need to define all the vectors corresponding to the various digits and add them to a list for easy access\n",
    "# Please finish the list of digit vectors asdasdas\n",
    "x = [\n",
    "    numpy.array([1,1,1,1,1,1,0]), # 0\n",
    "    numpy.array([0,1,1,0,0,0,0]), # 1\n",
    "    numpy.array([1,1,0,1,1,0,1]), # 2\n",
    "    numpy.array([1,1,1,1,0,0,1]), # 3\n",
    "    numpy.array([0,1,1,0,0,1,1]), # 4\n",
    "    numpy.array([1,0,1,1,0,1,1]), # 5\n",
    "    numpy.array([1,0,1,1,1,1,1]), # 6\n",
    "    numpy.array([1,1,1,0,0,0,0]), # 7\n",
    "    numpy.array([1,1,1,1,1,1,1]), # 8\n",
    "    numpy.array([1,1,1,1,0,1,1]), # 9\n",
    "]\n",
    "\n",
    "# TEACHER SOLUTION\n",
    "x = [\n",
    "    numpy.array([1,1,1,1,1,1,0]), # 0\n",
    "    numpy.array([0,1,1,0,0,0,0]), # 1\n",
    "    numpy.array([1,1,0,1,1,0,1]), # 2\n",
    "    numpy.array([1,1,1,1,0,0,1]), # 3\n",
    "    numpy.array([0,1,1,0,0,1,1]), # 4\n",
    "    numpy.array([1,0,1,1,0,1,1]), # 5\n",
    "    numpy.array([1,0,1,1,1,1,1]), # 6\n",
    "    numpy.array([1,1,1,0,0,0,0]), # 7\n",
    "    numpy.array([1,1,1,1,1,1,1]), # 8\n",
    "    numpy.array([1,1,1,1,0,1,1]), # 9\n",
    "]\n",
    "\n",
    "# And we print one of the vectors to show you how to get a specific vector\n",
    "print(f'Digit 5 corresponds to the vector {x[5]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-02T15:34:41.060492400Z",
     "start_time": "2024-02-02T15:34:41.032488Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6QpWjrd4akfM",
    "outputId": "230c91b4-8955-4b7c-d305-f75d2d577492"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-5, -1, -4, -4, -3, -4, -5, -2, -6, -5]\n"
     ]
    }
   ],
   "source": [
    "## Second we need to create ten perceptron with weights and biases\n",
    "# You also need to figure out which weights and biases to use for each perceptron\n",
    "# We've already created some of the perceptrons for you, but you need to create the rest\n",
    "# While we're using integers for our weight you can use floating point numbers (real numbers) as well if you want\n",
    "\n",
    "weights = [\n",
    "    numpy.array([1,1,1,1,1,1,-1]), # 0\n",
    "    numpy.array([-1,1,1,-1,-1,-1,-1]), # 1\n",
    "    numpy.array([1,1,-1,1,1,-1,1]), # 2\n",
    "    numpy.array([1,1,1,1,-1,-1,1]), # 3\n",
    "    numpy.array([-1,1,1,-1,-1,1,1]), # 4\n",
    "    numpy.array([1,-1,1,1,-1,1,1]), # 5\n",
    "    numpy.array([1,-1,1,1,1,1,1]), # 6\n",
    "    numpy.array([1,1,1,-1,-1,-1,-1]), # 7\n",
    "    numpy.array([1,1,1,1,1,1,1]), # 8\n",
    "    numpy.array([1,1,1,1,-1,1,1]), # 9\n",
    "\n",
    "]\n",
    "\n",
    "biases = [-sum(s) + 1 for s in x]\n",
    "print(biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-02T15:34:41.061489200Z",
     "start_time": "2024-02-02T15:34:41.046489800Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pJ-FCDmSakfM",
    "outputId": "9bc125fd-b70c-4d05-aadb-cdac9c944ff9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Digit 0 was predicted to be 0\n",
      "Outputs for all perceptrons for the digit 0: [1, -3, -2, -2, -3, -2, -1, -2, 0, -1]\n",
      "\n",
      "Digit 1 was predicted to be 1\n",
      "Outputs for all perceptrons for the digit 1: [-3, 1, -4, -2, -1, -4, -5, 0, -4, -3]\n",
      "\n",
      "Digit 2 was predicted to be 2\n",
      "Outputs for all perceptrons for the digit 2: [-2, -4, 1, -1, -4, -3, -2, -3, -1, -2]\n",
      "\n",
      "Digit 3 was predicted to be 3\n",
      "Outputs for all perceptrons for the digit 3: [-2, -2, -1, 1, -2, -1, -2, -1, -1, 0]\n",
      "\n",
      "Digit 4 was predicted to be 4\n",
      "Outputs for all perceptrons for the digit 4: [-3, -1, -4, -2, 1, -2, -3, -2, -2, -1]\n",
      "\n",
      "Digit 5 was predicted to be 5\n",
      "Outputs for all perceptrons for the digit 5: [-2, -4, -3, -1, -2, 1, 0, -3, -1, 0]\n",
      "\n",
      "Digit 6 was predicted to be 6\n",
      "Outputs for all perceptrons for the digit 6: [-1, -5, -2, -2, -3, 0, 1, -4, 0, -1]\n",
      "\n",
      "Digit 7 was predicted to be 7\n",
      "Outputs for all perceptrons for the digit 7: [-2, 0, -3, -1, -2, -3, -4, 1, -3, -2]\n",
      "\n",
      "Digit 8 was predicted to be 8\n",
      "Outputs for all perceptrons for the digit 8: [0, -4, -1, -1, -2, -1, 0, -3, 1, 0]\n",
      "\n",
      "Digit 9 was predicted to be 9\n",
      "Outputs for all perceptrons for the digit 9: [-1, -3, -2, 0, -1, 0, -1, -2, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "## Finally let's evaluate the output of the perceptrons.\n",
    "# The line computing the output of each perceptron is not using bias, so you need to add that as well.\n",
    "# If the perceptrons dont give the correct output then go back and edit the weights and biases in the\n",
    "# previous cell (or check that the vectors are implemented correctly) until the outputs discriminate\n",
    "# correctly between different digits. Remember to run a cell again (CONTROL + ENTER) if you update it.\n",
    "\n",
    "debug = True # Set this to True if the predictions are wrong to get a more detailed output\n",
    "\n",
    "for digit in range(10): # For each digit between 0 and 9 (range(n) gives a range (almost a list) of each number between 0 and n (excluding n)\n",
    "    vector = x[digit] # Get the correct vector representation of the digit\n",
    "\n",
    "    outputs = [] # Create an empty list to put the perceptrons' outputs in\n",
    "    for w, b in zip(weights, biases): # For each weight and bias in the lists (zip takes two lists [x1,x2,...] [y1,y2,...] and makes a new list [(x1,y1),(x2,y2),...])\n",
    "\n",
    "        # CHANGE THIS LINE TO ADD BIASES AS WELL\n",
    "        output = w.dot(vector) + b  # Calculating the output of the perceptron with weight w and bias b\n",
    "\n",
    "        outputs.append(output) # Adding the output to the list of outputs\n",
    "    prediction = outputs.index(max(outputs)) # Get prediction by taking the index of the output value with maximum input\n",
    "\n",
    "    print(f'Digit {digit} was predicted to be {prediction}') # This is an f-string with notation f'text {variable1} more text {variable2}'\n",
    "\n",
    "    if debug: # If debug is True\n",
    "        print(f'Outputs for all perceptrons for the digit {digit}: {outputs}')\n",
    "        print() # Add a newline for formating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ii7YdeVQakfN"
   },
   "source": [
    "### Task 1.2\n",
    "\n",
    "Next you will train a classifier for the seven-segment display digits using PyTorch, much like you did in Exercise 0 with another dataset. The goal is to learn the weights and biases to correctly classify the digits of a seven-segment display. For this task you have the data needed via the first task above.\n",
    "\n",
    "Create the network using the PyTorch model [torch.nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html), which you can find more information about in the [PyTorch documentation](https://pytorch.org/docs/stable/index.html). Whenever you encounter a new PyTorch function or class it's helpful to look it up in the documentation. If you wonder whether a particular machine learning feature exists in PyTorch you can also search in the documentation, or use the more brute force method of web search or asking an artificial intelligence for further information (see the discussion forum about AI in Canvas for further information)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-02T15:34:41.298227400Z",
     "start_time": "2024-02-02T15:34:41.055489400Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6Fi9d0upakfN",
    "outputId": "662c45ff-0eb2-40ba-96be-ba8382875a58"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[6], line 6\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m## Data and labels\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;66;03m# You don't need to edit this code, we simply show you how, in this case, we create the data and labels we need\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \n\u001B[1;32m      4\u001B[0m \u001B[38;5;66;03m# A matrix (or vector) in PyTorch is usually represented by a Tensor\u001B[39;00m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;66;03m# Create a Tensor with our digit vectors\u001B[39;00m\n\u001B[0;32m----> 6\u001B[0m data \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mTensor(np\u001B[38;5;241m.\u001B[39marray(x)) \u001B[38;5;66;03m# A Tensor can be created by simply giving it a nested numpy array/list of numbers as input\u001B[39;00m\n\u001B[1;32m      7\u001B[0m data \u001B[38;5;241m=\u001B[39m data\u001B[38;5;241m.\u001B[39mdetach() \u001B[38;5;66;03m# Since we won't be chaning the data during training we detach the Tensor from the computation graph\u001B[39;00m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;66;03m# Our labels will be the expected outputs of each perceptron for each digit\u001B[39;00m\n\u001B[1;32m     10\u001B[0m \u001B[38;5;66;03m# This means we can't simply say \"5\" is our expected output, but rather that we want [0,0,0,0,0,1,0,0,0,0]\u001B[39;00m\n\u001B[1;32m     11\u001B[0m \u001B[38;5;66;03m# This is called one-hot encoding where we have a vector where the value is one at the given index and zero everywhere else\u001B[39;00m\n\u001B[1;32m     12\u001B[0m \u001B[38;5;66;03m# Since a matrix with one-hot representations of the corresponding index is simply an identity matrix, we use that as our labels\u001B[39;00m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "## Data and labels\n",
    "# You don't need to edit this code, we simply show you how, in this case, we create the data and labels we need\n",
    "\n",
    "# A matrix (or vector) in PyTorch is usually represented by a Tensor\n",
    "# Create a Tensor with our digit vectors\n",
    "data = torch.Tensor(np.array(x)) # A Tensor can be created by simply giving it a nested numpy array/list of numbers as input\n",
    "data = data.detach() # Since we won't be chaning the data during training we detach the Tensor from the computation graph\n",
    "\n",
    "# Our labels will be the expected outputs of each perceptron for each digit\n",
    "# This means we can't simply say \"5\" is our expected output, but rather that we want [0,0,0,0,0,1,0,0,0,0]\n",
    "# This is called one-hot encoding where we have a vector where the value is one at the given index and zero everywhere else\n",
    "# Since a matrix with one-hot representations of the corresponding index is simply an identity matrix, we use that as our labels\n",
    "labels = torch.eye(10) # Get a matrix with one-hot representations of each digit in each row (an identity matrix)\n",
    "labels = labels.detach() # Since we won't be changing the labels during training we detach the Tensor from the computation graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-02T15:34:41.302230500Z"
    },
    "id": "UBvfcCIHakfN"
   },
   "outputs": [],
   "source": [
    "## The network\n",
    "# We create a simple network from torch.nn.Linear\n",
    "\n",
    "input_size = 7 # What is the size of the input vector to the network?\n",
    "output_size = 10 # What is the size of the output vector of the network?\n",
    "\n",
    "network1 = torch.nn.Linear(input_size, output_size) # Creating a single linear layer of a neural network with the given input and output sizes\n",
    "network2 = torch.nn.Linear(input_size, output_size) # Creating a single linear layer of a neural network with the given input and output sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-02T15:34:41.302230500Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "m_OzCnJoakfO",
    "outputId": "bd0e4450-de90-4a6b-ff26-f33448e45e8f"
   },
   "outputs": [],
   "source": [
    "## Training the network\n",
    "# You don't need to edit this code, we have given you the training loop to train the network\n",
    "\n",
    "epochs = 30 # How many epochs (complete runs of the data) to train for. Since our dataset is small 100 seems reasonable\n",
    "loss_function = torch.nn.MSELoss() # What function to use to calculate the loss given the prediction and labels\n",
    "optimizer = torch.optim.SGD(network1.parameters(), lr=1) # Function for updating the parameters of the network based on loss\n",
    "learning_rate = 1 # How fast to optimize the network. Since our problem is quite small we can have a large learning rate, otherise 0.01 is usually standard\n",
    "\n",
    "# Create a list to keep track of how the loss changes\n",
    "losses = []\n",
    "\n",
    "# For each epoch\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    # Predict for each vector what digit they represent\n",
    "    prediction = network1(data)\n",
    "\n",
    "    # Calculate the loss of the prediction by comparing to the expected output\n",
    "    loss = loss_function(prediction, labels)\n",
    "\n",
    "    # Backpropogate the loss through the network to find the gradients of all parameters\n",
    "    loss.backward()\n",
    "\n",
    "    # Update the parameters along their gradients\n",
    "    optimizer.step()\n",
    "\n",
    "    # Clear stored gradient values\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Add the loss to the total epoch loss (item() turns a PyTorch scalar into a normal Python datatype)\n",
    "    losses.append(loss.item())\n",
    "\n",
    "    #Print the epoch and loss every 10 epochs\n",
    "#    if epoch % 10 == 9:\n",
    "#        print(f'Epoch {epoch+1} - Loss: {loss}')\n",
    "\n",
    "# Plot the training loss per epoch\n",
    "print(\"Network 1 output\", network1(data))\n",
    "\n",
    "#for name, param in network1.named_parameters():\n",
    "#  print(name, param)\n",
    "\n",
    "# Plot the training loss per epoch\n",
    "plt.plot(range(1,epochs+1),losses)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()\n",
    "\n",
    "## Training the network\n",
    "# You don't need to edit this code, we have given you the training loop to train the network\n",
    "\n",
    "epochs = 1000 # How many epochs (complete runs of the data) to train for. Since our dataset is small 100 seems reasonable\n",
    "loss_function = torch.nn.MSELoss() # What function to use to calculate the loss given the prediction and labels\n",
    "optimizer = torch.optim.SGD(network2.parameters(), lr=1) # Function for updating the parameters of the network based on loss\n",
    "learning_rate = 1 # How fast to optimize the network. Since our problem is quite small we can have a large learning rate, otherise 0.01 is usually standard\n",
    "\n",
    "# Create a list to keep track of how the loss changes\n",
    "losses = []\n",
    "\n",
    "# For each epoch\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    # Predict for each vector what digit they represent\n",
    "    prediction = network2(data)\n",
    "\n",
    "    # Calculate the loss of the prediction by comparing to the expected output\n",
    "    loss = loss_function(prediction, labels)\n",
    "\n",
    "    # Backpropogate the loss through the network to find the gradients of all parameters\n",
    "    loss.backward()\n",
    "\n",
    "    # Update the parameters along their gradients\n",
    "    optimizer.step()\n",
    "\n",
    "    # Clear stored gradient values\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Add the loss to the total epoch loss (item() turns a PyTorch scalar into a normal Python datatype)\n",
    "    losses.append(loss.item())\n",
    "\n",
    "print(f\"Network2 output {network2(data)}\")\n",
    "\n",
    "# Plot the training loss per epoch\n",
    "plt.plot(range(1,epochs+1),losses)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()\n",
    "\n",
    "prediction = network2(data).detach()\n",
    "fig, ax0 = plt.subplots(1, 1)\n",
    "digits = numpy.arange(10)  # len = 6\n",
    "X, Y = numpy.meshgrid(digits, digits)\n",
    "im = ax0.pcolormesh(X, Y, prediction, vmin=0.0, vmax=1.0)  # vmax=0.8\n",
    "fig.colorbar(im, ax=ax0)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Labels {labels}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "awyEnJ0MakfO"
   },
   "source": [
    "### Task 1.3: Check the solution\n",
    "\n",
    "Execute the cell below to see whether the network managed to learn to make the correct predictions.\n",
    "Can you figure out what the learned weights and biases are, and how similar they are to your hardcoded solutions in the first task?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-02T15:34:41.306230700Z",
     "start_time": "2024-02-02T15:34:41.304230900Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UoxKyWpnakfP",
    "outputId": "9aba1fe0-e528-4518-ea30-c0356122d32d"
   },
   "outputs": [],
   "source": [
    "## Testing the trained network\n",
    "# You don't need to edit this code\n",
    "\n",
    "with torch.no_grad(): # Since we're not training we don't want to calculate the gradients for this prediction\n",
    "    prediction = network1(data) # Let's make one final prediction of the data\n",
    "\n",
    "for digit in range(10):\n",
    "    print(f'Digit {digit} was predicted to be {torch.argmax(prediction[digit])}') # argmax gets the index with the greatest value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OFdbURfUakfQ"
   },
   "source": [
    "## Task 2\n",
    "\n",
    "While it is unnecessary to use machine learning to develop classifiers for a seven-segment display there are many other datasets for which it is difficult, if not impossible, to engineer such solutions without data-driven training/optimization of a model. For example, we have no generally useful and efficient method enabling us to write a program that can tell the difference between images of cats and dogs. However, we can easily gather many images of each, label the images, and use the resulting dataset to train a neural network that solves the task. A central problem in machine learning is how to define and train such a model to obtain maximum performance on new data which has not been used to optimize the model, referred to as the generalization of the model. Another important problem in machine learning is to minimize the computational cost of training and using such models.\n",
    "\n",
    "Next you will extend the work started in Exercise 0 by developing a 2-layer neural network for classification of the MNIST dataset. MNIST consist of 70,000 grayscale images of handwritten digits that are 28x28 pixels each. Our goal is to train a 2-layer network that can recognize what digit an image represents. The subtasks are\n",
    "\n",
    "* Implementation of a 2 layer NN (very similar to ex0)\n",
    "* Training of this 2 layer NN (once again, very similar to ex0)\n",
    "* Validation of the network during training (requires splitting the training set)\n",
    "    - Save the model which performs the best on the validation data\n",
    "* Graph the training loss vs validation loss\n",
    "* You should obtain at least 85% accuracy on the test data (remember to load the best performing model before performing the accuracy test)\n",
    "\n",
    "The code below loads the dataset (downloads it if necessary) and displays one of the images. You have to modify this code to complete this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-02T15:34:41.305230800Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 812
    },
    "id": "vRJ_DR7BakfQ",
    "outputId": "31f96268-e170-4969-e1dc-2bdce60b64b0"
   },
   "outputs": [],
   "source": [
    "# Define the mini-batch size\n",
    "batch_size = 1000\n",
    "\n",
    "train_loader = DataLoader(parts[0], batch_size=batch_size, shuffle=False)\n",
    "validate_loader = DataLoader(parts[1], batch_size=batch_size, shuffle=False)\n",
    "print(\"Train\", type(train_loader))\n",
    "\n",
    "test_loader = DataLoader(mnist_test, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "to_onehot = nn.Embedding(10, 10)\n",
    "to_onehot.weight.data = torch.eye(10)\n",
    "\n",
    "def plot_digit(data):\n",
    "    data = data.view(28, 28)\n",
    "    plt.imshow(data, cmap=\"gray\")\n",
    "    plt.show()\n",
    "\n",
    "images, labels = next(iter(train_loader))\n",
    "plot_digit(images[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e9BEaX-zakfQ"
   },
   "source": [
    "### Task 2.1\n",
    "\n",
    "Implement a 2-layer neural network using pytorch as well as a procedure for training and testing it. The training protocol should include both training and validation. Thus you need to split the training data into a training set (for which the error is backpropagated to update the parameters) and a validation set (which will not be used to directly update the model parameters, and instead be used to keep track of how good the model is at unseen data).\n",
    "\n",
    "The weights of the model which performs the best on the validation data should be stored and then be used for the final check on the test set. Validation sets are often created by taking a fraction of the training data (often, but not always, around 20%) at random. In Pytorch you might want to use [random_split](https://pytorch.org/docs/stable/data.html#torch.utils.data.random_split) for this. Using random split would require you to edit the way the Dataloaders are created.\n",
    "\n",
    "You are free to choose any optimizer and loss function. Just note that some loss functions require the labels to be 1-hot encoded. As you will not use convolutional layers for this exercise (will be introduced later in the course), the inputs need to be transformed to 1d tensors (see [view](https://pytorch.org/docs/stable/tensors.html?highlight=view#torch.Tensor.view)).\n",
    "\n",
    "**GOAL:** You should evaluate the network from the epoch with best validation score (early stopping) on the test set aiming to reach at least 85% accuracy.\n",
    "\n",
    "**Remember** to run all your code before grading so that the teacher doesn't have to wait around for long training runs. Plot the training and validation losses for each epoch.\n",
    "\n",
    "*Hint:* Validation and Testing loops are very similar to training except they don't use backpropagation. Additionally testing should only be performed once, while validation should be performed continually to make sure training is proceeding as intended and to save the parameters of the best epoch.\n",
    "\n",
    "*Hint:* Storing the best model is more difficult than just assigning it to a variable as this only means you have two variables referencing the same network instance in memory (not a copy of the best betwork and one containing the current network). Instead you ned to make a copy of the network which can be achived with [deepcopy](https://docs.python.org/3/library/copy.html). Other ways to store models include saving them as a file which can be done with [torch.save](https://pytorch.org/tutorials/beginner/saving_loading_models.html).\n",
    "\n",
    "*Hint:* Everytime you train a network with random parameter initialization and random batches you get networks with different performance. Sometimes just running the training again can be enough to get a better result. However, if you do this too many times you run the risk of training (overfitting) on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-02T15:34:41.307231600Z",
     "start_time": "2024-02-02T15:34:41.306230700Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nO84iGvDakfQ",
    "outputId": "7e5586a9-f944-4622-86bd-e282f3ba8628"
   },
   "outputs": [],
   "source": [
    "# Implement the network here using PyTorch\n",
    "net = nn.Sequential(\n",
    "    nn.Linear(28 * 28, 100),\n",
    "    nn.Linear(100, 10),\n",
    ").to(device)\n",
    "\n",
    "def build_input(images):\n",
    "  return images.view(-1, 28 * 28)\n",
    "\n",
    "learning_rate = 1e-3\n",
    "optimizer = torch.optim.RMSprop(net.parameters(), lr=learning_rate)\n",
    "\n",
    "_m = nn.Sigmoid()\n",
    "_loss_fn = nn.BCELoss()\n",
    "def loss_fn(prediction, labels):\n",
    "  return _loss_fn(_m(prediction), labels)\n",
    "\n",
    "def to_onehot(tensor):\n",
    "  return nn.functional.one_hot(tensor, num_classes=10)\n",
    "\n",
    "epocs = 100\n",
    "epocs_since_best = 0\n",
    "prev_fraction = 0.0\n",
    "best_fraction = 0.0\n",
    "best_net = None\n",
    "for epoch in range(epocs):\n",
    "  # Training\n",
    "  for batch_no, (images, labels) in zip(itertools.count(), iter(train_loader)):\n",
    "    labels = to_onehot(labels).type(torch.Tensor).to(device)\n",
    "    images = build_input(images).to(device)\n",
    "\n",
    "    prediction = net(images)\n",
    "\n",
    "    loss = loss_fn(prediction, labels)\n",
    "\n",
    "    net.zero_grad()\n",
    "    gradients = loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "  # Validation\n",
    "  correct = 0\n",
    "  validation_size = 0\n",
    "  for batch_no, (images, labels) in zip(itertools.count(), iter(validate_loader)):\n",
    "    labels = labels.to(device)\n",
    "    images = build_input(images).to(device)\n",
    "\n",
    "    predictions = net(images)\n",
    "\n",
    "    guesses = torch.argmax(predictions, dim=1)\n",
    "\n",
    "    correct += torch.sum(guesses == labels)\n",
    "    validation_size += labels.shape[0]\n",
    "\n",
    "  epocs_since_best += 1\n",
    "  correct_fraction = correct / validation_size\n",
    "  if correct_fraction > best_fraction:\n",
    "    direction = \"best yet\"\n",
    "    best_net = copy.deepcopy(net)\n",
    "    best_fraction = correct_fraction\n",
    "    epocs_since_best = 0\n",
    "  elif correct_fraction > prev_fraction:\n",
    "    direction = \"up\"\n",
    "  else:\n",
    "    direction = \"stagnant or down\"\n",
    "  prev_fraction = correct_fraction\n",
    "  print(f\"Epoch {epoch} not improving {epocs_since_best}. Total validation success {100 * correct_fraction:.1f}% {direction}\")\n",
    "  if epocs_since_best > 8:\n",
    "      print(\"early break - no progress\")\n",
    "      break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-02T15:34:41.310231300Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZiQkDfISE8iY",
    "outputId": "789abd83-a352-402c-b8a7-127c3da659ad"
   },
   "outputs": [],
   "source": [
    "# Test\n",
    "correct = 0\n",
    "validation_size = 0\n",
    "for batch_no, (images, labels) in zip(itertools.count(), iter(test_loader)):\n",
    "  labels = labels.to(device)\n",
    "  images = build_input(images).to(device)\n",
    "\n",
    "  predictions = best_net(images)\n",
    "\n",
    "  guesses = torch.argmax(predictions, dim=1)\n",
    "\n",
    "  correct += torch.sum(guesses == labels)\n",
    "  validation_size += labels.shape[0]\n",
    "\n",
    "correct_fraction = correct / validation_size\n",
    "print(f\"Test. Total test success {100 * correct_fraction:.1f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5EgJ7RVxakfR"
   },
   "source": [
    "## Task 3\n",
    "\n",
    "Implement a 2-layer neural network like above, but now without using a high-level machine learning library like pytorch. The network should function in the same way as the network in Task 2. The code block below contains code to change the training data from the DataLoader format to the \"raw\" numpy format. It also contains some comments to guide you in the right direction. The substasks involved are\n",
    "\n",
    "* Implementation of a 2 layer NN using numpy\n",
    "* Training and validation of the 2 layer NN\n",
    "    - Once again, save the best performing model (can be done in memory)\n",
    "* Graph the training vs validation loss\n",
    "* At least 50% accuracy on the validation data (can be hard to get high accuracy)\n",
    "\n",
    "**Note that the solution does not need to be fast/scalable, thus it is OK to develop a custom solution with, e.g., two explicit weight matrix variables representing the two layers.**\n",
    "\n",
    "### The foward pass\n",
    "This is easy. Recall that each differnt layer is calculated by the formula:\n",
    "$$ y = g(\\mathbf{W}*\\mathbf{x} + b) $$\n",
    "where $W$ is the weight matrix, $x$ the input, $b$ the bias and $g$ the non-linearity. For this exercise you are allowed to put $b = 0$ for simplicity when calculating the backwards pass.\n",
    "\n",
    "### Backward pass\n",
    "This can be tricky. In canvas there is lecture material which explains back propogation and all the maths behind it. It should be under *Modules > Artificial Neural Networks (ANN) - Part 1 > Lecture: Backpropagation derivation.mp4*. This, the supplementary material for the lecture, together with the course book should be enough material for you to be able to implement the training algorithm.\n",
    "\n",
    "### Weight update\n",
    "Once you have calculated the gradient of both weight matrixes, this is updated by:\n",
    "$$ W_i = W_i - \\gamma \\dfrac{dL}{dW_i} $$\n",
    "where $\\gamma$ is the step size, or learning rate.\n",
    "\n",
    "***Remember*** to run all your code before grading so the teacher doesn't have to wait around for long training runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-02T15:34:41.311231400Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g3-2kIZGakfR",
    "outputId": "bad562c7-01f5-4d7f-de57-c951e2876eab"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = 'trace'\n",
    "import time\n",
    "\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import partial\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import v2\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "\n",
    "print(\"devices\", jax.devices())\n",
    "\n",
    "train_batch_size = 32\n",
    "batch_size = 64\n",
    "epochs = 50  # Set the number of epochs to train for\n",
    "D_in = 28*28   # Input size, images are 28x28 = 784 element vectors\n",
    "D_out = 10   # Output size, 10 digit classes\n",
    "H1 = 256     # Hidden layer size\n",
    "gamma = 1e-3 / batch_size # Learning rate\n",
    "\n",
    "mnist_train = datasets.MNIST(\"./\", train=True, download=True, transform=v2.Compose([v2.ToImage(),\n",
    "                                                                                    v2.ToDtype(torch.float32, scale=True)]))\n",
    "parts = torch.utils.data.random_split(mnist_train, [0.8, 0.2])\n",
    "mnist_test = datasets.MNIST(\"./\", train=False, download=True, transform=v2.Compose([v2.ToImage(),\n",
    "                                                                                    v2.ToDtype(torch.float32, scale=True)]))\n",
    "\n",
    "train_loader = DataLoader(parts[0], batch_size=train_batch_size, shuffle=False, num_workers=0)\n",
    "validate_loader = DataLoader(parts[1], batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "test_loader = DataLoader(mnist_test, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "# Define network with one hidden layer, random initial weights\n",
    "rnd = jax.random.PRNGKey(42)\n",
    "rnd, rw1, rw2, rb1, rb2 = jax.random.split(rnd, 5)\n",
    "w1 = jax.random.normal(rw1, shape=(H1, D_in))\n",
    "w2 = jax.random.normal(rw2, shape=(D_out, H1))\n",
    "\n",
    "b1=jax.random.normal(rb1, shape=(H1,))\n",
    "b2=jax.random.normal(rb2, shape=(D_out,))\n",
    "\n",
    "# Training iterations\n",
    "@partial(jax.jit, inline=True)\n",
    "def build_input(images: jax.Array) -> jax.Array:\n",
    "  return images.view(-1, 28 * 28)\n",
    "\n",
    "@partial(jax.jit, inline=True)\n",
    "def to_onehot(labels: jax.Array) -> jax.Array:\n",
    "  return jnp.eye(10)[labels]\n",
    "\n",
    "EPSILON = 1e-6\n",
    "\n",
    "@partial(jax.jit, inline=True)\n",
    "def softmax(y_pred):\n",
    "  y_pred = jnp.array(y_pred, dtype=jnp.float32)\n",
    "  y_max = jnp.max(y_pred, axis=1)\n",
    "  z = jnp.subtract(y_pred.T, y_max).T  # for numerical stability\n",
    "  z_exp = jnp.exp(z)\n",
    "  z_exp_sum = jnp.sum(z_exp, axis=1)\n",
    "  return (z_exp.T / z_exp_sum).T\n",
    "\n",
    "\n",
    "@partial(jax.jit, inline=True)\n",
    "def binary_cross_entropy(y_pred, y_true, axis=0):\n",
    "  \"\"\" Computes binary cross entropy between two distributions.\n",
    "  Input: y_pred: iterable of N non-negative values\n",
    "         y_true: iterable of N non-negative values\n",
    "  Returns: scalar\n",
    "  \"\"\"\n",
    "\n",
    "  # Force to proper probability mass function.\n",
    "  y_true = jnp.array(y_true, dtype=jnp.float32)\n",
    "  y_pred = jnp.array(y_pred, dtype=jnp.float32)\n",
    "\n",
    "  # Ignore zero and negative 'y_pred' elements.\n",
    "  y_pred_when_one = jnp.where(y_pred > 0,\n",
    "                              y_pred,\n",
    "                              jnp.exp(-1))  # y_true = 1, y_pred = 0 should give -1\n",
    "\n",
    "  y_pred_when_zero = jnp.where((1 - y_pred) > 0,\n",
    "                               1 - y_pred,\n",
    "                               jnp.exp(-1))  # y_true = 0 y_pred = 1 should give -1\n",
    "\n",
    "  multiply_01 = jnp.log(jnp.where(y_true == 1,\n",
    "                                  y_pred_when_one,\n",
    "                                  y_pred_when_zero))\n",
    "\n",
    "  if False:\n",
    "    # d/dx lg(x) = 1/x\n",
    "    # -(y_true / y_pred + g'(y_pred) * (1 - y_true) / (1 - y_pred)) where g(x) = 1 - y_pred\n",
    "    # rewrite to common denominator, make sure divisor does not get very close to 0\n",
    "    # TODO softmax?\n",
    "    loss_d = jnp.divide(y_pred - y_true,\n",
    "                        jnp.multiply(y_pred - 1, y_pred))\n",
    "  else:\n",
    "    # Categorical Cross-Entropy Loss\n",
    "    # https://towardsdatascience.com/derivative-of-the-softmax-function-and-the-categorical-cross-entropy-loss-ffceefc081d1\n",
    "    loss_d = y_pred - y_true  # Note: y_pred is limited by softmax\n",
    "\n",
    "  return -multiply_01.mean(axis=1), loss_d\n",
    "\n",
    "def test_cross_entropy():\n",
    "    tbce = binary_cross_entropy([[0.5, 0.5, 0, 1, 0, 1]],\n",
    "                                [[0,   1,   0, 0, 1, 1]])\n",
    "    print(\"test binary_cross_entropy\", tbce)\n",
    "    y_pred_sm_ = softmax([[0.5, 0.5, 0, 1, 0, 1]])\n",
    "    print(\"y_pred softmax\", y_pred_sm_)\n",
    "    tbce = binary_cross_entropy(y_pred_sm_,\n",
    "                                [[0,   1,   0, 0, 1, 1]])\n",
    "    print(\"test binary_cross_entropy softmax\", tbce)\n",
    "\n",
    "# test_cross_entropy()\n",
    "\n",
    "@partial(jax.jit, inline=True)\n",
    "def loss_fn(y_pred, y):\n",
    "  if False:\n",
    "    # predictions > 1 are interpreted as 1, predictions < 0 are interpreted as 0\n",
    "    err=np.subtract(y_pred.clip(0, 1), y)\n",
    "    loss = np.sum(np.power(err, 2), axis=1)\n",
    "    loss_d = 2*err\n",
    "  else:\n",
    "    y_pred_ = softmax(y_pred)\n",
    "    loss, loss_d = binary_cross_entropy(y_pred_, y)\n",
    "  return loss, loss_d\n",
    "\n",
    "\n",
    "@partial(jax.jit, inline=True)\n",
    "def collect(h1_: jax.Array, g: jax.Array, x: jax.Array, w2: jax.Array):\n",
    "    #layer2\n",
    "    #activater function g=g cause we don't have an activater function\n",
    "    #lamda=0 no regulization then\n",
    "    b2_d = g.reshape((D_out,))\n",
    "    w2_d = jnp.matmul(g, h1_.T)\n",
    "    g = jnp.matmul(w2.T, g)\n",
    "    #layer1\n",
    "    b1_d = g.reshape((H1,))\n",
    "    w1_d = jnp.matmul(g, x.T)\n",
    "\n",
    "    return (w1_d, b1_d), (w2_d, b2_d)\n",
    "\n",
    "vmap_collect = jax.jit(jax.vmap(collect, in_axes=(0, 0, 0, None)))\n",
    "\n",
    "@partial(jax.jit)\n",
    "def train_batch(inputs: jax.Array, labels: jax.Array, w1: jax.Array, b1: jax.Array, w2: jax.Array, b2: jax.Array):\n",
    "    # iterate through the mini-batch and perform forward pass and backward pass\n",
    "    x = inputs.reshape((-1, D_in))\n",
    "    y = to_onehot(labels)    # 1-hot encoding\n",
    "\n",
    "    # Forward pass\n",
    "    h1 = jnp.matmul(x, w1.T) + b1\n",
    "    h2 = jnp.matmul(h1,w2.T) + b2\n",
    "    #h_relu = jnp.maximum(0,h1) # x * (x > 0)\n",
    "    y_pred = h2\n",
    "\n",
    "    # Compute loss function, squared error\n",
    "    loss, loss_d = loss_fn(y_pred, y)\n",
    "    total_training_loss = loss.sum()\n",
    "\n",
    "    # Compute gradients of square-error loss with respect to w1 and w2 using backpropagation\n",
    "    #derivate of loss function\n",
    "\n",
    "\n",
    "    der = vmap_collect(h1.reshape(-1, H1, 1),\n",
    "                                                              loss_d.reshape(-1, D_out, 1),\n",
    "                                                              x.reshape(-1, D_in, 1),\n",
    "                                                              w2)\n",
    "    (w1_da, b1_da), (w2_da, b2_da) = jax.tree_util.tree_map(lambda arr: jnp.sum(arr, axis=0), der)\n",
    "\n",
    "    # Return update to accumulators\n",
    "    return (total_training_loss,\n",
    "            (w1 - gamma * w1_da, b1 - gamma * b1_da),\n",
    "            (w2 - gamma * w2_da, b2 - gamma * b2_da))\n",
    "\n",
    "@partial(jax.jit)\n",
    "def validate_batch(x: jax.Array, w1: jax.Array, h1: jax.Array, w2: jax.Array, h2: jax.Array, labels: jax.Array):\n",
    "    h1 = jnp.matmul(x, w1.T) + b1\n",
    "    h2 = jnp.matmul(h1, w2.T) + b2\n",
    "    predictions = h2\n",
    "\n",
    "    guesses = jnp.argmax(predictions, axis=1)\n",
    "    correct = jnp.sum(guesses == labels)\n",
    "    loss, _ = loss_fn(predictions, to_onehot(labels))\n",
    "    return correct, loss\n",
    "\n",
    "prev_wall = time.perf_counter()\n",
    "jit_train_batch = jax.jit(train_batch)\n",
    "jit_validate_batch = jax.jit(validate_batch)\n",
    "now_wall = time.perf_counter()\n",
    "print(f\"JAX compile time {now_wall - prev_wall:.3f}\")\n",
    "prev_wall = now_wall\n",
    "\n",
    "print(\"big bang\")\n",
    "# Train for a number of epochs\n",
    "plot_data = []\n",
    "for epoch in range(epochs):\n",
    "    # Training by looping over training set\n",
    "    total_trainings = 0\n",
    "    total_training_loss = 0.0\n",
    "    for batch_no, (inputs, labels) in zip(itertools.count(), iter(train_loader)):\n",
    "        inputs_ = inputs.numpy()  #x\n",
    "        labels_ = labels.numpy()  #y\n",
    "\n",
    "        # Compute loss function, squared error\n",
    "        loss, (w1, b1), (w2, b2) = jit_train_batch(inputs_, labels_, w1, b1, w2, b2)\n",
    "\n",
    "#        print(f\"Epoch {epoch} batch {batch_no} loss {batch_training_loss / batch_size:.3f}\")\n",
    "\n",
    "        batch_trainings = labels.shape[0]\n",
    "        batch_training_loss = loss.sum()\n",
    "        total_trainings += batch_trainings\n",
    "        total_training_loss += batch_training_loss\n",
    "\n",
    "    # Validate the model\n",
    "    total_validations = 0\n",
    "    correct = 0\n",
    "    total_validation_loss = 0.0\n",
    "    for batch_no, (images, labels) in zip(itertools.count(), iter(validate_loader)):\n",
    "      labels = labels.numpy()\n",
    "      images = images.numpy()\n",
    "\n",
    "      x = images.reshape((-1, D_in))\n",
    "\n",
    "      batch_correct, batch_loss = jit_validate_batch(x, w1, b1, w2, b2, labels)\n",
    "      correct += batch_correct\n",
    "      total_validations += labels.shape[0]\n",
    "      total_validation_loss += batch_loss.sum()\n",
    "\n",
    "    correct_fraction = correct / total_validations\n",
    "    validation_loss_mean = total_validation_loss / total_validations\n",
    "    training_loss_mean = total_training_loss / total_trainings\n",
    "    batch_training_loss_mean = batch_training_loss / batch_trainings  # last batch\n",
    "    now_wall = time.perf_counter()\n",
    "    print(f\"Epoch {epoch} {now_wall - prev_wall:.3f}s Training loss {training_loss_mean:.3f}, last batch {batch_training_loss_mean:.3f} \"\n",
    "          f\"Validation loss {validation_loss_mean:.3f} \"\n",
    "          f\"success {100 * correct_fraction:.1f}%\")\n",
    "    prev_wall = now_wall\n",
    "# Plot training and validation loss\n",
    "    plot_data.append((training_loss_mean, batch_training_loss_mean, validation_loss_mean, correct_fraction))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-02T15:34:41.311231400Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 881
    },
    "id": "05M3rBmsEY6i",
    "outputId": "21033501-2a76-46d2-daa7-a8cb379f72f8"
   },
   "outputs": [],
   "source": [
    "plot_data_ = numpy.array(plot_data)\n",
    "epochs_ = plot_data_.shape[0]  # actual epochs recorded\n",
    "\n",
    "# Plot the training loss per epoch\n",
    "fig, ax = plt.subplots()\n",
    "ttl = ax.plot(range(1, epochs_ + 1), plot_data_[:, 0:3])\n",
    "ax.legend([\"total training loss\", \"last batch training loss\", \"validation loss\"])\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(range(1, epochs_ + 1), plot_data_[:, 3])\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation Correctness')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-02T15:34:41.312230500Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SU7ZQs882spZ",
    "outputId": "85e4f518-637a-4013-e257-c3a52501baa7"
   },
   "outputs": [],
   "source": [
    "# Calculate accuracy on one test batch\n",
    "\n",
    "total = 0\n",
    "correct = 0\n",
    "for batch_no, (images, labels) in zip(itertools.count(), iter(test_loader)):\n",
    "  labels = jnp.array(labels.numpy())\n",
    "  images = jnp.array(images.numpy())\n",
    "\n",
    "  x = images.reshape((-1, D_in))\n",
    "\n",
    "  h1 = jnp.matmul(x, w1.T) + b1\n",
    "  h2 = jnp.matmul(h1, w2.T) + b2\n",
    "  predictions = h2\n",
    "\n",
    "  guesses = jnp.argmax(predictions, axis=1)\n",
    "\n",
    "  correct += jnp.sum(guesses == labels)\n",
    "  total += labels.shape[0]\n",
    "\n",
    "correct_fraction = correct / total\n",
    "print(f\"Test. Total test success {100 * correct_fraction:.1f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j7dn5B3makfR"
   },
   "source": [
    "## Optional programming challenge\n",
    "You have now implemented a 2-layer neural network from scratch! Use this new gained knowledge to create an implementation where we can create and train a network with an arbitrary number of layers and units in each layer. This means that we should be able to specify the structure of the network and then train it using backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-02T15:34:41.312730900Z"
    },
    "id": "Ln_Q9YAKakfR"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uqrVMN9TakfR"
   },
   "source": [
    "## End\n",
    "\n",
    "You have now reached the end of ANN1. When you have completed and understood the task above please make sure that all results inluding plots have been computed and then schedule a meeting with a teacher. The teacher will then assess orally that you (the lab group) has completed the exercise and that you understand its essental elements."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "cell_execution_strategy": "setup",
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
