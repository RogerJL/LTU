{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 - RNN theory\n",
    "## What is an RNN?\n",
    "An RNN is a neural network just like the network you designed in ANN1, but with two important distinctions:\n",
    "1) It iteratively produces outputs based on both inputs and previous (recurrent) outputs using shared parameters across time steps\n",
    "2) It employs a \"hidden state\" that is iteratively computed and updated in each forward pass (the hidden state is not a trainable parameter)\n",
    "\n",
    "### Sequence to sequence data\n",
    "RNNs are used mainly for sequential data, i.e. data arranged in sequences where order matters. Typically, this is either time series data, such as weather forecasts or stock markets, or in NLP due to the sequential nature of language. The sequence-to-sequence (seq2seq) image below showcases a comprehensive example of ways sequential data can flow through an RNN.\n",
    "\n",
    "![Seq2seq](http://karpathy.github.io/assets/rnn/diags.jpeg)\n",
    "\n",
    "The leftmost rectangle is a normal feedforward network without RNNs, e.g. your digit predictor from ANN1. <br>\n",
    "The \"one to many\" rectangle takes one input (non-sequential) and produces a sequence of output, e.g. image captioning which takes one image and produces a sequence of text in response. <br>\n",
    "\"many to one\" is the opposite of image captioning, and can be for instance image generatino, where a sequence of text is processed to produce one image. A simpler example is language classification tasks such as sentiment analysis. <br>\n",
    "Finally, there are two examples of \"many to many\". The first example reads the entire input before producing an output, commonly seen in machine translation, where the model reads the whole text in one language to produce a hidden state that represents the latent meaning of the text, then decodes the hidden state to the translated language. The other example of many to many is synced, where each input requires one output in response, such as video frame classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Briefly describe two major challenges and drawbacks with running a standard feed forward ANN (like the one from ANN1) on sequential data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-17T22:42:37.957960700Z",
     "start_time": "2024-02-17T22:42:37.714962100Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to NNLM Exercise ANN2\n",
    "\n",
    "This exercise is split into three parts:\n",
    "- In part 1, you will learn the fundamentals of RNNs\n",
    "- In part 2, you will implement a basic RNN yourselves using numpy\n",
    "- In part 3, you will implement an LSTM using pytorch\n",
    "\n",
    "In this exercise you'll explore recurrent neural networks (RNNs), and some basic natural language processing (NLP). More specifically, your task will be to design an RNN from scratch using only numpy, similar to task 3 in ANN1. The goal of this network is to predict the correct next character given a sequence of characters. This is a simplified version of [causal language modelling](https://huggingface.co/docs/transformers/tasks/language_modeling), which is the main objective that is used to train large language models such as the [GPT series](http://jalammar.github.io/illustrated-gpt2/).\n",
    "\n",
    "## Reading material\n",
    "Before doing this exercise, please study the ANN2 module in Canvas and read Chapter 10 in the Deep Learning book, especially 10.2.2. If you want to learn more details about RNNs, we recommend [Andrej Karpathy's blog post](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) and [this lecture](https://www.youtube.com/watch?v=0LixFSa7yts) and [this lecture](https://www.youtube.com/watch?v=6niqTuYFZLQ)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-19T12:49:03.472531900Z",
     "start_time": "2024-02-19T12:49:03.416533900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM(\n",
      "  (lstm): LSTM(4, 25)\n",
      "  (l_out): Linear(in_features=25, out_features=4, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Initialise our network\n",
    "model = LSTM(vocab_size=vocab_size, hidden_size=25).to(device)\n",
    "print(model)\n",
    "min_LSTM_loss = 10000\n",
    "\n",
    "# Store training and validation loss\n",
    "training_loss, validation_loss = [], []\n",
    "\n",
    "# Define a loss function and optimizer for this problem\n",
    "criterion = nn.BCELoss(reduction='sum')  # nn.CrossEntropyLoss()  # \n",
    "optimizer = torch.optim.Adam(model.parameters())  #, lr=3e-3)\n",
    "# A way to get learning rate decay\n",
    "#scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write your answer here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fixed architecture it can't be changed on the fly, it needs to be predetermined.\n",
    "Memory/Context, no memory of past inputs or outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN structure\n",
    "\n",
    "![rnn-unroll image](https://upload.wikimedia.org/wikipedia/commons/b/b5/Recurrent_neural_network_unfold.svg)\n",
    "\n",
    "The above image explains the basics of an RNN quite well. Looking at the time-unfolded representation to the right, you'll see how the time-sequential data $x$ is transformed to the outputs $o$. The trainable parameters, the weight matrices in the network that you're building, consist of $U$, $V$, and $W$, where \n",
    "\n",
    "\n",
    "- $U$ defines the connections between the input sample $x_t$ and the current hidden state $h_t$\n",
    "- $V$ defines the connections between the previous hidden state $h_{t-1}$ and $h_t$, and \n",
    "- $W$ defines the connections between the hidden state $h_t$ and the output $o_t$.\n",
    "\n",
    "\n",
    "Note that, as per the seq2seq image above, all parameters are not always used in every time step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This network can thus be represented mathematically as follows:\n",
    "\\begin{equation*}\n",
    "h_t = f(U\\,{x_t} + V\\,{h_{t-1}})\n",
    "\\tag{1}\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "o_t = \\mathrm{softmax}(W\\,{h_t})\n",
    "\\tag{2}\\end{equation*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A forward pass through an RNN\n",
    "- Let's start at $t = 0$ and take two steps through this network. <br>\n",
    "- Initialise the hidden state h_init with zeros or randomly, $dims: [hidden\\_size, 1]$. <br>\n",
    "- Also initialise the parameters $U$, $V$, and $W$, again with zeros or randomly, since our network is not trained yet.\n",
    "  - $U$ transforms the input to a hidden state contribution $input\\_dim \\rightarrow hidden\\_size$.\n",
    "  - $V$ updates the hidden state $hidden\\_size \\rightarrow hidden\\_size$.\n",
    "  - $W$ transforms the hidden state to the output $hidden\\_size \\rightarrow output\\_dim$. <br>\n",
    "- First, compute the hidden state $h_0$ as a function of $x_0*U$ and $h_{init} * V$. <br>\n",
    "- Then, compute $o_0$ as a function of $h_0$ and $W$. <br>\n",
    "- Then, for the next time step, compute $h_1$ as a function of $x_1*U$ and $h_0*V$. <br>\n",
    "- Finally, compute $o_1$ based on $h_1*W$.<br>\n",
    "- Note that the parameters $U$, $V$ and $W$ remain the same for each iteration. This is because the passing of the sequential data through the same network, as seen to the left of the image, which can be conceptualised as a network with shared weights, as in the right image. <br>\n",
    "- Important: the hidden state $h$ is not learned, but produced during the recurrence. The weights to update the hidden state are learned however."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to train RNNs?\n",
    "\n",
    "Well, you have to backpropagate through time, which thankfully is not as complicated as it sounds. <br>\n",
    "The final loss between output $o_N$ and label $y_N$ will be a function not only of $x_N$, but of $h_N$. But $h_N$ is a function of $x_{N-1}$ and $h_{N-1}$, which is a function of ... <br>\n",
    "You get the point, the longer the sequence in our network, the longer you have to roll back. <br>\n",
    "\n",
    "### How to backpropagate through time?\n",
    "\n",
    "It's not actually that much different compared to ANN1 if you do it step by step. Starting with the last output, $o_N$ in this case, compute the loss. Then backpropagate from loss to $o_N$, i.e. the derivate of the loss function. Then it is simple to calculate the contribution of $W$ to this loss, i.e. $dL/dW$. This is the contribution of $o_N$ to $W_{grad}$. Save this, but don't update yet! <br>\n",
    "Next, backpropagate into $h_N$, similar to backpropagating into the activation function of $w_1$ in ANN1. Compute $dL/dh_N$, then also backprop through the activation function, as in ANN1. <br>\n",
    "Think of these two steps as going backwards from the last output through $W$ to the last hidden state in the image above. \n",
    "\n",
    "Now, you're inside the hidden state, and you have to backprop in two directions. First, compute $dL/dU$, i.e. follow the $U$ arrow back to the input. <br>\n",
    "Then, compute $dL/dV$, i.e. the contribution of the V parameter to the loss, following the arrow back towards the previous hidden state. However, you're not quite done yet, as you also want to compute the contribution of the previous hidden state to the loss, i.e. $dL/dh_{prev}$. Save this for next iteration. <br>\n",
    "Take one time step back, and repeat the process, until you're at $t_0$. <br>\n",
    "\n",
    "### But how does this process take time into account?\n",
    "\n",
    "Note the last derivate you computed, $dL/dh_{prev}$. By solving the partial derivatives you'll find that when computing $dL/dh_{N}$, you relied on the \"fake\" hidden state not yet computed, $dL/dh_{prev}$, which you should initialise to zeros for the first (i.e. last) time step in the backprop. When you compute $dL/dh_{N-1}$ coming from $o_{N-1}$ and $W$, you must include $dL/dh_{prev}$, which this time won't be zero, and corresponds to the backwards movement through time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 - Implementing an RNN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Import packages\n",
    "First, import some packages. We've put all support functions in another file to make the main notebook more readable. If you're interested in how data is loaded and processed, and some very basic NLP, feel free to have a look in the ANN2_support_functions.py file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-17T23:52:11.427885200Z",
     "start_time": "2024-02-17T23:52:11.412885100Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import ANN2_support_functions\n",
    "from importlib import reload\n",
    "reload(ANN2_support_functions)\n",
    "from ANN2_support_functions import set_up_datasets, init_orthogonal, one_hot_encode, one_hot_encode_sequence, sequences_to_dicts, set_up_sequences, clip_gradient_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up data\n",
    "Let's load the data from the support functions. You'll be using two datasets for this task. First, a \"toy\" dataset with simple generated sequences. The function returns four variables: <br>\n",
    "sequence, which is a list of sequences of characters. The sequences have varying length. <br>\n",
    "char_to_idx, a dict that converts a character to an index. <br>\n",
    "idx_to_char, a dict that converts an index to a character. <br>\n",
    "num_sequences, the number of the sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-19T11:38:57.961086500Z",
     "start_time": "2024-02-19T11:38:57.887088200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A sentence from the generated dataset:\n",
      "['a', 'a', 'a', 'a', 'b', 'b', 'b', 'b', 'EOS']\n",
      "There are 100 sentences and 4 unique tokens in our dataset (including UNK).\n",
      "\n",
      "The index of 'b' is 1\n",
      "The char corresponding to index 1 is 'b'\n",
      "We have 80 samples in the training set.\n",
      "We have 10 samples in the validation set.\n",
      "We have 10 samples in the test set.\n"
     ]
    }
   ],
   "source": [
    "sequences, char_to_idx, idx_to_char, num_sequences, vocab_size = set_up_sequences('toy')\n",
    "print('A sentence from the generated dataset:')\n",
    "print(sequences[0])\n",
    "\n",
    "print(f'There are {num_sequences} sentences and {len(char_to_idx)} unique tokens in our dataset (including UNK).\\n')\n",
    "print('The index of \\'b\\' is', char_to_idx['b'])\n",
    "print(f'The char corresponding to index 1 is \\'{idx_to_char[1]}\\'')\n",
    "\n",
    "training_set, validation_set, test_set = set_up_datasets(sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialising the network\n",
    "Recall the RNN structure from part 1 and use this knowledge to finish the code for network initialisation.\n",
    "\n",
    "**Exercise**: Finish the init_network function below to set up the trainable parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-17T22:42:38.132961900Z",
     "start_time": "2024-02-17T22:42:38.055962900Z"
    }
   },
   "outputs": [],
   "source": [
    "def init_network(hidden_size, vocab_size):\n",
    "\n",
    "    \"\"\"\n",
    "    Initialises the RNN parameters\n",
    "    \n",
    "    Args:\n",
    "    `hidden_size`: the dimensions of the hidden state\n",
    "    `vocab_size`: the dimensions of our vocabulary\n",
    "    \"\"\"\n",
    "\n",
    "    # Weight matrix (input to hidden state)\n",
    "    # What is the shape of the input and output for this node?\n",
    "    # TODO:\n",
    "    U = np.zeros((hidden_size, vocab_size))\n",
    "\n",
    "    # Weight matrix (recurrent computation)\n",
    "    # What is the shape of the input and output for this node?\n",
    "    # TODO:\n",
    "    V = np.zeros((hidden_size, hidden_size))\n",
    "\n",
    "    # Weight matrix (hidden state to output)\n",
    "    # What is the shape of the input and output for this node?\n",
    "    # TODO:\n",
    "    W = np.zeros((vocab_size, hidden_size))\n",
    "\n",
    "    # Bias (hidden state)\n",
    "    # What is the shape of the input to this node?\n",
    "    # TODO:\n",
    "    b_hidden = np.zeros((hidden_size, 1))\n",
    "\n",
    "    # Bias (output)\n",
    "    # What is the shape of the input to this node?\n",
    "    # TODO:\n",
    "    b_out = np.zeros((vocab_size, 1))\n",
    "    \n",
    "    # Initialize weights\n",
    "    # You don't have to change anything here\n",
    "    # Think of it as a better way of doing np.random.rand()\n",
    "    U = init_orthogonal(U)\n",
    "    V = init_orthogonal(V)\n",
    "    W = init_orthogonal(W)\n",
    "\n",
    "    \n",
    "    parameters = U, V, W, b_hidden, b_out\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-17T22:42:38.221838600Z",
     "start_time": "2024-02-17T22:42:38.144963400Z"
    }
   },
   "outputs": [],
   "source": [
    "# Checking the init runs\n",
    "\n",
    "hidden_size = 50 # Number of dimensions in the hidden state\n",
    "vocab_size  = vocab_size # Size of the vocabulary used\n",
    "\n",
    "parameters = init_network(hidden_size=hidden_size, vocab_size=vocab_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The forward pass\n",
    "Here you will implement the forward pass. You'll be using tanh as the activation function for the hidden state update (eq. 1), and softmax as the activation function for the output (eq. 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-17T22:42:38.225805600Z",
     "start_time": "2024-02-17T22:42:38.168962100Z"
    }
   },
   "outputs": [],
   "source": [
    "def tanh(x, derivative=False):\n",
    "    \"\"\"\n",
    "    Computes the element-wise tanh activation function for an array x.\n",
    "\n",
    "    Args:\n",
    "     `x`: the array where the function is applied\n",
    "     `derivative`: if set to True will return the derivative instead of the forward pass\n",
    "    \"\"\"\n",
    "    # Avoid division with zeros\n",
    "    x = x + 1e-12\n",
    "    # The tanh function\n",
    "    a_x = (np.exp(x)-np.exp(-x))/(np.exp(x)+np.exp(-x))\n",
    "    \n",
    "    if derivative: # Return the derivative of the function evaluated at x\n",
    "        return 1-a_x**2\n",
    "    else: # Return the forward pass of the function at x\n",
    "        return a_x\n",
    "    \n",
    "def softmax(x):\n",
    "    \"\"\"\n",
    "    Computes the softmax for an array x.\n",
    "    \n",
    "    Args:\n",
    "     `x`: the array where the function is applied\n",
    "    \"\"\"\n",
    "    # Avoid division with zeros\n",
    "    x = x + 1e-12\n",
    "    # The softmax function\n",
    "    a_x = np.exp(x) / np.sum(np.exp(x))\n",
    "    return a_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Finish the code for the forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-17T22:42:38.243246100Z",
     "start_time": "2024-02-17T22:42:38.212784100Z"
    }
   },
   "outputs": [],
   "source": [
    "def forward_pass(inputs, hidden_state, parameters):\n",
    "    \"\"\"\n",
    "    Computes the forward pass of a vanilla RNN.\n",
    "    \n",
    "    Args:\n",
    "    `inputs`: sequence of inputs to be processed\n",
    "    `hidden_state`: an already initialized hidden state\n",
    "    `parameters`: the parameters of the RNN\n",
    "    \"\"\"\n",
    "    # First unpack parameters\n",
    "    U, V, W, b_hidden, b_out = parameters\n",
    "    \n",
    "    # Create a list to store outputs and hidden states\n",
    "    outputs, hidden_states = [], [hidden_state]\n",
    "    \n",
    "    # For each element in input sequence\n",
    "    for t in range(len(inputs)):\n",
    "\n",
    "        # Compute new hidden state\n",
    "        # Hint: look at eq. 1\n",
    "        # TODO:\n",
    "        #print(inputs)\n",
    "        x_temp = U @ inputs[t] + V @ hidden_states[-1] + b_hidden\n",
    "        hidden_state = tanh(x_temp)\n",
    "\n",
    "\n",
    "        # Compute output\n",
    "        # Hint: Look at eq. 2\n",
    "        # TODO:\n",
    "        out = softmax(W @ hidden_state + b_out)\n",
    "        \n",
    "        # Save results and continue\n",
    "        outputs.append(out)\n",
    "        hidden_states.append(hidden_state.copy())\n",
    "    \n",
    "    return outputs, hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-17T22:42:38.268245300Z",
     "start_time": "2024-02-17T22:42:38.231804400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sequence:\n",
      "['a', 'a', 'a', 'a', 'a', 'a', 'a', 'b', 'b', 'b', 'b', 'b', 'b', 'b']\n",
      "\n",
      "Target sequence:\n",
      "['a', 'a', 'a', 'a', 'a', 'a', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'EOS']\n",
      "\n",
      "Predicted sequence:\n",
      "['UNK', 'UNK', 'UNK', 'b', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'EOS', 'EOS', 'b']\n"
     ]
    }
   ],
   "source": [
    "# Checking the forward pass\n",
    "\n",
    "# Get first sequence in training set\n",
    "test_input_sequence, test_target_sequence = training_set[0]\n",
    "\n",
    "# One-hot encode input and target sequence\n",
    "test_input = one_hot_encode_sequence(test_input_sequence, vocab_size, char_to_idx)\n",
    "test_target = one_hot_encode_sequence(test_target_sequence, vocab_size, char_to_idx)\n",
    "\n",
    "# Initialize hidden state as zeros\n",
    "hidden_state = np.zeros((hidden_size, 1))\n",
    "\n",
    "# Now let's try out our new function\n",
    "outputs, hidden_states = forward_pass(test_input, hidden_state, parameters)\n",
    "\n",
    "print('Input sequence:')\n",
    "print(test_input_sequence)\n",
    "\n",
    "print('\\nTarget sequence:')\n",
    "print(test_target_sequence)\n",
    "\n",
    "print('\\nPredicted sequence:')\n",
    "print([idx_to_char[np.argmax(output)] for output in outputs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The backward pass\n",
    "**Exercise:** Finish the code for the backward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-17T22:42:38.348345400Z",
     "start_time": "2024-02-17T22:42:38.251245900Z"
    }
   },
   "outputs": [],
   "source": [
    "def backward_pass(inputs, outputs, hidden_states, targets, parameters):\n",
    "    \"\"\"\n",
    "    Computes the backward pass of a vanilla RNN.\n",
    "    \n",
    "    Args:\n",
    "     `inputs`: sequence of inputs to be processed\n",
    "     `outputs`: sequence of outputs from the forward pass\n",
    "     `hidden_states`: sequence of hidden_states from the forward pass\n",
    "     `targets`: sequence of targets\n",
    "     `params`: the parameters of the RNN\n",
    "    \"\"\"\n",
    "    # First unpack parameters\n",
    "    U, V, W, b_hidden, b_out = parameters\n",
    "    \n",
    "    # Initialize gradients as zero\n",
    "    d_U, d_V, d_W = np.zeros_like(U), np.zeros_like(V), np.zeros_like(W)\n",
    "    d_b_hidden, d_b_out = np.zeros_like(b_hidden), np.zeros_like(b_out)\n",
    "    \n",
    "    # Keep track of hidden state derivative and loss\n",
    "    d_h_prev = np.zeros_like(hidden_states[0])\n",
    "    loss = 0\n",
    "    \n",
    "    # For each element in output sequence\n",
    "    # iterate backwards over t -> t = N, N-1, ... 1, 0\n",
    "    for t in reversed(range(len(outputs))):\n",
    "\n",
    "        # Compute cross-entropy loss\n",
    "        loss += -np.mean(np.log(outputs[t]+1e-12) * targets[t])\n",
    "        \n",
    "        # Backpropagate into output (derivative of cross-entropy and softmax)\n",
    "        # dL/do cancels out all annoying parts of dL/dz * dz/do into a simple function\n",
    "        # please see https://towardsdatascience.com/derivative-of-the-softmax-function-and-the-categorical-cross-entropy-loss-ffceefc081d1\n",
    "        # for more details\n",
    "        d_o = outputs[t].copy()\n",
    "        d_o[np.argmax(targets[t])] -= 1\n",
    "        \n",
    "        \n",
    "        #print(d_W.shape)\n",
    "        #print(d_o.shape)\n",
    "        #print(hidden_states[t+1].T.shape)\n",
    "        \n",
    "        # Backpropagate into W\n",
    "        # TODO:\n",
    "        d_W += d_o @ hidden_states[t+1].T       \n",
    "        \n",
    "        # Backpropagate into bias\n",
    "        d_b_out += d_o\n",
    "        \n",
    "        # Backpropagate into h\n",
    "        # TODO:\n",
    "        d_h = W.T @ d_o\n",
    "        \n",
    "        # Backpropagate through the activation function\n",
    "        # Hint: the tanh function can be called using derivative=True\n",
    "        # TODO: \n",
    "        d_a = 1-hidden_states[t+1]**2\n",
    "\n",
    "        # Backpropagate into bias\n",
    "        #print(d_h.shape)\n",
    "        #print(d_a.shape)\n",
    "        d_b_hidden += d_a.T @ d_h\n",
    "        \n",
    "        # Backpropagate into U\n",
    "        # TODO:\n",
    "        d_U += d_h @ inputs[t].T\n",
    "        \n",
    "        # Backpropagate into V\n",
    "        # TODO:\n",
    "        d_V += d_h @ hidden_states[(t+1)-1].T\n",
    "\n",
    "        # Backpropagate into previous hidden state\n",
    "        d_h_prev = d_h\n",
    "    \n",
    "    # Pack gradients\n",
    "    grads = d_U, d_V, d_W, d_b_hidden, d_b_out    \n",
    "    \n",
    "    # Clip gradients to prevent explosions\n",
    "    grads = clip_gradient_norm(grads)\n",
    "    \n",
    "    return loss, grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-17T22:42:38.351344300Z",
     "start_time": "2024-02-17T22:42:38.283347200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We get a loss of: 4.553408581307237\n"
     ]
    }
   ],
   "source": [
    "# Checking the backwards pass\n",
    "\n",
    "# Perform a backward pass\n",
    "loss, gradients = backward_pass(test_input, outputs, hidden_states, test_target, parameters)\n",
    "\n",
    "\n",
    "print('We get a loss of:', loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The optimisation algorithm\n",
    "**Exercise:** Finish the code for gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-17T22:42:38.367345200Z",
     "start_time": "2024-02-17T22:42:38.336345700Z"
    }
   },
   "outputs": [],
   "source": [
    "def gradient_descent(parameters, gradients, lr=1e-3):\n",
    "    # Take a step\n",
    "    updated_params = []\n",
    "    for parameter, gradient in zip(parameters, gradients):\n",
    "        #TODO: \n",
    "        parameter -= lr * gradient\n",
    "        updated_params.append(parameter)\n",
    "    return updated_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-17T22:42:38.376344100Z",
     "start_time": "2024-02-17T22:42:38.358346100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters seem to update during backprop\n"
     ]
    }
   ],
   "source": [
    "# Check that our gradient descent works\n",
    "# Ignore all the arcane unrolling of lists. All this function does is to check that your update function actually changes the parameters\n",
    "\n",
    "# Copy old parameters so we can check that they've updated\n",
    "prev_parameters = [parameter.copy() for parameter in parameters]\n",
    "\n",
    "# Use computed gradients to update parameters\n",
    "parameters = gradient_descent(parameters, gradients, lr=3e-4)\n",
    "\n",
    "# Check that parameters have updated\n",
    "parameters_delta = [parameter - prev_parameter for parameter, prev_parameter in zip(parameters, prev_parameters)]\n",
    "\n",
    "assert all([parameter.any() for parameter in parameters_delta]), \"parameters have not updated\"\n",
    "print(\"parameters seem to update during backprop\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the network\n",
    "**Exercise:** Implement all individual steps together to loop over the dataset. Train until you have a loss under 2 or an accuracy over 50%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-19T10:15:51.673023600Z",
     "start_time": "2024-02-19T10:13:39.502795100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, training loss: 3.7711596471569764, validation loss: 3.818351654802872\n",
      "Epoch 50, training loss: 2.770525815794774, validation loss: 2.8295015636012435\n",
      "Epoch 100, training loss: 2.1545927043427424, validation loss: 2.2106557185459255\n",
      "Epoch 150, training loss: 1.842155811446005, validation loss: 1.8996491758877152\n",
      "Epoch 200, training loss: 1.656959262544754, validation loss: 1.7208400068464145\n",
      "Epoch 250, training loss: 1.5237981648512187, validation loss: 1.592329553222644\n",
      "Epoch 300, training loss: 1.4466558591701255, validation loss: 1.522029930527808\n",
      "Epoch 350, training loss: 1.412476912768723, validation loss: 1.4944508213554255\n",
      "Epoch 400, training loss: 1.3930642870499428, validation loss: 1.4798287045242033\n",
      "Epoch 450, training loss: 1.3793085233413294, validation loss: 1.4694083562512747\n",
      "Epoch 500, training loss: 1.3678937218027591, validation loss: 1.4602254207973697\n",
      "Epoch 550, training loss: 1.357314953257989, validation loss: 1.451007909403735\n",
      "Epoch 600, training loss: 1.3467560569307355, validation loss: 1.4410877499345558\n",
      "Epoch 650, training loss: 1.3356961821219886, validation loss: 1.430036409482398\n",
      "Epoch 700, training loss: 1.3237435524135641, validation loss: 1.4175161924975066\n",
      "Epoch 750, training loss: 1.3105503803915104, validation loss: 1.4032051471826166\n",
      "Epoch 800, training loss: 1.2957657502235524, validation loss: 1.3867579223249464\n",
      "Epoch 850, training loss: 1.2790612187923602, validation loss: 1.3678548763340401\n",
      "Epoch 900, training loss: 1.2605802086819244, validation loss: 1.3467893561429427\n",
      "Epoch 950, training loss: 1.2435003550547281, validation loss: 1.3277556124657597\n",
      "Epoch 1000, training loss: 1.2353015773051095, validation loss: 1.3204986052741072\n",
      "Epoch 1050, training loss: 1.2348353207653195, validation loss: 1.323173606755402\n",
      "Epoch 1100, training loss: 1.2368107259070886, validation loss: 1.32850848247816\n",
      "Epoch 1150, training loss: 1.238686617547032, validation loss: 1.3332090322543664\n",
      "Epoch 1200, training loss: 1.23938552259381, validation loss: 1.3360060010956674\n",
      "Epoch 1250, training loss: 1.2384308105257043, validation loss: 1.3364093689786372\n",
      "Epoch 1300, training loss: 1.2356130541931902, validation loss: 1.3342599861895603\n",
      "Epoch 1350, training loss: 1.230827676218103, validation loss: 1.32952459183423\n",
      "Epoch 1400, training loss: 1.2240507299819723, validation loss: 1.3222645155335253\n",
      "Epoch 1450, training loss: 1.2154701056138586, validation loss: 1.3127950162397393\n",
      "Epoch 1500, training loss: 1.205760789957425, validation loss: 1.3020247368095252\n",
      "Epoch 1550, training loss: 1.1963652516765362, validation loss: 1.2918063759840643\n",
      "Epoch 1600, training loss: 1.1892248388121422, validation loss: 1.2845994777578265\n",
      "Epoch 1650, training loss: 1.1853941426192578, validation loss: 1.2817094481464044\n",
      "Epoch 1700, training loss: 1.1840144964489405, validation loss: 1.2819777623751887\n",
      "Epoch 1750, training loss: 1.183344170111256, validation loss: 1.2831256378333347\n",
      "Epoch 1800, training loss: 1.1821068073319654, validation loss: 1.2835026175796296\n",
      "Epoch 1850, training loss: 1.1797877596006656, validation loss: 1.2824549803669207\n",
      "Epoch 1900, training loss: 1.1763887688140184, validation loss: 1.2799938571545273\n",
      "Epoch 1950, training loss: 1.17215490050611, validation loss: 1.2764373032957619\n",
      "Input sentence....: ['a', 'a', 'a', 'a', 'a', 'a', 'a', 'b', 'b', 'b', 'b', 'b', 'b', 'b']\n",
      "Target sequence...: ['a', 'a', 'a', 'a', 'a', 'a', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'EOS']\n",
      "Predicted sequence: ['a', 'a', 'a', 'a', 'a', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'EOS']\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Hyper-parameters\n",
    "num_epochs = 5000\n",
    "hidden_size = 50\n",
    "\n",
    "# Initialize a new network\n",
    "parameters = init_network(hidden_size=hidden_size, vocab_size=vocab_size)\n",
    "\n",
    "# Initialize hidden state as zeros\n",
    "hidden_state = np.zeros((hidden_size, 1))\n",
    "\n",
    "# Track loss\n",
    "training_loss, validation_loss = [], []\n",
    "\n",
    "# Keep track of best validation loss\n",
    "min_loss = 10000\n",
    "\n",
    "\n",
    "# For each epoch\n",
    "for i in range(num_epochs):\n",
    "    \n",
    "    # Track loss\n",
    "    epoch_training_loss = 0\n",
    "    epoch_validation_loss = 0\n",
    "    \n",
    "     # For each sentence in validation set\n",
    "    for inputs, targets in validation_set:\n",
    "        \n",
    "        # One-hot encode input and target sequence\n",
    "        inputs_one_hot = one_hot_encode_sequence(inputs, vocab_size, char_to_idx)\n",
    "        targets_one_hot = one_hot_encode_sequence(targets, vocab_size, char_to_idx)\n",
    "        \n",
    "        # Re-initialize hidden state\n",
    "        hidden_state = np.zeros_like(hidden_state)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs, hidden_states = forward_pass(inputs_one_hot, hidden_state, parameters)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss, grads = backward_pass(inputs_one_hot, outputs, hidden_states, targets_one_hot, parameters)\n",
    "        \n",
    "        # Update loss\n",
    "        epoch_validation_loss += loss\n",
    "        \n",
    "    # If lowest val loss, save parameters of model\n",
    "    if epoch_validation_loss < min_loss:\n",
    "        min_loss = epoch_validation_loss\n",
    "        best_parameters = parameters\n",
    "        \n",
    "    \n",
    "    \n",
    "    # For each sentence in training set\n",
    "    for inputs, targets in training_set:\n",
    "        \n",
    "        # One-hot encode input and target sequence\n",
    "        inputs_one_hot = one_hot_encode_sequence(inputs, vocab_size, char_to_idx)\n",
    "        targets_one_hot = one_hot_encode_sequence(targets, vocab_size, char_to_idx)\n",
    "        \n",
    "        # Re-initialize hidden state\n",
    "        hidden_state = np.zeros_like(hidden_state)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs, hidden_states = forward_pass(inputs_one_hot, hidden_state, parameters)\n",
    "\n",
    "        # Backward pass\n",
    "        loss, grads = backward_pass(inputs_one_hot, outputs, hidden_states, targets_one_hot, parameters)\n",
    "        \n",
    "        # Update parameters\n",
    "        parameters = gradient_descent(parameters, grads)\n",
    "        \n",
    "        # Update loss\n",
    "        epoch_training_loss += loss\n",
    "        \n",
    "        if np.isnan(loss):\n",
    "            raise ValueError('Gradients have vanished!')\n",
    "        \n",
    "    \n",
    "        \n",
    "        \n",
    "        \n",
    "    # Save loss for plot\n",
    "    training_loss.append(epoch_training_loss/len(training_set))\n",
    "    validation_loss.append(epoch_validation_loss/len(validation_set))\n",
    "\n",
    "    # Print loss every 100 epochs\n",
    "    if i % 50 == 0:\n",
    "        print(f'Epoch {i}, training loss: {training_loss[-1]}, validation loss: {validation_loss[-1]}')\n",
    "\n",
    "parameters = best_parameters\n",
    "# Get first sentence in train set\n",
    "inputs, targets = training_set[0]\n",
    "\n",
    "# One-hot encode input and target sequence\n",
    "inputs_one_hot = one_hot_encode_sequence(inputs, vocab_size, char_to_idx)\n",
    "targets_one_hot = one_hot_encode_sequence(targets, vocab_size, char_to_idx)\n",
    "\n",
    "# Initialize hidden state as zeros\n",
    "hidden_state = np.zeros((hidden_size, 1))\n",
    "\n",
    "# Forward pass\n",
    "outputs, hidden_states = forward_pass(inputs_one_hot, hidden_state, parameters)\n",
    "\n",
    "print('Input sentence....:', inputs)\n",
    "print('Target sequence...:', targets)\n",
    "print('Predicted sequence:', [idx_to_char[np.argmax(output)] for output in outputs])\n",
    "\n",
    "# Plot training and validation loss\n",
    "epoch = np.arange(len(training_loss))\n",
    "plt.figure()\n",
    "plt.plot(epoch, training_loss, 'r', label='Training loss',)\n",
    "plt.plot(epoch, validation_loss, 'b', label='Validation loss')\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch'), plt.ylabel('NLL')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-19T10:32:28.532863100Z",
     "start_time": "2024-02-19T10:32:28.511865500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sequence:     ['a', 'b']\n",
      "Target sequence:    ['b', 'EOS']\n",
      "Predicted sequence: ['a', 'b']\n",
      "Test accuracy is 0.0%.\n",
      "\n",
      "Input sequence:     ['a', 'a', 'a', 'a', 'a', 'a', 'a', 'b', 'b', 'b', 'b', 'b', 'b', 'b']\n",
      "Target sequence:    ['a', 'a', 'a', 'a', 'a', 'a', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'EOS']\n",
      "Predicted sequence: ['a', 'a', 'a', 'a', 'a', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'EOS']\n",
      "Test accuracy is 93.0%.\n",
      "\n",
      "Input sequence:     ['a', 'a', 'a', 'a', 'a', 'a', 'a', 'b', 'b', 'b', 'b', 'b', 'b', 'b']\n",
      "Target sequence:    ['a', 'a', 'a', 'a', 'a', 'a', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'EOS']\n",
      "Predicted sequence: ['a', 'a', 'a', 'a', 'a', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'EOS']\n",
      "Test accuracy is 93.0%.\n",
      "\n",
      "Input sequence:     ['a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b']\n",
      "Target sequence:    ['a', 'a', 'a', 'a', 'a', 'a', 'a', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'EOS']\n",
      "Predicted sequence: ['a', 'a', 'a', 'a', 'a', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'EOS', 'EOS']\n",
      "Test accuracy is 81.0%.\n",
      "\n",
      "Input sequence:     ['a', 'a', 'a', 'a', 'a', 'b', 'b', 'b', 'b', 'b']\n",
      "Target sequence:    ['a', 'a', 'a', 'a', 'b', 'b', 'b', 'b', 'b', 'EOS']\n",
      "Predicted sequence: ['a', 'a', 'a', 'a', 'a', 'b', 'b', 'b', 'b', 'b']\n",
      "Test accuracy is 80.0%.\n",
      "\n",
      "Input sequence:     ['a', 'a', 'a', 'b', 'b', 'b']\n",
      "Target sequence:    ['a', 'a', 'b', 'b', 'b', 'EOS']\n",
      "Predicted sequence: ['a', 'a', 'a', 'b', 'b', 'b']\n",
      "Test accuracy is 67.0%.\n",
      "\n",
      "Input sequence:     ['a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b']\n",
      "Target sequence:    ['a', 'a', 'a', 'a', 'a', 'a', 'a', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'EOS']\n",
      "Predicted sequence: ['a', 'a', 'a', 'a', 'a', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'EOS', 'EOS']\n",
      "Test accuracy is 81.0%.\n",
      "\n",
      "Input sequence:     ['a', 'a', 'a', 'a', 'a', 'a', 'b', 'b', 'b', 'b', 'b', 'b']\n",
      "Target sequence:    ['a', 'a', 'a', 'a', 'a', 'b', 'b', 'b', 'b', 'b', 'b', 'EOS']\n",
      "Predicted sequence: ['a', 'a', 'a', 'a', 'a', 'b', 'b', 'b', 'b', 'b', 'b', 'EOS']\n",
      "Test accuracy is 100.0%.\n",
      "\n",
      "Input sequence:     ['a', 'a', 'a', 'b', 'b', 'b']\n",
      "Target sequence:    ['a', 'a', 'b', 'b', 'b', 'EOS']\n",
      "Predicted sequence: ['a', 'a', 'a', 'b', 'b', 'b']\n",
      "Test accuracy is 67.0%.\n",
      "\n",
      "Input sequence:     ['a', 'b']\n",
      "Target sequence:    ['b', 'EOS']\n",
      "Predicted sequence: ['a', 'b']\n",
      "Test accuracy is 0.0%.\n",
      "\n",
      "\n",
      "Test accuracy is 66.2%.\n"
     ]
    }
   ],
   "source": [
    "# Testing your network\n",
    "accuracy_lst = []\n",
    "\n",
    "# Get first sequence in testing set\n",
    "for test_input_sequence, test_target_sequence in test_set:\n",
    "\n",
    "\n",
    "    # One-hot encode input and target sequence\n",
    "    test_input = one_hot_encode_sequence(test_input_sequence, vocab_size, char_to_idx)\n",
    "    test_target = one_hot_encode_sequence(test_target_sequence, vocab_size, char_to_idx)\n",
    "    \n",
    "    # Initialize hidden state as zeros\n",
    "    hidden_state = np.zeros((hidden_size, 1))\n",
    "\n",
    "    # Forward pass\n",
    "    # TODO:\n",
    "    outputs, hidden_states = forward_pass(test_input, hidden_state, parameters)\n",
    "\n",
    "\n",
    "\n",
    "    preds = [idx_to_char[np.argmax(output)] for output in outputs]\n",
    "    print('Input sequence:    ', test_input_sequence)\n",
    "    print('Target sequence:   ', test_target_sequence) \n",
    "    print('Predicted sequence:', preds)\n",
    "    \n",
    "    \n",
    "    accuracy = 0\n",
    "    \n",
    "    for target, pred in zip(test_target_sequence, preds):\n",
    "        accuracy += target == pred\n",
    "    accuracy /= len(test_target_sequence)/100\n",
    "\n",
    "    print(f\"Test accuracy is {np.round(accuracy)}%.\") \n",
    "    print()\n",
    "    accuracy_lst.append(accuracy)\n",
    " \n",
    "    \n",
    "print()\n",
    "print(f\"Test accuracy is {(sum(accuracy_lst)/len(accuracy_lst)):.1f}%.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying a more complex task\n",
    "The you dataset is an extremely simple set of characters to predict. Now, let's try using a more complicated dataset. The code below imports data from a collection of Shakespeare's plays and turns it into sequences of characters. Try running your network on this data and see if it works.\n",
    "\n",
    "**Exercise:** Paste and run your training and testing on the Shakespeare dataset. You don't have to achieve any specific loss or accuracy. Does it work? Please speculate on why/why not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your answer here: It is much harder to predict this cause of the letters increased from two to 26. The messages are also much longer. This makes it alot harder to predict if the algorithm don't fully recognice what is happening. If it did fully understand what was different between the target and input it would not have mattered. But because it doesn't the more states that it has the harder it becomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-19T10:34:44.449213300Z",
     "start_time": "2024-02-19T10:34:44.161284700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A sentence from the Shakespeare dataset:\n",
      "We have 80 samples in the training set.\n",
      "We have 10 samples in the validation set.\n",
      "We have 10 samples in the test set.\n"
     ]
    }
   ],
   "source": [
    "sequences, char_to_idx, idx_to_char, num_sequences, vocab_size = set_up_sequences()\n",
    "\n",
    "#num_sequences = len(sequences)\n",
    "#vocab_size = len(np.unique(sequences))\n",
    "print('A sentence from the Shakespeare dataset:')\n",
    "\n",
    "# Whole dataset is too big to effectively train on, so let's start by grabbing the first 100 sequences\n",
    "sequences = sequences[0:100]\n",
    "training_set, validation_set, test_set = set_up_datasets(sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Your code for training and testing here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-19T11:29:55.488847800Z",
     "start_time": "2024-02-19T10:34:51.359285300Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, training loss: 11.614730549376304, validation loss: 11.133895925208044\n",
      "Epoch 50, training loss: 11.305653370553703, validation loss: 10.859694811032716\n",
      "Epoch 100, training loss: 10.795227060360634, validation loss: 10.397436082484507\n",
      "Epoch 150, training loss: 10.076146915071709, validation loss: 9.681835329475343\n",
      "Epoch 200, training loss: 9.76651212201909, validation loss: 9.32214601162382\n",
      "Epoch 250, training loss: 9.643311352405343, validation loss: 9.171022195816363\n",
      "Epoch 300, training loss: 9.57038878428074, validation loss: 9.087622305891035\n",
      "Epoch 350, training loss: 9.519404012760122, validation loss: 9.032909226205176\n",
      "Epoch 400, training loss: 9.479900336724546, validation loss: 8.992751909663703\n",
      "Epoch 450, training loss: 9.446843572895956, validation loss: 8.960646894949601\n",
      "Epoch 500, training loss: 9.417623130738196, validation loss: 8.933265707576961\n",
      "Epoch 550, training loss: 9.390785172238932, validation loss: 8.90878317756707\n",
      "Epoch 600, training loss: 9.365463947111776, validation loss: 8.886138124423548\n",
      "Epoch 650, training loss: 9.341121092770226, validation loss: 8.864686840003078\n",
      "Epoch 700, training loss: 9.317418967346303, validation loss: 8.844031815069432\n",
      "Epoch 750, training loss: 9.294156754616541, validation loss: 8.823932334630411\n",
      "Epoch 800, training loss: 9.271241779820382, validation loss: 8.804255886311498\n",
      "Epoch 850, training loss: 9.248689067613237, validation loss: 8.784953377655668\n",
      "Epoch 900, training loss: 9.226661527212709, validation loss: 8.766054050643001\n",
      "Epoch 950, training loss: 9.205868308751459, validation loss: 8.747630600878974\n",
      "Epoch 1000, training loss: 9.270080292026986, validation loss: 8.728673368268938\n",
      "Epoch 1050, training loss: 9.261691818013134, validation loss: 8.709796112265181\n",
      "Epoch 1100, training loss: 9.241597963051731, validation loss: 8.692753528435423\n",
      "Epoch 1150, training loss: 9.276490124617144, validation loss: 8.678845124388216\n",
      "Epoch 1200, training loss: 9.254232439942149, validation loss: 9.873756212028791\n",
      "Epoch 1250, training loss: 9.238513802548528, validation loss: 9.847718123640268\n",
      "Epoch 1300, training loss: 9.21144240354786, validation loss: 9.813920280210143\n",
      "Epoch 1350, training loss: 9.185382706014352, validation loss: 9.780604923354419\n",
      "Epoch 1400, training loss: 9.254221855514514, validation loss: 9.732533837014282\n",
      "Epoch 1450, training loss: 9.223635980764037, validation loss: 9.687216650391123\n",
      "Epoch 1500, training loss: 9.194568419579802, validation loss: 9.646605758731427\n",
      "Epoch 1550, training loss: 9.166363625181893, validation loss: 9.608142464157456\n",
      "Epoch 1600, training loss: 9.138748479423397, validation loss: 9.570620118898702\n",
      "Epoch 1650, training loss: 9.111605051383266, validation loss: 9.533491522898405\n",
      "Epoch 1700, training loss: 9.084863271825515, validation loss: 9.496420544145625\n",
      "Epoch 1750, training loss: 9.058445405459619, validation loss: 9.458957403937823\n",
      "Epoch 1800, training loss: 9.032218691826403, validation loss: 9.419913089686457\n",
      "Epoch 1850, training loss: 9.005833740325334, validation loss: 9.372374526035083\n",
      "Epoch 1900, training loss: 8.928182784571245, validation loss: 8.415329690118321\n",
      "Epoch 1950, training loss: 8.909752019232327, validation loss: 9.311459046535912\n",
      "Epoch 2000, training loss: 8.8907705877196, validation loss: 9.300170622389299\n",
      "Epoch 2050, training loss: 8.870522840714475, validation loss: 9.279710339076244\n",
      "Epoch 2100, training loss: 8.851071644921912, validation loss: 9.24176961187929\n",
      "Epoch 2150, training loss: 8.831805283181046, validation loss: 8.34679090340806\n",
      "Epoch 2200, training loss: 8.812472314724012, validation loss: 8.33268628462918\n",
      "Epoch 2250, training loss: 8.792758138728441, validation loss: 8.31969773806133\n",
      "Epoch 2300, training loss: 8.746346376650632, validation loss: 8.307208249191982\n",
      "Epoch 2350, training loss: 8.751319906403285, validation loss: 8.295410839571172\n",
      "Epoch 2400, training loss: 8.714351397597213, validation loss: 8.28400968867179\n",
      "Epoch 2450, training loss: 8.699316055907298, validation loss: 8.27289045407839\n",
      "Epoch 2500, training loss: 8.683748788149462, validation loss: 8.262125157798682\n",
      "Epoch 2550, training loss: 8.667733030179765, validation loss: 8.250713463906994\n",
      "Epoch 2600, training loss: 8.652610456358783, validation loss: 8.239763304892971\n",
      "Epoch 2650, training loss: 8.74887580710464, validation loss: 8.22949923268615\n",
      "Epoch 2700, training loss: 8.73106802302527, validation loss: 8.219629758161648\n",
      "Epoch 2750, training loss: 8.602851733208631, validation loss: 8.21006900027546\n",
      "Epoch 2800, training loss: 8.587181035046743, validation loss: 8.200817048013056\n",
      "Epoch 2850, training loss: 8.570666792098793, validation loss: 8.19180852616098\n",
      "Epoch 2900, training loss: 8.656492770058533, validation loss: 8.182968009542018\n",
      "Epoch 2950, training loss: 8.538772405679923, validation loss: 8.174573228734786\n",
      "Epoch 3000, training loss: 8.62956965805244, validation loss: 8.166588447506351\n",
      "Epoch 3050, training loss: 8.615893560745016, validation loss: 8.15890719347864\n",
      "Epoch 3100, training loss: 8.602138932767811, validation loss: 8.15142846931276\n",
      "Epoch 3150, training loss: 8.587784554823722, validation loss: 8.144061285242987\n",
      "Epoch 3200, training loss: 8.573415546515472, validation loss: 8.136407549589569\n",
      "Epoch 3250, training loss: 8.562400537590168, validation loss: 8.12910484369545\n",
      "Epoch 3300, training loss: 8.551061013742178, validation loss: 8.122102500843775\n",
      "Epoch 3350, training loss: 8.539621884936215, validation loss: 8.115298073264317\n",
      "Epoch 3400, training loss: 8.528166927562037, validation loss: 8.108662032336131\n",
      "Epoch 3450, training loss: 8.516657309118015, validation loss: 8.10220326665303\n",
      "Epoch 3500, training loss: 8.504843185717485, validation loss: 8.095962152951815\n",
      "Epoch 3550, training loss: 8.492746733530542, validation loss: 8.089889158004725\n",
      "Epoch 3600, training loss: 8.480671095934664, validation loss: 8.083929402902069\n",
      "Epoch 3650, training loss: 8.468676177805595, validation loss: 8.078086885102843\n",
      "Epoch 3700, training loss: 8.456046408885758, validation loss: 8.07231790113927\n",
      "Epoch 3750, training loss: 8.44470844990106, validation loss: 8.06605345211933\n",
      "Epoch 3800, training loss: 8.436037835189133, validation loss: 8.060071049915909\n",
      "Epoch 3850, training loss: 8.427543174306656, validation loss: 8.054686694539868\n",
      "Epoch 3900, training loss: 8.418998827414526, validation loss: 8.04951322657716\n",
      "Epoch 3950, training loss: 8.410432643421569, validation loss: 8.044499395244257\n",
      "Epoch 4000, training loss: 8.401897420865968, validation loss: 8.039642780753395\n",
      "Epoch 4050, training loss: 8.393434256010627, validation loss: 8.034935894392886\n",
      "Epoch 4100, training loss: 8.385066169839657, validation loss: 8.030368260500275\n",
      "Epoch 4150, training loss: 8.376807974765502, validation loss: 8.02593021573334\n",
      "Epoch 4200, training loss: 8.368672570777825, validation loss: 8.021613843907963\n",
      "Epoch 4250, training loss: 8.360671757103619, validation loss: 8.01741271784551\n",
      "Epoch 4300, training loss: 8.35281577827012, validation loss: 8.013321809094865\n",
      "Epoch 4350, training loss: 8.345115456085704, validation loss: 8.009338506131659\n",
      "Epoch 4400, training loss: 8.337590153726195, validation loss: 8.005466043619782\n",
      "Epoch 4450, training loss: 8.330272475028005, validation loss: 8.001717472065822\n",
      "Epoch 4500, training loss: 8.323160718633954, validation loss: 7.998104253372435\n",
      "Epoch 4550, training loss: 8.316188353375422, validation loss: 7.994621850291864\n",
      "Epoch 4600, training loss: 8.309292758221053, validation loss: 7.991259955290542\n",
      "Epoch 4650, training loss: 8.302446070352357, validation loss: 7.988006449853813\n",
      "Epoch 4700, training loss: 8.295641635361072, validation loss: 7.984848954043111\n",
      "Epoch 4750, training loss: 8.288880215233295, validation loss: 7.981777215377766\n",
      "Epoch 4800, training loss: 8.282161906746389, validation loss: 7.978782476113276\n",
      "Epoch 4850, training loss: 8.275481409336518, validation loss: 7.975855166027115\n",
      "Epoch 4900, training loss: 8.268821815801154, validation loss: 7.972982455462422\n",
      "Epoch 4950, training loss: 8.26213855956511, validation loss: 7.970145194269874\n",
      "Input sentence....: ['f', 'i', 'r', 's', 't', 'c', 'i', 't', 'i', 'z', 'e', 'n', 'b', 'e', 'f', 'o', 'r', 'e', 'w', 'e', 'p', 'r', 'o', 'c', 'e', 'e', 'd', 'a', 'n', 'y', 'f', 'u', 'r', 't', 'h', 'e', 'r', 'h', 'e', 'a', 'r', 'm', 'e', 's', 'p', 'e', 'a', 'k']\n",
      "Target sequence...: ['i', 'r', 's', 't', 'c', 'i', 't', 'i', 'z', 'e', 'n', 'b', 'e', 'f', 'o', 'r', 'e', 'w', 'e', 'p', 'r', 'o', 'c', 'e', 'e', 'd', 'a', 'n', 'y', 'f', 'u', 'r', 't', 'h', 'e', 'r', 'h', 'e', 'a', 'r', 'm', 'e', 's', 'p', 'e', 'a', 'k', 'EOS']\n",
      "Predicted sequence: ['i', 'r', 's', 't', 'c', 'i', 't', 'i', 'z', 'e', 'n', 'o', 'e', 'n', 'o', 'r', 'e', 's', 'o', 'r', 'e', 'e', 'u', 'i', 'n', 's', 'o', 'n', 'd', 'o', 'o', 'r', 's', 'h', 'e', 'r', 'e', 'e', 'n', 'n', 'e', 'e', 'r', 't', 'e', 'r', 't', 'e']\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Hyper-parameters\n",
    "num_epochs = 5000\n",
    "hidden_size = 50\n",
    "\n",
    "# Initialize a new network\n",
    "parameters = init_network(hidden_size=hidden_size, vocab_size=vocab_size)\n",
    "\n",
    "# Initialize hidden state as zeros\n",
    "hidden_state = np.zeros((hidden_size, 1))\n",
    "\n",
    "# Track loss\n",
    "training_loss, validation_loss = [], []\n",
    "\n",
    "# Keep track of best validation loss\n",
    "min_loss = 10000\n",
    "\n",
    "\n",
    "# For each epoch\n",
    "for i in range(num_epochs):\n",
    "\n",
    "    # Track loss\n",
    "    epoch_training_loss = 0\n",
    "    epoch_validation_loss = 0\n",
    "\n",
    "     # For each sentence in validation set\n",
    "    for inputs, targets in validation_set:\n",
    "\n",
    "        # One-hot encode input and target sequence\n",
    "        inputs_one_hot = one_hot_encode_sequence(inputs, vocab_size, char_to_idx)\n",
    "        targets_one_hot = one_hot_encode_sequence(targets, vocab_size, char_to_idx)\n",
    "\n",
    "        # Re-initialize hidden state\n",
    "        hidden_state = np.zeros_like(hidden_state)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs, hidden_states = forward_pass(inputs_one_hot, hidden_state, parameters)\n",
    "\n",
    "        # Backward pass\n",
    "        loss, grads = backward_pass(inputs_one_hot, outputs, hidden_states, targets_one_hot, parameters)\n",
    "\n",
    "        # Update loss\n",
    "        epoch_validation_loss += loss\n",
    "\n",
    "    # If lowest val loss, save parameters of model\n",
    "    if epoch_validation_loss < min_loss:\n",
    "        min_loss = epoch_validation_loss\n",
    "        best_parameters = parameters\n",
    "\n",
    "\n",
    "\n",
    "    # For each sentence in training set\n",
    "    for inputs, targets in training_set:\n",
    "\n",
    "        # One-hot encode input and target sequence\n",
    "        inputs_one_hot = one_hot_encode_sequence(inputs, vocab_size, char_to_idx)\n",
    "        targets_one_hot = one_hot_encode_sequence(targets, vocab_size, char_to_idx)\n",
    "\n",
    "\n",
    "        # Re-initialize hidden state\n",
    "        hidden_state = np.zeros_like(hidden_state)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs, hidden_states = forward_pass(inputs_one_hot, hidden_state, parameters)\n",
    "\n",
    "        # Backward pass\n",
    "        loss, grads = backward_pass(inputs_one_hot, outputs, hidden_states, targets_one_hot, parameters)\n",
    "\n",
    "        # Update parameters\n",
    "        parameters = gradient_descent(parameters, grads)\n",
    "\n",
    "        # Update loss\n",
    "        epoch_training_loss += loss\n",
    "\n",
    "        if np.isnan(loss):\n",
    "            raise ValueError('Gradients have vanished!')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Save loss for plot\n",
    "    training_loss.append(epoch_training_loss/len(training_set))\n",
    "    validation_loss.append(epoch_validation_loss/len(validation_set))\n",
    "\n",
    "    # Print loss every 100 epochs\n",
    "    if i % 50 == 0:\n",
    "        print(f'Epoch {i}, training loss: {training_loss[-1]}, validation loss: {validation_loss[-1]}')\n",
    "\n",
    "parameters = best_parameters\n",
    "# Get first sentence in train set\n",
    "inputs, targets = training_set[0]\n",
    "\n",
    "# One-hot encode input and target sequence\n",
    "inputs_one_hot = one_hot_encode_sequence(inputs, vocab_size, char_to_idx)\n",
    "targets_one_hot = one_hot_encode_sequence(targets, vocab_size, char_to_idx)\n",
    "\n",
    "# Initialize hidden state as zeros\n",
    "hidden_state = np.zeros((hidden_size, 1))\n",
    "\n",
    "# Forward pass\n",
    "outputs, hidden_states = forward_pass(inputs_one_hot, hidden_state, parameters)\n",
    "\n",
    "print('Input sentence....:', inputs)\n",
    "print('Target sequence...:', targets)\n",
    "print('Predicted sequence:', [idx_to_char[np.argmax(output)] for output in outputs])\n",
    "\n",
    "# Plot training and validation loss\n",
    "epoch = np.arange(len(training_loss))\n",
    "plt.figure()\n",
    "plt.plot(epoch, training_loss, 'r', label='Training loss',)\n",
    "plt.plot(epoch, validation_loss, 'b', label='Validation loss')\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch'), plt.ylabel('NLL')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3 - LSTMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most famous challenges for RNNs is [the vanishing gradient problem](http://neuralnetworksanddeeplearning.com/chap5.html#the_vanishing_gradient_problem), where the gradient, i.e. the signal to update our weights, goes to zero due to repetetive multiplications of values <1 in the backprop (or goes to oo due to repetetive multiplications of values >1 in what's called exploding gradient). This can happen to all networks, but RNNs are far more vulnurable to it due to the long sequences of backprops through time. In this section of the lab you'll see how this issue can be addressed, and \n",
    "\n",
    "Please read through [Christopher Olah's walk through](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) to understand this issue and its solutions in more detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Vanilla RNN](http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-SimpleRNN.png)\n",
    "![Terminology](http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM2-notation.png)\n",
    "\n",
    "The image above represents a normal \"vanilla\" RNN, with the yellow \"tanh\" rectangle symbolising our current network. $h_t$ represents the output in this image, so as you can see there's the x->hidden state connection, hidden state update connection, and hidden state->output connection. \n",
    "\n",
    "To amend the issue of vanishing gradients, gated hidden units were introduced, which act like a highway between states to facilitate modelling long-range dependencies. Two famous solutions exist, one called \"long short-term memory\" (LSTM) and the other \"gated recurrent unit\" (GRU). Today, we're going to look a bit closer at the LSTM (don't worry though, you're using pytorch to implement it this time).\n",
    "\n",
    "The image below, fully explained in the walk through, shows the schematic of an LSTM cell. The fundamentals from our vanilla RNN are still there, namely the tanh of our input and previous hidden state, but we now have a \"cell state\", which acts as the aforementioned highway, and three \"gates\" which update this cell state.\n",
    "\n",
    "The three gates are the input gate $i$, the forget gate $f$, and the output gate $o$. They are defined as \n",
    "\\begin{equation*}\n",
    "\\begin{aligned}\n",
    "i = \\sigma ( W^i [h_{t-1}, x_t])\\\\\n",
    "f = \\sigma ( W^f [h_{t-1},x_t])\\\\\n",
    "o = \\sigma ( W^o [h_{t-1},x_t])\n",
    "\\end{aligned}\n",
    "\\tag{2}\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "where $W^i$, $W^f$, and $W^o$ are the weight matrices applied to $h_{t-1},x_t$.\n",
    "\n",
    "![LSTM](http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Read the walk through above and explain the function of the three gates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write your answer here:\n",
    "- forget gate: network decides when to forget a hidden state value\n",
    "- input gate: network decides when to remember an input in hidden state\n",
    "- output gate: network decides when to output a hidden state value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using an LSTM with Pytorch\n",
    "Now for your final task. Having learned how LSTMs work in theory, it's now time to put this knowledge into practice.\n",
    "\n",
    "Using the [LSTM pytorch documentations](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html), build and train an LSTM network and compare it to your self-made RNN. You won't need to import any more packages than those already imported, but you're free to use any approach as long as you can defend it during the presentation.\n",
    "\n",
    "**Exercise:** Build the LSTM network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-19T11:35:56.024513800Z",
     "start_time": "2024-02-19T11:35:56.006512300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, vocab_size=4, hidden_size = 25):\n",
    "        super(LSTM, self).__init__()\n",
    "        num_layers = 1\n",
    "        # Recurrent layer\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=vocab_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            bias=True,\n",
    "            bidirectional=False)\n",
    "\n",
    "        # Output layer\n",
    "        self.l_out = nn.Linear(in_features=hidden_size,\n",
    "                            out_features=vocab_size,\n",
    "                            bias=True)\n",
    "\n",
    "    def forward(self, x_seq):\n",
    "        # RNN returns output and last hidden state\n",
    "        h_seq, _ = self.lstm(x_seq)\n",
    "\n",
    "        # Flatten output for feed-forward layer\n",
    "        h_seq = h_seq.view(-1, self.lstm.hidden_size)\n",
    "        # Output layer\n",
    "        y_seq = self.l_out(h_seq)\n",
    "        return y_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the network hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-19T13:00:44.907472200Z",
     "start_time": "2024-02-19T12:49:26.036465400Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best 337.9017734527588\n",
      "Epoch  0, training loss: 26.198208957910538, validation loss: 33.79017734527588, correct 0.0%\n",
      "Best 220.28423595428467\n",
      "Best 170.33819198608398\n",
      "Best 145.96120643615723\n",
      "Best 132.79995155334473\n",
      "Best 125.09611129760742\n",
      "Epoch  5, training loss: 11.942869752645493, validation loss: 12.509611129760742, correct 83.9%\n",
      "Best 119.17232513427734\n",
      "Best 111.44455623626709\n",
      "Best 104.12984848022461\n",
      "Best 96.83576107025146\n",
      "Best 88.22494983673096\n",
      "Epoch 10, training loss: 8.474828559160233, validation loss: 8.822494983673096, correct 86.3%\n",
      "Best 79.91858673095703\n",
      "Best 75.15499496459961\n",
      "Best 69.34554862976074\n",
      "Best 65.66763114929199\n",
      "Best 62.512502670288086\n",
      "Epoch 15, training loss: 6.428520506620407, validation loss: 6.251250267028809, correct 93.5%\n",
      "Best 59.88538312911987\n",
      "Best 57.78392171859741\n",
      "Best 56.06846570968628\n",
      "Best 54.63923501968384\n",
      "Best 53.43217706680298\n",
      "Epoch 20, training loss: 5.654812353849411, validation loss: 5.3432177066802975, correct 93.5%\n",
      "Best 52.402482986450195\n",
      "Best 51.51625919342041\n",
      "Best 50.746410846710205\n",
      "Best 50.07087278366089\n",
      "Best 49.47221660614014\n",
      "Epoch 25, training loss: 5.283985412120819, validation loss: 4.947221660614014, correct 93.5%\n",
      "Best 48.93732929229736\n",
      "Best 48.4567437171936\n",
      "Best 48.023802280426025\n",
      "Best 47.63373899459839\n",
      "Best 47.282888412475586\n",
      "Epoch 30, training loss: 5.063790160417557, validation loss: 4.728288841247559, correct 93.5%\n",
      "Best 46.967825412750244\n",
      "Best 46.68486547470093\n",
      "Best 46.430102825164795\n",
      "Best 46.19983243942261\n",
      "Best 45.9909086227417\n",
      "Epoch 35, training loss: 4.924271500110626, validation loss: 4.59909086227417, correct 93.5%\n",
      "Best 45.8006477355957\n",
      "Best 45.6267352104187\n",
      "Best 45.46718454360962\n",
      "Best 45.3202862739563\n",
      "Best 45.184582233428955\n",
      "Epoch 40, training loss: 4.829966607689857, validation loss: 4.518458223342895, correct 93.5%\n",
      "Best 45.05880689620972\n",
      "Best 44.941855907440186\n",
      "Best 44.832741260528564\n",
      "Best 44.73056983947754\n",
      "Best 44.63451814651489\n",
      "Epoch 45, training loss: 4.762205734848976, validation loss: 4.463451814651489, correct 93.5%\n",
      "Best 44.54385566711426\n",
      "Best 44.458017349243164\n",
      "Best 44.37683582305908\n",
      "Best 44.30003261566162\n",
      "Best 44.226219177246094\n",
      "Epoch 50, training loss: 4.714949461817741, validation loss: 4.4226219177246096, correct 93.5%\n",
      "Best 44.186509132385254\n",
      "Best 44.08184623718262\n",
      "Best 44.00112724304199\n",
      "Best 43.962321758270264\n",
      "Epoch 55, training loss: 4.663529390096665, validation loss: 4.396232175827026, correct 93.5%\n",
      "Best 43.92561483383179\n",
      "Best 43.88865566253662\n",
      "Best 43.852017879486084\n",
      "Best 43.81630992889404\n",
      "Best 43.78183889389038\n",
      "Epoch 60, training loss: 4.633360260725022, validation loss: 4.378183889389038, correct 93.5%\n",
      "Best 43.74868726730347\n",
      "Best 43.716872692108154\n",
      "Best 43.68638229370117\n",
      "Best 43.65718984603882\n",
      "Best 43.62926530838013\n",
      "Epoch 65, training loss: 4.608636569976807, validation loss: 4.3629265308380125, correct 93.5%\n",
      "Best 43.602577686309814\n",
      "Best 43.57709455490112\n",
      "Best 43.55278491973877\n",
      "Best 43.5295844078064\n",
      "Best 43.50745248794556\n",
      "Epoch 70, training loss: 4.587865403294563, validation loss: 4.3507452487945555, correct 93.5%\n",
      "Best 43.48633003234863\n",
      "Best 43.46616554260254\n",
      "Best 43.446900367736816\n",
      "Best 43.42846918106079\n",
      "Best 43.41081953048706\n",
      "Epoch 75, training loss: 4.570295691490173, validation loss: 4.341081953048706, correct 93.5%\n",
      "Best 43.39389991760254\n",
      "Best 43.37765645980835\n",
      "Best 43.362038135528564\n",
      "Best 43.347010135650635\n",
      "Best 43.33252573013306\n",
      "Epoch 80, training loss: 4.555395427346229, validation loss: 4.333252573013306, correct 93.5%\n",
      "Best 43.31856679916382\n",
      "Best 43.305100440979004\n",
      "Best 43.29210805892944\n",
      "Best 43.279574394226074\n",
      "Best 43.267489433288574\n",
      "Epoch 85, training loss: 4.542666161060334, validation loss: 4.326748943328857, correct 93.5%\n",
      "Best 43.2558479309082\n",
      "Best 43.2446403503418\n",
      "Best 43.233861446380615\n",
      "Best 43.223504066467285\n",
      "Best 43.2135705947876\n",
      "Epoch 90, training loss: 4.531633287668228, validation loss: 4.32135705947876, correct 93.5%\n",
      "Best 43.204041481018066\n",
      "Best 43.194921016693115\n",
      "Best 43.18620204925537\n",
      "Best 43.17787981033325\n",
      "Best 43.16995620727539\n",
      "Epoch 95, training loss: 4.52192260324955, validation loss: 4.316995620727539, correct 93.5%\n",
      "Best 43.16243028640747\n",
      "Best 43.155292987823486\n",
      "Best 43.14855194091797\n",
      "Best 43.14220094680786\n",
      "Best 43.13622808456421\n",
      "Epoch 100, training loss: 4.5132659584283825, validation loss: 4.3136228084564205, correct 93.5%\n",
      "Best 43.130630016326904\n",
      "Best 43.12539720535278\n",
      "Best 43.12050247192383\n",
      "Best 43.115943908691406\n",
      "Best 43.11169910430908\n",
      "Epoch 105, training loss: 4.505437883734703, validation loss: 4.311169910430908, correct 93.5%\n",
      "Best 43.107750415802\n",
      "Best 43.10408878326416\n",
      "Best 43.10069561004639\n",
      "Best 43.09755325317383\n",
      "Best 43.09465169906616\n",
      "Epoch 110, training loss: 4.498273813724518, validation loss: 4.309465169906616, correct 93.5%\n",
      "Best 43.09198331832886\n",
      "Best 43.0895299911499\n",
      "Best 43.08728361129761\n",
      "Best 43.0852313041687\n",
      "Best 43.08336544036865\n",
      "Epoch 115, training loss: 4.491660803556442, validation loss: 4.308336544036865, correct 93.5%\n",
      "Best 43.08167219161987\n",
      "Best 43.0801477432251\n",
      "Best 43.07878255844116\n",
      "Best 43.07756519317627\n",
      "Best 43.076486110687256\n",
      "Epoch 120, training loss: 4.485515859723091, validation loss: 4.307648611068726, correct 93.5%\n",
      "Best 43.075544357299805\n",
      "Best 43.07472562789917\n",
      "Best 43.07402944564819\n",
      "Best 43.07343864440918\n",
      "Best 43.0729603767395\n",
      "Epoch 125, training loss: 4.479771250486374, validation loss: 4.30729603767395, correct 93.5%\n",
      "Best 43.07257604598999\n",
      "Best 43.072288036346436\n",
      "Best 43.0720911026001\n",
      "Best 43.071972370147705\n",
      "Best 43.071937084198\n",
      "Epoch 130, training loss: 4.47437330186367, validation loss: 4.3071937084198, correct 93.5%\n",
      "Epoch 135, training loss: 4.469279995560646, validation loss: 4.307275056838989, correct 93.5%\n",
      "Epoch 140, training loss: 4.464459446072579, validation loss: 4.307489585876465, correct 93.5%\n",
      "Epoch 145, training loss: 4.459887725114823, validation loss: 4.307797002792358, correct 93.5%\n",
      "Epoch 150, training loss: 4.455544218420982, validation loss: 4.30816535949707, correct 93.5%\n",
      "Epoch 155, training loss: 4.451409703493118, validation loss: 4.308567714691162, correct 93.5%\n",
      "Epoch 160, training loss: 4.447466960549354, validation loss: 4.308983659744262, correct 93.5%\n",
      "Epoch 165, training loss: 4.443700203299523, validation loss: 4.309393358230591, correct 93.5%\n",
      "Epoch 170, training loss: 4.440094739198685, validation loss: 4.309783172607422, correct 93.5%\n",
      "Epoch 175, training loss: 4.436634808778763, validation loss: 4.3101418018341064, correct 93.5%\n",
      "Epoch 180, training loss: 4.433299598097801, validation loss: 4.310453224182129, correct 93.5%\n",
      "Epoch 185, training loss: 4.430069160461426, validation loss: 4.310691213607788, correct 93.5%\n",
      "Epoch 190, training loss: 4.426935875415802, validation loss: 4.310813283920288, correct 93.5%\n",
      "Epoch 195, training loss: 4.423912867903709, validation loss: 4.31081256866455, correct 93.5%\n",
      "Epoch 200, training loss: 4.4210114777088165, validation loss: 4.310771226882935, correct 93.5%\n",
      "Epoch 205, training loss: 4.418213936686516, validation loss: 4.310791063308716, correct 93.5%\n",
      "Epoch 210, training loss: 4.4155027866363525, validation loss: 4.310871267318726, correct 93.5%\n",
      "Epoch 215, training loss: 4.412871301174164, validation loss: 4.3109790802001955, correct 93.5%\n",
      "Epoch 220, training loss: 4.410318803787232, validation loss: 4.311097097396851, correct 93.5%\n",
      "Epoch 225, training loss: 4.407845669984818, validation loss: 4.311217975616455, correct 93.5%\n",
      "Epoch 230, training loss: 4.405447861552238, validation loss: 4.311344146728516, correct 93.5%\n",
      "Epoch 235, training loss: 4.403118509054184, validation loss: 4.311477708816528, correct 93.5%\n",
      "Epoch 240, training loss: 4.400851878523826, validation loss: 4.311615371704102, correct 93.5%\n",
      "Epoch 245, training loss: 4.39864436686039, validation loss: 4.311754560470581, correct 93.5%\n",
      "Epoch 250, training loss: 4.396495661139488, validation loss: 4.311893033981323, correct 93.5%\n",
      "Epoch 255, training loss: 4.394409629702568, validation loss: 4.312031841278076, correct 93.5%\n",
      "Epoch 260, training loss: 4.3923925250768665, validation loss: 4.312171888351441, correct 93.5%\n",
      "Epoch 265, training loss: 4.390451371669769, validation loss: 4.312320637702942, correct 93.5%\n",
      "Epoch 270, training loss: 4.388592824339867, validation loss: 4.3124932765960695, correct 93.5%\n",
      "Epoch 275, training loss: 4.3868233323097225, validation loss: 4.31271345615387, correct 93.5%\n",
      "Epoch 280, training loss: 4.385148718953133, validation loss: 4.313010406494141, correct 93.5%\n",
      "Epoch 285, training loss: 4.383573961257935, validation loss: 4.313418960571289, correct 93.5%\n",
      "Epoch 290, training loss: 4.38210318684578, validation loss: 4.313972544670105, correct 93.5%\n",
      "Epoch 295, training loss: 4.380739006400108, validation loss: 4.314702987670898, correct 93.5%\n",
      "Epoch 300, training loss: 4.37948344051838, validation loss: 4.31563789844513, correct 93.5%\n",
      "Epoch 305, training loss: 4.378336802124977, validation loss: 4.3167977094650265, correct 93.5%\n",
      "Epoch 310, training loss: 4.377298447489738, validation loss: 4.318194770812989, correct 92.7%\n",
      "Epoch 315, training loss: 4.37636584341526, validation loss: 4.319832038879395, correct 92.7%\n",
      "Epoch 320, training loss: 4.375535240769386, validation loss: 4.32170033454895, correct 92.7%\n",
      "Epoch 325, training loss: 4.3748013287782666, validation loss: 4.323780679702759, correct 92.7%\n",
      "Epoch 330, training loss: 4.374156665802002, validation loss: 4.326044869422913, correct 92.7%\n",
      "Epoch 335, training loss: 4.373592504858971, validation loss: 4.328460669517517, correct 92.7%\n",
      "Epoch 340, training loss: 4.373098489642143, validation loss: 4.330992031097412, correct 92.7%\n",
      "Epoch 345, training loss: 4.372663813829422, validation loss: 4.333604264259338, correct 92.7%\n",
      "Epoch 350, training loss: 4.372278010845184, validation loss: 4.336262202262878, correct 92.7%\n",
      "Epoch 355, training loss: 4.371931114792824, validation loss: 4.338931584358216, correct 92.7%\n",
      "Epoch 360, training loss: 4.371614083647728, validation loss: 4.341580867767334, correct 92.7%\n",
      "Epoch 365, training loss: 4.371318885684014, validation loss: 4.34418089389801, correct 92.7%\n",
      "Epoch 370, training loss: 4.371039605140686, validation loss: 4.346707487106324, correct 92.7%\n",
      "Epoch 375, training loss: 4.370771279931068, validation loss: 4.349141073226929, correct 92.7%\n",
      "Epoch 380, training loss: 4.370509684085846, validation loss: 4.351466274261474, correct 92.7%\n",
      "Epoch 385, training loss: 4.370252254605293, validation loss: 4.353672742843628, correct 92.7%\n",
      "Epoch 390, training loss: 4.369997248053551, validation loss: 4.355755186080932, correct 92.7%\n",
      "Epoch 395, training loss: 4.369743338227272, validation loss: 4.357710552215576, correct 92.7%\n",
      "Epoch 400, training loss: 4.369489723443985, validation loss: 4.3595390796661375, correct 92.7%\n",
      "Epoch 405, training loss: 4.369236114621162, validation loss: 4.361243867874146, correct 92.7%\n",
      "Epoch 410, training loss: 4.368982303142547, validation loss: 4.362828063964844, correct 92.7%\n",
      "Epoch 415, training loss: 4.3687284111976625, validation loss: 4.3642974376678465, correct 92.7%\n",
      "Epoch 420, training loss: 4.368474549055099, validation loss: 4.365657305717468, correct 92.7%\n",
      "Epoch 425, training loss: 4.36822107732296, validation loss: 4.366914010047912, correct 92.7%\n",
      "Epoch 430, training loss: 4.367968213558197, validation loss: 4.368074631690979, correct 92.7%\n",
      "Epoch 435, training loss: 4.36771619617939, validation loss: 4.369146156311035, correct 92.7%\n",
      "Epoch 440, training loss: 4.367465230822563, validation loss: 4.370134425163269, correct 92.7%\n",
      "Epoch 445, training loss: 4.367215698957443, validation loss: 4.371046829223633, correct 92.7%\n",
      "Epoch 450, training loss: 4.366967949271202, validation loss: 4.371889066696167, correct 92.7%\n",
      "Epoch 455, training loss: 4.366721874475479, validation loss: 4.372667908668518, correct 92.7%\n",
      "Epoch 460, training loss: 4.366478082537651, validation loss: 4.373387932777405, correct 92.7%\n",
      "Epoch 465, training loss: 4.366236278414727, validation loss: 4.374055242538452, correct 92.7%\n",
      "Epoch 470, training loss: 4.365996950864792, validation loss: 4.374673843383789, correct 92.7%\n",
      "Epoch 475, training loss: 4.36575993001461, validation loss: 4.375248527526855, correct 92.7%\n",
      "Epoch 480, training loss: 4.365525421500206, validation loss: 4.37578318119049, correct 92.7%\n",
      "Epoch 485, training loss: 4.365293544530869, validation loss: 4.376280570030213, correct 92.7%\n",
      "Epoch 490, training loss: 4.365064093470574, validation loss: 4.376745080947876, correct 92.7%\n",
      "Epoch 495, training loss: 4.364837250113487, validation loss: 4.377178406715393, correct 92.7%\n",
      "Epoch 500, training loss: 4.364613124728203, validation loss: 4.37758367061615, correct 92.7%\n",
      "Epoch 505, training loss: 4.364391520619392, validation loss: 4.377963399887085, correct 92.7%\n",
      "Epoch 510, training loss: 4.364172649383545, validation loss: 4.378318238258362, correct 92.7%\n",
      "Epoch 515, training loss: 4.3639561295509335, validation loss: 4.378652334213257, correct 92.7%\n",
      "Epoch 520, training loss: 4.363742271065712, validation loss: 4.3789653301239015, correct 92.7%\n",
      "Epoch 525, training loss: 4.36353078186512, validation loss: 4.379259943962097, correct 92.7%\n",
      "Epoch 530, training loss: 4.363322082161903, validation loss: 4.379536652565003, correct 92.7%\n",
      "Epoch 535, training loss: 4.363115772604942, validation loss: 4.379797124862671, correct 92.7%\n",
      "Epoch 540, training loss: 4.362911760807037, validation loss: 4.380042839050293, correct 92.7%\n",
      "Epoch 545, training loss: 4.362710282206535, validation loss: 4.380274391174316, correct 92.7%\n",
      "Epoch 550, training loss: 4.362511229515076, validation loss: 4.380492925643921, correct 92.7%\n",
      "Epoch 555, training loss: 4.362314605712891, validation loss: 4.380699539184571, correct 92.7%\n",
      "Epoch 560, training loss: 4.362120255827904, validation loss: 4.380894255638123, correct 92.7%\n",
      "Epoch 565, training loss: 4.361928278207779, validation loss: 4.3810786485671995, correct 92.7%\n",
      "Epoch 570, training loss: 4.361738586425782, validation loss: 4.381252884864807, correct 92.7%\n",
      "Epoch 575, training loss: 4.36155119240284, validation loss: 4.381417608261108, correct 92.7%\n",
      "Epoch 580, training loss: 4.361366146802903, validation loss: 4.381574130058288, correct 92.7%\n",
      "Epoch 585, training loss: 4.361183217167854, validation loss: 4.381722497940063, correct 92.7%\n",
      "Epoch 590, training loss: 4.361002597212791, validation loss: 4.381863045692444, correct 92.7%\n",
      "Epoch 595, training loss: 4.360824182629585, validation loss: 4.3819968700408936, correct 92.7%\n",
      "Epoch 600, training loss: 4.360647970438004, validation loss: 4.38212399482727, correct 92.7%\n",
      "Epoch 605, training loss: 4.360473689436913, validation loss: 4.382245397567749, correct 92.7%\n",
      "Epoch 610, training loss: 4.360301333665848, validation loss: 4.382360434532165, correct 92.7%\n",
      "Epoch 615, training loss: 4.360130396485329, validation loss: 4.382468748092651, correct 92.7%\n",
      "Epoch 620, training loss: 4.35996005833149, validation loss: 4.382567405700684, correct 92.7%\n",
      "Epoch 625, training loss: 4.359790396690369, validation loss: 4.382656788825988, correct 92.7%\n",
      "Epoch 630, training loss: 4.359623304009437, validation loss: 4.382746267318725, correct 92.7%\n",
      "Epoch 635, training loss: 4.3594588309526445, validation loss: 4.382836437225341, correct 92.7%\n",
      "Epoch 640, training loss: 4.359296661615372, validation loss: 4.3829244613647464, correct 92.7%\n",
      "Epoch 645, training loss: 4.359136712551117, validation loss: 4.383009147644043, correct 92.7%\n",
      "Epoch 650, training loss: 4.358978846669197, validation loss: 4.383091902732849, correct 92.7%\n",
      "Epoch 655, training loss: 4.358823344111443, validation loss: 4.383170628547669, correct 92.7%\n",
      "Epoch 660, training loss: 4.3586696028709415, validation loss: 4.383246850967407, correct 92.7%\n",
      "Epoch 665, training loss: 4.358517959713936, validation loss: 4.383320951461792, correct 92.7%\n",
      "Epoch 670, training loss: 4.358368331193924, validation loss: 4.3833925247192385, correct 92.7%\n",
      "Epoch 675, training loss: 4.358220681548119, validation loss: 4.383461928367614, correct 92.7%\n",
      "Epoch 680, training loss: 4.358074909448623, validation loss: 4.3835286617279055, correct 92.7%\n",
      "Epoch 685, training loss: 4.357930815219879, validation loss: 4.38359375, correct 92.7%\n",
      "Epoch 690, training loss: 4.357788521051407, validation loss: 4.383656334877014, correct 92.7%\n",
      "Epoch 695, training loss: 4.357648041844368, validation loss: 4.383717155456543, correct 92.7%\n",
      "Epoch 700, training loss: 4.357509124279022, validation loss: 4.383775782585144, correct 92.7%\n",
      "Epoch 705, training loss: 4.357371664047241, validation loss: 4.383833217620849, correct 92.7%\n",
      "Epoch 710, training loss: 4.357236012816429, validation loss: 4.3838886499404905, correct 92.7%\n",
      "Epoch 715, training loss: 4.357102057337761, validation loss: 4.383942413330078, correct 92.7%\n",
      "Epoch 720, training loss: 4.356969279050827, validation loss: 4.3839949607849125, correct 92.7%\n",
      "Epoch 725, training loss: 4.356837755441665, validation loss: 4.384046030044556, correct 92.7%\n",
      "Epoch 730, training loss: 4.356707999110222, validation loss: 4.384095072746277, correct 92.7%\n",
      "Epoch 735, training loss: 4.356579223275185, validation loss: 4.384143114089966, correct 92.7%\n",
      "Epoch 740, training loss: 4.35645190179348, validation loss: 4.384189605712891, correct 92.7%\n",
      "Epoch 745, training loss: 4.356326028704643, validation loss: 4.38423490524292, correct 92.7%\n",
      "Epoch 750, training loss: 4.356201151013375, validation loss: 4.384279036521912, correct 92.7%\n",
      "Epoch 755, training loss: 4.356077632308006, validation loss: 4.384321761131287, correct 92.7%\n",
      "Epoch 760, training loss: 4.355955132842064, validation loss: 4.384363460540771, correct 92.7%\n",
      "Epoch 765, training loss: 4.355833867192269, validation loss: 4.384404420852661, correct 92.7%\n",
      "Epoch 770, training loss: 4.3557136952877045, validation loss: 4.384443879127502, correct 92.7%\n",
      "Epoch 775, training loss: 4.355594635009766, validation loss: 4.384483528137207, correct 92.7%\n",
      "Epoch 780, training loss: 4.35547636449337, validation loss: 4.3845223665237425, correct 92.7%\n",
      "Epoch 785, training loss: 4.355359265208245, validation loss: 4.384560012817383, correct 92.7%\n",
      "Epoch 790, training loss: 4.35524286031723, validation loss: 4.38459734916687, correct 92.7%\n",
      "Epoch 795, training loss: 4.355127489566803, validation loss: 4.384635162353516, correct 92.7%\n",
      "Epoch 800, training loss: 4.355012980103493, validation loss: 4.384671783447265, correct 92.7%\n",
      "Epoch 805, training loss: 4.354899051785469, validation loss: 4.384709644317627, correct 92.7%\n",
      "Epoch 810, training loss: 4.354786211252213, validation loss: 4.38474588394165, correct 92.7%\n",
      "Epoch 815, training loss: 4.354673862457275, validation loss: 4.38478274345398, correct 92.7%\n",
      "Epoch 820, training loss: 4.354562419652939, validation loss: 4.384819602966308, correct 92.7%\n",
      "Epoch 825, training loss: 4.35445157289505, validation loss: 4.3848552942276005, correct 92.7%\n",
      "Epoch 830, training loss: 4.354341620206833, validation loss: 4.384891843795776, correct 92.7%\n",
      "Epoch 835, training loss: 4.354232108592987, validation loss: 4.384928202629089, correct 92.7%\n",
      "Epoch 840, training loss: 4.354123282432556, validation loss: 4.384963846206665, correct 92.7%\n",
      "Epoch 845, training loss: 4.3540150910615925, validation loss: 4.384999632835388, correct 92.7%\n",
      "Epoch 850, training loss: 4.353907430171967, validation loss: 4.385034799575806, correct 92.7%\n",
      "Epoch 855, training loss: 4.353800302743911, validation loss: 4.385069799423218, correct 92.7%\n",
      "Epoch 860, training loss: 4.353693825006485, validation loss: 4.3851042747497555, correct 92.7%\n",
      "Epoch 865, training loss: 4.35358778834343, validation loss: 4.385138368606567, correct 92.7%\n",
      "Epoch 870, training loss: 4.353482353687286, validation loss: 4.385171842575073, correct 92.7%\n",
      "Epoch 875, training loss: 4.353377535939217, validation loss: 4.385205221176148, correct 92.7%\n",
      "Epoch 880, training loss: 4.3532730609178545, validation loss: 4.3852375745773315, correct 92.7%\n",
      "Epoch 885, training loss: 4.353168964385986, validation loss: 4.385269379615783, correct 92.7%\n",
      "Epoch 890, training loss: 4.353065556287765, validation loss: 4.385299944877625, correct 92.7%\n",
      "Epoch 895, training loss: 4.352962645888328, validation loss: 4.385330486297607, correct 92.7%\n",
      "Epoch 900, training loss: 4.352860030531883, validation loss: 4.385360765457153, correct 92.7%\n",
      "Epoch 905, training loss: 4.352758058905602, validation loss: 4.385389947891236, correct 92.7%\n",
      "Epoch 910, training loss: 4.352656564116478, validation loss: 4.385418248176575, correct 92.7%\n",
      "Epoch 915, training loss: 4.3525555521249775, validation loss: 4.385446238517761, correct 92.7%\n",
      "Epoch 920, training loss: 4.352454915642738, validation loss: 4.385473108291626, correct 92.7%\n",
      "Epoch 925, training loss: 4.352354905009269, validation loss: 4.385500025749207, correct 92.7%\n",
      "Epoch 930, training loss: 4.352255302667618, validation loss: 4.385526275634765, correct 92.7%\n",
      "Epoch 935, training loss: 4.352156177163124, validation loss: 4.385551691055298, correct 92.7%\n",
      "Epoch 940, training loss: 4.3520576626062395, validation loss: 4.3855767250061035, correct 92.7%\n",
      "Epoch 945, training loss: 4.351959538459778, validation loss: 4.385601663589478, correct 92.7%\n",
      "Epoch 950, training loss: 4.351861935853958, validation loss: 4.3856261491775514, correct 92.7%\n",
      "Epoch 955, training loss: 4.351764824986458, validation loss: 4.385650539398194, correct 92.7%\n",
      "Epoch 960, training loss: 4.351668256521225, validation loss: 4.385674524307251, correct 92.7%\n",
      "Epoch 965, training loss: 4.351572203636169, validation loss: 4.385698246955871, correct 92.7%\n",
      "Epoch 970, training loss: 4.351476570963859, validation loss: 4.385722017288208, correct 92.7%\n",
      "Epoch 975, training loss: 4.351381656527519, validation loss: 4.385745573043823, correct 92.7%\n",
      "Epoch 980, training loss: 4.351287168264389, validation loss: 4.385769557952881, correct 92.7%\n",
      "Epoch 985, training loss: 4.351193153858185, validation loss: 4.385793566703796, correct 92.7%\n",
      "Epoch 990, training loss: 4.351099583506584, validation loss: 4.385817837715149, correct 92.7%\n",
      "Epoch 995, training loss: 4.351006704568863, validation loss: 4.385841917991638, correct 92.7%\n",
      "Epoch 1000, training loss: 4.350914233922959, validation loss: 4.385866451263428, correct 92.7%\n",
      "Epoch 1005, training loss: 4.350822269916534, validation loss: 4.385891842842102, correct 92.7%\n",
      "Epoch 1010, training loss: 4.350730875134468, validation loss: 4.385916709899902, correct 92.7%\n",
      "Epoch 1015, training loss: 4.350639933347702, validation loss: 4.3859422445297245, correct 92.7%\n",
      "Epoch 1020, training loss: 4.350549429655075, validation loss: 4.385967540740967, correct 92.7%\n",
      "Epoch 1025, training loss: 4.3504593282938, validation loss: 4.385992693901062, correct 92.7%\n",
      "Epoch 1030, training loss: 4.350369644165039, validation loss: 4.386017990112305, correct 92.7%\n",
      "Epoch 1035, training loss: 4.3502801895141605, validation loss: 4.386041641235352, correct 92.7%\n",
      "Epoch 1040, training loss: 4.350190836191177, validation loss: 4.38606481552124, correct 92.7%\n",
      "Epoch 1045, training loss: 4.350101253390312, validation loss: 4.386084938049317, correct 92.7%\n",
      "Epoch 1050, training loss: 4.350011992454529, validation loss: 4.386101579666137, correct 92.7%\n",
      "Epoch 1055, training loss: 4.349922540783882, validation loss: 4.386113429069519, correct 92.7%\n",
      "Epoch 1060, training loss: 4.349833089113235, validation loss: 4.386119222640991, correct 92.7%\n",
      "Epoch 1065, training loss: 4.349744182825089, validation loss: 4.386120080947876, correct 92.7%\n",
      "Epoch 1070, training loss: 4.349656108021736, validation loss: 4.386116981506348, correct 92.7%\n",
      "Epoch 1075, training loss: 4.349568966031074, validation loss: 4.386111068725586, correct 92.7%\n",
      "Epoch 1080, training loss: 4.349482694268227, validation loss: 4.386103725433349, correct 92.7%\n",
      "Epoch 1085, training loss: 4.34939746260643, validation loss: 4.386094665527343, correct 92.7%\n",
      "Epoch 1090, training loss: 4.349313166737557, validation loss: 4.386084485054016, correct 92.7%\n",
      "Epoch 1095, training loss: 4.349229776859284, validation loss: 4.3860732316970825, correct 92.7%\n",
      "Epoch 1100, training loss: 4.3491474211215975, validation loss: 4.386061477661133, correct 92.7%\n",
      "Epoch 1105, training loss: 4.3490659385919574, validation loss: 4.386049461364746, correct 92.7%\n",
      "Epoch 1110, training loss: 4.348985087871552, validation loss: 4.386036539077759, correct 92.7%\n",
      "Epoch 1115, training loss: 4.34890513420105, validation loss: 4.3860231876373295, correct 92.7%\n",
      "Epoch 1120, training loss: 4.3488257884979244, validation loss: 4.386010336875915, correct 92.7%\n",
      "Epoch 1125, training loss: 4.348747244477272, validation loss: 4.385997414588928, correct 92.7%\n",
      "Epoch 1130, training loss: 4.3486693620681764, validation loss: 4.385984945297241, correct 92.7%\n",
      "Epoch 1135, training loss: 4.348592174053192, validation loss: 4.385972738265991, correct 92.7%\n",
      "Epoch 1140, training loss: 4.348515608906746, validation loss: 4.385961627960205, correct 92.7%\n",
      "Epoch 1145, training loss: 4.348439747095108, validation loss: 4.38595130443573, correct 92.7%\n",
      "Epoch 1150, training loss: 4.348364287614823, validation loss: 4.385942554473877, correct 92.7%\n",
      "Epoch 1155, training loss: 4.348289656639099, validation loss: 4.385935044288635, correct 92.7%\n",
      "Epoch 1160, training loss: 4.348215499520302, validation loss: 4.3859290599823, correct 92.7%\n",
      "Epoch 1165, training loss: 4.348142024874687, validation loss: 4.385924577713013, correct 92.7%\n",
      "Epoch 1170, training loss: 4.348069113492966, validation loss: 4.385922384262085, correct 92.7%\n",
      "Epoch 1175, training loss: 4.347996735572815, validation loss: 4.385920906066895, correct 92.7%\n",
      "Epoch 1180, training loss: 4.347925013303756, validation loss: 4.3859209537506105, correct 92.7%\n",
      "Epoch 1185, training loss: 4.347853943705559, validation loss: 4.385922908782959, correct 92.7%\n",
      "Epoch 1190, training loss: 4.347783607244492, validation loss: 4.385926580429077, correct 92.7%\n",
      "Epoch 1195, training loss: 4.34771418273449, validation loss: 4.385930728912354, correct 92.7%\n",
      "Epoch 1200, training loss: 4.347645696997643, validation loss: 4.385935592651367, correct 92.7%\n",
      "Epoch 1205, training loss: 4.347578406333923, validation loss: 4.3859413146972654, correct 92.7%\n",
      "Epoch 1210, training loss: 4.3475122332572935, validation loss: 4.38594765663147, correct 92.7%\n",
      "Epoch 1215, training loss: 4.347446984052658, validation loss: 4.385953330993653, correct 92.7%\n",
      "Epoch 1220, training loss: 4.3473828673362735, validation loss: 4.385960245132447, correct 92.7%\n",
      "Epoch 1225, training loss: 4.3473195999860765, validation loss: 4.385966944694519, correct 92.7%\n",
      "Epoch 1230, training loss: 4.347256994247436, validation loss: 4.385974264144897, correct 92.7%\n",
      "Epoch 1235, training loss: 4.347195154428482, validation loss: 4.385981559753418, correct 92.7%\n",
      "Epoch 1240, training loss: 4.3471340268850325, validation loss: 4.385989022254944, correct 92.7%\n",
      "Epoch 1245, training loss: 4.347073552012444, validation loss: 4.385997343063354, correct 92.7%\n",
      "Epoch 1250, training loss: 4.347013717889785, validation loss: 4.386005544662476, correct 92.7%\n",
      "Epoch 1255, training loss: 4.346954292058944, validation loss: 4.386014127731324, correct 92.7%\n",
      "Epoch 1260, training loss: 4.346895536780357, validation loss: 4.386022520065308, correct 92.7%\n",
      "Epoch 1265, training loss: 4.3468371540307995, validation loss: 4.386031937599182, correct 92.7%\n",
      "Epoch 1270, training loss: 4.346779456734657, validation loss: 4.386041641235352, correct 92.7%\n",
      "Epoch 1275, training loss: 4.346722319722176, validation loss: 4.386051416397095, correct 92.7%\n",
      "Epoch 1280, training loss: 4.346665519475937, validation loss: 4.386061835289001, correct 92.7%\n",
      "Epoch 1285, training loss: 4.346609222888946, validation loss: 4.386072278022766, correct 92.7%\n",
      "Epoch 1290, training loss: 4.346553495526313, validation loss: 4.386083364486694, correct 92.7%\n",
      "Epoch 1295, training loss: 4.346498155593872, validation loss: 4.386094427108764, correct 92.7%\n",
      "Epoch 1300, training loss: 4.346443301439285, validation loss: 4.386106252670288, correct 92.7%\n",
      "Epoch 1305, training loss: 4.34638886153698, validation loss: 4.3861195087432865, correct 92.7%\n",
      "Epoch 1310, training loss: 4.346334919333458, validation loss: 4.386132335662841, correct 92.7%\n",
      "Epoch 1315, training loss: 4.346281325817108, validation loss: 4.386146545410156, correct 92.7%\n",
      "Epoch 1320, training loss: 4.346228340268135, validation loss: 4.386160516738892, correct 92.7%\n",
      "Epoch 1325, training loss: 4.346175742149353, validation loss: 4.38617627620697, correct 92.7%\n",
      "Epoch 1330, training loss: 4.346123498678208, validation loss: 4.386192178726196, correct 92.7%\n",
      "Epoch 1335, training loss: 4.34607173204422, validation loss: 4.386208653450012, correct 92.7%\n",
      "Epoch 1340, training loss: 4.346020269393921, validation loss: 4.3862261295318605, correct 92.7%\n",
      "Epoch 1345, training loss: 4.345969396829605, validation loss: 4.386244916915894, correct 92.7%\n",
      "Epoch 1350, training loss: 4.345919066667557, validation loss: 4.386264085769653, correct 92.7%\n",
      "Epoch 1355, training loss: 4.345868989825249, validation loss: 4.3862844705581665, correct 92.7%\n",
      "Epoch 1360, training loss: 4.345819288492203, validation loss: 4.386306285858154, correct 92.7%\n",
      "Epoch 1365, training loss: 4.345770251750946, validation loss: 4.386328005790711, correct 92.7%\n",
      "Epoch 1370, training loss: 4.345721405744553, validation loss: 4.386350965499878, correct 92.7%\n",
      "Epoch 1375, training loss: 4.345673090219497, validation loss: 4.386375427246094, correct 92.7%\n",
      "Epoch 1380, training loss: 4.34562519788742, validation loss: 4.386401224136352, correct 92.7%\n",
      "Epoch 1385, training loss: 4.345577657222748, validation loss: 4.386426901817321, correct 92.7%\n",
      "Epoch 1390, training loss: 4.3455306679010395, validation loss: 4.3864541292190555, correct 92.7%\n",
      "Epoch 1395, training loss: 4.345484045147896, validation loss: 4.3864825963974, correct 92.7%\n",
      "Epoch 1400, training loss: 4.345437926054001, validation loss: 4.386511898040771, correct 92.7%\n",
      "Epoch 1405, training loss: 4.345392033457756, validation loss: 4.386542177200317, correct 92.7%\n",
      "Epoch 1410, training loss: 4.345346581935883, validation loss: 4.386572980880738, correct 92.7%\n",
      "Epoch 1415, training loss: 4.345301476120949, validation loss: 4.386605167388916, correct 92.7%\n",
      "Epoch 1420, training loss: 4.345256674289703, validation loss: 4.386638283729553, correct 92.7%\n",
      "Epoch 1425, training loss: 4.3452121078968045, validation loss: 4.386671829223633, correct 92.7%\n",
      "Epoch 1430, training loss: 4.345167875289917, validation loss: 4.386707830429077, correct 92.7%\n",
      "Epoch 1435, training loss: 4.345123922824859, validation loss: 4.3867439270019535, correct 92.7%\n",
      "Epoch 1440, training loss: 4.345080250501633, validation loss: 4.386781311035156, correct 92.7%\n",
      "Epoch 1445, training loss: 4.345036768913269, validation loss: 4.386818218231201, correct 92.7%\n",
      "Epoch 1450, training loss: 4.344993633031845, validation loss: 4.386857271194458, correct 92.7%\n",
      "Epoch 1455, training loss: 4.344950792193413, validation loss: 4.38689706325531, correct 92.7%\n",
      "Epoch 1460, training loss: 4.3449082434177395, validation loss: 4.386938500404358, correct 92.7%\n",
      "Epoch 1465, training loss: 4.34486611187458, validation loss: 4.386980414390564, correct 92.7%\n",
      "Epoch 1470, training loss: 4.344824314117432, validation loss: 4.387024450302124, correct 92.7%\n",
      "Epoch 1475, training loss: 4.344782975316048, validation loss: 4.387069320678711, correct 92.7%\n",
      "Epoch 1480, training loss: 4.344741928577423, validation loss: 4.387115454673767, correct 92.7%\n",
      "Epoch 1485, training loss: 4.34470132291317, validation loss: 4.387162828445435, correct 92.7%\n",
      "Epoch 1490, training loss: 4.344661006331444, validation loss: 4.387211871147156, correct 92.7%\n",
      "Epoch 1495, training loss: 4.344621166586876, validation loss: 4.387262511253357, correct 92.7%\n",
      "Epoch 1500, training loss: 4.344581508636475, validation loss: 4.387314319610596, correct 92.7%\n",
      "Epoch 1505, training loss: 4.344542273879052, validation loss: 4.387367510795594, correct 92.7%\n",
      "Epoch 1510, training loss: 4.3445032209157946, validation loss: 4.387422180175781, correct 92.7%\n",
      "Epoch 1515, training loss: 4.344464462995529, validation loss: 4.387478685379028, correct 92.7%\n",
      "Epoch 1520, training loss: 4.344426164031029, validation loss: 4.387536263465881, correct 92.7%\n",
      "Epoch 1525, training loss: 4.344388034939766, validation loss: 4.387594532966614, correct 92.7%\n",
      "Epoch 1530, training loss: 4.344350269436836, validation loss: 4.387655258178711, correct 92.7%\n",
      "Epoch 1535, training loss: 4.3443125694990155, validation loss: 4.3877167224884035, correct 92.7%\n",
      "Epoch 1540, training loss: 4.344275319576264, validation loss: 4.387780427932739, correct 92.7%\n",
      "Epoch 1545, training loss: 4.344238179922104, validation loss: 4.38784441947937, correct 92.7%\n",
      "Epoch 1550, training loss: 4.344201365113259, validation loss: 4.387910628318787, correct 92.7%\n",
      "Epoch 1555, training loss: 4.344164797663689, validation loss: 4.387977409362793, correct 92.7%\n",
      "Epoch 1560, training loss: 4.344128492474556, validation loss: 4.3880450963974, correct 92.7%\n",
      "Epoch 1565, training loss: 4.344092297554016, validation loss: 4.388113927841187, correct 92.7%\n",
      "Epoch 1570, training loss: 4.34405642747879, validation loss: 4.388183736801148, correct 92.7%\n",
      "Epoch 1575, training loss: 4.344020754098892, validation loss: 4.388253498077392, correct 92.7%\n",
      "Epoch 1580, training loss: 4.343985339999199, validation loss: 4.388325238227845, correct 92.7%\n",
      "Epoch 1585, training loss: 4.34395003914833, validation loss: 4.38839704990387, correct 92.7%\n",
      "Epoch 1590, training loss: 4.343915051221847, validation loss: 4.388468837738037, correct 92.7%\n",
      "Epoch 1595, training loss: 4.343880298733711, validation loss: 4.388541579246521, correct 92.7%\n",
      "Epoch 1600, training loss: 4.343845734000206, validation loss: 4.388614583015442, correct 92.7%\n",
      "Epoch 1605, training loss: 4.343811362981796, validation loss: 4.3886883020401, correct 92.7%\n",
      "Epoch 1610, training loss: 4.343777120113373, validation loss: 4.388761329650879, correct 92.7%\n",
      "Epoch 1615, training loss: 4.34374313056469, validation loss: 4.388835620880127, correct 92.7%\n",
      "Epoch 1620, training loss: 4.343709242343903, validation loss: 4.388909578323364, correct 92.7%\n",
      "Epoch 1625, training loss: 4.343675419688225, validation loss: 4.3889840841293335, correct 92.7%\n",
      "Epoch 1630, training loss: 4.343641978502274, validation loss: 4.389058637619018, correct 92.7%\n",
      "Epoch 1635, training loss: 4.343608644604683, validation loss: 4.3891336679458615, correct 92.7%\n",
      "Epoch 1640, training loss: 4.343575567007065, validation loss: 4.389208960533142, correct 92.7%\n",
      "Epoch 1645, training loss: 4.343542557954788, validation loss: 4.389284896850586, correct 92.7%\n",
      "Epoch 1650, training loss: 4.343509808182716, validation loss: 4.389360141754151, correct 92.7%\n",
      "Epoch 1655, training loss: 4.343477231264115, validation loss: 4.389434814453125, correct 92.7%\n",
      "Epoch 1660, training loss: 4.3434449404478075, validation loss: 4.389509296417236, correct 92.7%\n",
      "Epoch 1665, training loss: 4.343412819504738, validation loss: 4.389583492279053, correct 92.7%\n",
      "Epoch 1670, training loss: 4.343380886316299, validation loss: 4.389656138420105, correct 92.7%\n",
      "Epoch 1675, training loss: 4.343349307775497, validation loss: 4.38972840309143, correct 92.7%\n",
      "Epoch 1680, training loss: 4.343317702412605, validation loss: 4.389798998832703, correct 92.7%\n",
      "Epoch 1685, training loss: 4.343286484479904, validation loss: 4.3898687839508055, correct 92.7%\n",
      "Epoch 1690, training loss: 4.343255418539047, validation loss: 4.389936232566834, correct 92.7%\n",
      "Epoch 1695, training loss: 4.343224638700486, validation loss: 4.390003895759582, correct 92.7%\n",
      "Epoch 1700, training loss: 4.3431938827037815, validation loss: 4.390069770812988, correct 92.7%\n",
      "Epoch 1705, training loss: 4.34316349029541, validation loss: 4.390134263038635, correct 92.7%\n",
      "Epoch 1710, training loss: 4.343133142590522, validation loss: 4.390197610855102, correct 92.7%\n",
      "Epoch 1715, training loss: 4.343103063106537, validation loss: 4.390260219573975, correct 92.7%\n",
      "Epoch 1720, training loss: 4.343073317408562, validation loss: 4.390320777893066, correct 92.7%\n",
      "Epoch 1725, training loss: 4.343043649196625, validation loss: 4.3903806686401365, correct 92.7%\n",
      "Epoch 1730, training loss: 4.343014296889305, validation loss: 4.390438747406006, correct 92.7%\n",
      "Epoch 1735, training loss: 4.3429850846529, validation loss: 4.39049596786499, correct 92.7%\n",
      "Epoch 1740, training loss: 4.342956188321113, validation loss: 4.3905517816543576, correct 92.7%\n",
      "Epoch 1745, training loss: 4.3429273694753645, validation loss: 4.390606880187988, correct 92.7%\n",
      "Epoch 1750, training loss: 4.342898744344711, validation loss: 4.390660905838013, correct 92.7%\n",
      "Epoch 1755, training loss: 4.34287046790123, validation loss: 4.390714168548584, correct 92.7%\n",
      "Epoch 1760, training loss: 4.342842370271683, validation loss: 4.390767002105713, correct 92.7%\n",
      "Epoch 1765, training loss: 4.3428144484758375, validation loss: 4.390818452835083, correct 92.7%\n",
      "Epoch 1770, training loss: 4.342786583304405, validation loss: 4.390870189666748, correct 92.7%\n",
      "Epoch 1775, training loss: 4.342759123444557, validation loss: 4.3909204483032225, correct 92.7%\n",
      "Epoch 1780, training loss: 4.3427318096160885, validation loss: 4.390970277786255, correct 92.7%\n",
      "Epoch 1785, training loss: 4.342704525589943, validation loss: 4.391019797325134, correct 92.7%\n",
      "Epoch 1790, training loss: 4.342677700519562, validation loss: 4.391069984436035, correct 92.7%\n",
      "Epoch 1795, training loss: 4.34265099465847, validation loss: 4.391118717193604, correct 92.7%\n",
      "Epoch 1800, training loss: 4.342624327540397, validation loss: 4.391167640686035, correct 92.7%\n",
      "Epoch 1805, training loss: 4.342598006129265, validation loss: 4.391216659545899, correct 92.7%\n",
      "Epoch 1810, training loss: 4.342571794986725, validation loss: 4.391265726089477, correct 92.7%\n",
      "Epoch 1815, training loss: 4.342545950412751, validation loss: 4.391315150260925, correct 92.7%\n",
      "Epoch 1820, training loss: 4.3425200372934345, validation loss: 4.391364169120789, correct 92.7%\n",
      "Epoch 1825, training loss: 4.342494621872902, validation loss: 4.391413927078247, correct 92.7%\n",
      "Epoch 1830, training loss: 4.342469111084938, validation loss: 4.391463732719421, correct 92.7%\n",
      "Epoch 1835, training loss: 4.342444077134132, validation loss: 4.391513776779175, correct 92.7%\n",
      "Epoch 1840, training loss: 4.342418864369392, validation loss: 4.391564297676086, correct 92.7%\n",
      "Epoch 1845, training loss: 4.3423941969871525, validation loss: 4.391614365577698, correct 92.7%\n",
      "Epoch 1850, training loss: 4.342369458079338, validation loss: 4.391665410995484, correct 92.7%\n",
      "Epoch 1855, training loss: 4.342345029115677, validation loss: 4.391716647148132, correct 92.7%\n",
      "Epoch 1860, training loss: 4.342320704460144, validation loss: 4.391768312454223, correct 92.7%\n",
      "Epoch 1865, training loss: 4.342296475172043, validation loss: 4.391820478439331, correct 92.7%\n",
      "Epoch 1870, training loss: 4.342272502183914, validation loss: 4.391872262954712, correct 92.7%\n",
      "Epoch 1875, training loss: 4.342248612642289, validation loss: 4.391925096511841, correct 92.7%\n",
      "Epoch 1880, training loss: 4.342224952578545, validation loss: 4.3919775009155275, correct 92.7%\n",
      "Epoch 1885, training loss: 4.342201533913612, validation loss: 4.392030143737793, correct 92.7%\n",
      "Epoch 1890, training loss: 4.3421780377626416, validation loss: 4.39208300113678, correct 92.7%\n",
      "Epoch 1895, training loss: 4.342154857516289, validation loss: 4.392136263847351, correct 92.7%\n",
      "Epoch 1900, training loss: 4.3421316087245945, validation loss: 4.392189908027649, correct 92.7%\n",
      "Epoch 1905, training loss: 4.342108592391014, validation loss: 4.392242860794068, correct 92.7%\n",
      "Epoch 1910, training loss: 4.342085820436478, validation loss: 4.3922970533370975, correct 92.7%\n",
      "Epoch 1915, training loss: 4.342063066363335, validation loss: 4.392350530624389, correct 92.7%\n",
      "Epoch 1920, training loss: 4.342040613293648, validation loss: 4.392403721809387, correct 92.7%\n",
      "Epoch 1925, training loss: 4.342018175125122, validation loss: 4.392458534240722, correct 92.7%\n",
      "Epoch 1930, training loss: 4.341995790600777, validation loss: 4.392512536048889, correct 92.7%\n",
      "Epoch 1935, training loss: 4.341973578929901, validation loss: 4.392566823959351, correct 92.7%\n",
      "Epoch 1940, training loss: 4.341951525211334, validation loss: 4.3926208257675174, correct 92.7%\n",
      "Epoch 1945, training loss: 4.341929632425308, validation loss: 4.392675304412842, correct 92.7%\n",
      "Epoch 1950, training loss: 4.3419076800346375, validation loss: 4.3927290678024296, correct 92.7%\n",
      "Epoch 1955, training loss: 4.341886076331138, validation loss: 4.392783427238465, correct 92.7%\n",
      "Epoch 1960, training loss: 4.341864463686943, validation loss: 4.392837810516357, correct 92.7%\n",
      "Epoch 1965, training loss: 4.3418428778648375, validation loss: 4.392891788482666, correct 92.7%\n",
      "Epoch 1970, training loss: 4.341821536421776, validation loss: 4.392946386337281, correct 92.7%\n",
      "Epoch 1975, training loss: 4.341800311207772, validation loss: 4.39300127029419, correct 92.7%\n",
      "Epoch 1980, training loss: 4.341779085993767, validation loss: 4.393054676055908, correct 92.7%\n",
      "Epoch 1985, training loss: 4.34175805747509, validation loss: 4.393108630180359, correct 92.7%\n",
      "Epoch 1990, training loss: 4.341737058758736, validation loss: 4.393162775039673, correct 92.7%\n",
      "Epoch 1995, training loss: 4.341716268658638, validation loss: 4.393216705322265, correct 92.7%\n",
      "Epoch 2000, training loss: 4.341695418953895, validation loss: 4.393269968032837, correct 92.7%\n",
      "Epoch 2005, training loss: 4.341674739122391, validation loss: 4.393323469161987, correct 92.7%\n",
      "Epoch 2010, training loss: 4.341653972864151, validation loss: 4.393377208709717, correct 92.7%\n",
      "Epoch 2015, training loss: 4.34163353741169, validation loss: 4.393429851531982, correct 92.7%\n",
      "Epoch 2020, training loss: 4.341613087058067, validation loss: 4.393483686447143, correct 92.7%\n",
      "Epoch 2025, training loss: 4.341592791676521, validation loss: 4.393536996841431, correct 92.7%\n",
      "Epoch 2030, training loss: 4.341572415828705, validation loss: 4.39358971118927, correct 92.7%\n",
      "Epoch 2035, training loss: 4.341552221775055, validation loss: 4.3936429738998415, correct 92.7%\n",
      "Epoch 2040, training loss: 4.341532161831855, validation loss: 4.393696045875549, correct 92.7%\n",
      "Epoch 2045, training loss: 4.341512194275856, validation loss: 4.393748641014099, correct 92.7%\n",
      "Epoch 2050, training loss: 4.341492173075676, validation loss: 4.39380145072937, correct 92.7%\n",
      "Epoch 2055, training loss: 4.3414724439382555, validation loss: 4.393854141235352, correct 92.7%\n",
      "Epoch 2060, training loss: 4.341452631354332, validation loss: 4.393906784057617, correct 92.7%\n",
      "Epoch 2065, training loss: 4.341432973742485, validation loss: 4.393959426879883, correct 92.7%\n",
      "Epoch 2070, training loss: 4.341413348913193, validation loss: 4.394012403488159, correct 92.7%\n",
      "Epoch 2075, training loss: 4.341393744945526, validation loss: 4.394065260887146, correct 92.7%\n",
      "Epoch 2080, training loss: 4.341374310851097, validation loss: 4.394117474555969, correct 92.7%\n",
      "Epoch 2085, training loss: 4.341355001926422, validation loss: 4.394169759750366, correct 92.7%\n",
      "Epoch 2090, training loss: 4.3413356959819795, validation loss: 4.394222640991211, correct 92.7%\n",
      "Epoch 2095, training loss: 4.341316559910775, validation loss: 4.394275140762329, correct 92.7%\n",
      "Epoch 2100, training loss: 4.341297423839569, validation loss: 4.39432692527771, correct 92.7%\n",
      "Epoch 2105, training loss: 4.341278553009033, validation loss: 4.394378972053528, correct 92.7%\n",
      "Epoch 2110, training loss: 4.3412596672773365, validation loss: 4.394430017471313, correct 92.7%\n",
      "Epoch 2115, training loss: 4.341241067647934, validation loss: 4.394482254981995, correct 92.7%\n",
      "Epoch 2120, training loss: 4.341222220659256, validation loss: 4.394534802436828, correct 92.7%\n",
      "Epoch 2125, training loss: 4.341203784942627, validation loss: 4.394586205482483, correct 92.7%\n",
      "Epoch 2130, training loss: 4.341185209155083, validation loss: 4.3946373701095585, correct 92.7%\n",
      "Epoch 2135, training loss: 4.3411668807268144, validation loss: 4.394688129425049, correct 92.7%\n",
      "Epoch 2140, training loss: 4.341148516535759, validation loss: 4.394739389419556, correct 92.7%\n",
      "Epoch 2145, training loss: 4.341130417585373, validation loss: 4.394790506362915, correct 92.7%\n",
      "Epoch 2150, training loss: 4.34111233651638, validation loss: 4.39484167098999, correct 92.7%\n",
      "Epoch 2155, training loss: 4.34109436571598, validation loss: 4.394891953468322, correct 92.7%\n",
      "Epoch 2160, training loss: 4.341076490283013, validation loss: 4.394943380355835, correct 92.7%\n",
      "Epoch 2165, training loss: 4.341058641672134, validation loss: 4.394994163513184, correct 92.7%\n",
      "Epoch 2170, training loss: 4.3410408109426495, validation loss: 4.395045232772827, correct 92.7%\n",
      "Epoch 2175, training loss: 4.341023144125939, validation loss: 4.395094466209412, correct 92.7%\n",
      "Epoch 2180, training loss: 4.34100549519062, validation loss: 4.395144033432007, correct 92.7%\n",
      "Epoch 2185, training loss: 4.34098810851574, validation loss: 4.3951935291290285, correct 92.7%\n",
      "Epoch 2190, training loss: 4.340970519185066, validation loss: 4.395243382453918, correct 92.7%\n",
      "Epoch 2195, training loss: 4.3409531056880954, validation loss: 4.39529275894165, correct 92.7%\n",
      "Epoch 2200, training loss: 4.340935918688774, validation loss: 4.395342302322388, correct 92.7%\n",
      "Epoch 2205, training loss: 4.3409185916185375, validation loss: 4.395391845703125, correct 92.7%\n",
      "Epoch 2210, training loss: 4.340901443362236, validation loss: 4.395441198348999, correct 92.7%\n",
      "Epoch 2215, training loss: 4.3408843785524365, validation loss: 4.3954897165298465, correct 92.7%\n",
      "Epoch 2220, training loss: 4.340867298841476, validation loss: 4.395538091659546, correct 92.7%\n",
      "Epoch 2225, training loss: 4.340850386023521, validation loss: 4.395586085319519, correct 92.7%\n",
      "Epoch 2230, training loss: 4.340833470225334, validation loss: 4.395634651184082, correct 92.7%\n",
      "Epoch 2235, training loss: 4.340816748142243, validation loss: 4.3956825733184814, correct 92.7%\n",
      "Epoch 2240, training loss: 4.340800070762635, validation loss: 4.395730710029602, correct 92.7%\n",
      "Epoch 2245, training loss: 4.340783387422562, validation loss: 4.395777916908264, correct 92.7%\n",
      "Epoch 2250, training loss: 4.340766778588295, validation loss: 4.395825576782227, correct 92.7%\n",
      "Epoch 2255, training loss: 4.340750285983086, validation loss: 4.39587254524231, correct 92.7%\n",
      "Epoch 2260, training loss: 4.340733850002289, validation loss: 4.395919299125671, correct 92.7%\n",
      "Epoch 2265, training loss: 4.340717387199402, validation loss: 4.3959653854370115, correct 92.7%\n",
      "Epoch 2270, training loss: 4.340701192617416, validation loss: 4.396011781692505, correct 92.7%\n",
      "Epoch 2275, training loss: 4.340684920549393, validation loss: 4.396057724952698, correct 92.7%\n",
      "Epoch 2280, training loss: 4.340668633580208, validation loss: 4.396104049682617, correct 92.7%\n",
      "Epoch 2285, training loss: 4.340652644634247, validation loss: 4.396149682998657, correct 92.7%\n",
      "Epoch 2290, training loss: 4.340636575222016, validation loss: 4.396194505691528, correct 92.7%\n",
      "Epoch 2295, training loss: 4.340620586276055, validation loss: 4.396239566802978, correct 92.7%\n",
      "Epoch 2300, training loss: 4.340604776144028, validation loss: 4.396284008026123, correct 92.7%\n",
      "Epoch 2305, training loss: 4.340588879585266, validation loss: 4.396328139305115, correct 92.7%\n",
      "Epoch 2310, training loss: 4.340573111176491, validation loss: 4.396372079849243, correct 92.7%\n",
      "Epoch 2315, training loss: 4.340557491779327, validation loss: 4.396415710449219, correct 92.7%\n",
      "Epoch 2320, training loss: 4.340541797876358, validation loss: 4.3964605808258055, correct 92.7%\n",
      "Epoch 2325, training loss: 4.3405262410640715, validation loss: 4.396503353118897, correct 92.7%\n",
      "Epoch 2330, training loss: 4.34051074385643, validation loss: 4.39654700756073, correct 92.7%\n",
      "Epoch 2335, training loss: 4.340495371818543, validation loss: 4.396589922904968, correct 92.7%\n",
      "Epoch 2340, training loss: 4.340480077266693, validation loss: 4.39663188457489, correct 92.7%\n",
      "Epoch 2345, training loss: 4.340464776754379, validation loss: 4.396674537658692, correct 92.7%\n",
      "Epoch 2350, training loss: 4.340449583530426, validation loss: 4.396716213226318, correct 92.7%\n",
      "Epoch 2355, training loss: 4.340434411168099, validation loss: 4.396758699417115, correct 92.7%\n",
      "Epoch 2360, training loss: 4.3404191672801975, validation loss: 4.396800184249878, correct 92.7%\n",
      "Epoch 2365, training loss: 4.340404188632965, validation loss: 4.396841526031494, correct 92.7%\n",
      "Epoch 2370, training loss: 4.340389204025269, validation loss: 4.396883010864258, correct 92.7%\n",
      "Epoch 2375, training loss: 4.340374240279198, validation loss: 4.3969244956970215, correct 92.7%\n",
      "Epoch 2380, training loss: 4.3403593599796295, validation loss: 4.396965074539184, correct 92.7%\n",
      "Epoch 2385, training loss: 4.340344500541687, validation loss: 4.397006249427795, correct 92.7%\n",
      "Epoch 2390, training loss: 4.340329656004906, validation loss: 4.39704704284668, correct 92.7%\n",
      "Epoch 2395, training loss: 4.340315046906471, validation loss: 4.397087836265564, correct 92.7%\n",
      "Epoch 2400, training loss: 4.3403003990650175, validation loss: 4.397129321098328, correct 92.7%\n",
      "Epoch 2405, training loss: 4.340285846590996, validation loss: 4.397169613838196, correct 92.7%\n",
      "Epoch 2410, training loss: 4.340271353721619, validation loss: 4.397209286689758, correct 92.7%\n",
      "Epoch 2415, training loss: 4.340256923437119, validation loss: 4.3972498893737795, correct 92.7%\n",
      "Epoch 2420, training loss: 4.3402425169944765, validation loss: 4.397290849685669, correct 92.7%\n",
      "Epoch 2425, training loss: 4.340228196978569, validation loss: 4.397330975532531, correct 92.7%\n",
      "Epoch 2430, training loss: 4.340213930606842, validation loss: 4.397372102737426, correct 92.7%\n",
      "Epoch 2435, training loss: 4.34019975066185, validation loss: 4.397412919998169, correct 92.7%\n",
      "Epoch 2440, training loss: 4.340185669064522, validation loss: 4.397452878952026, correct 92.7%\n",
      "Epoch 2445, training loss: 4.340171480178833, validation loss: 4.397493028640747, correct 92.7%\n",
      "Epoch 2450, training loss: 4.340157601237297, validation loss: 4.3975337266922, correct 92.7%\n",
      "Epoch 2455, training loss: 4.3401436597108844, validation loss: 4.397574400901794, correct 92.7%\n",
      "Epoch 2460, training loss: 4.340129688382149, validation loss: 4.397615528106689, correct 92.7%\n",
      "Epoch 2465, training loss: 4.340115928649903, validation loss: 4.397655963897705, correct 92.7%\n",
      "Epoch 2470, training loss: 4.340102028846741, validation loss: 4.397696542739868, correct 92.7%\n",
      "Epoch 2475, training loss: 4.340088307857513, validation loss: 4.397736883163452, correct 92.7%\n",
      "Epoch 2480, training loss: 4.34007457792759, validation loss: 4.397777414321899, correct 92.7%\n",
      "Epoch 2485, training loss: 4.340061050653458, validation loss: 4.397818255424499, correct 92.7%\n",
      "Epoch 2490, training loss: 4.340047436952591, validation loss: 4.397859597206116, correct 92.7%\n",
      "Epoch 2495, training loss: 4.34003387093544, validation loss: 4.397900247573853, correct 92.7%\n",
      "Epoch 2500, training loss: 4.340020427107811, validation loss: 4.3979416847229, correct 92.7%\n",
      "Epoch 2505, training loss: 4.340007004141808, validation loss: 4.397982954978943, correct 92.7%\n",
      "Epoch 2510, training loss: 4.339993667602539, validation loss: 4.398024153709412, correct 92.7%\n",
      "Epoch 2515, training loss: 4.339980250597, validation loss: 4.398066186904908, correct 92.7%\n",
      "Epoch 2520, training loss: 4.339966961741448, validation loss: 4.398106956481934, correct 92.7%\n",
      "Epoch 2525, training loss: 4.339953771233558, validation loss: 4.39814817905426, correct 92.7%\n",
      "Epoch 2530, training loss: 4.339940676093102, validation loss: 4.3981897830963135, correct 92.7%\n",
      "Epoch 2535, training loss: 4.339927417039871, validation loss: 4.398231387138367, correct 92.7%\n",
      "Epoch 2540, training loss: 4.339914336800575, validation loss: 4.398273086547851, correct 92.7%\n",
      "Epoch 2545, training loss: 4.3399012923240665, validation loss: 4.398314619064331, correct 92.7%\n",
      "Epoch 2550, training loss: 4.339888268709183, validation loss: 4.398356246948242, correct 92.7%\n",
      "Epoch 2555, training loss: 4.339875355362892, validation loss: 4.398398590087891, correct 92.7%\n",
      "Epoch 2560, training loss: 4.339862498641014, validation loss: 4.398441171646118, correct 92.7%\n",
      "Epoch 2565, training loss: 4.3398496329784395, validation loss: 4.398483562469482, correct 92.7%\n",
      "Epoch 2570, training loss: 4.3398368686437605, validation loss: 4.398526120185852, correct 92.7%\n",
      "Epoch 2575, training loss: 4.339824050664902, validation loss: 4.398568344116211, correct 92.7%\n",
      "Epoch 2580, training loss: 4.339811238646507, validation loss: 4.398610353469849, correct 92.7%\n",
      "Epoch 2585, training loss: 4.339798620343208, validation loss: 4.398652982711792, correct 92.7%\n",
      "Epoch 2590, training loss: 4.339786028861999, validation loss: 4.398695540428162, correct 92.7%\n",
      "Epoch 2595, training loss: 4.339773410558701, validation loss: 4.398737621307373, correct 92.7%\n",
      "Epoch 2600, training loss: 4.339760816097259, validation loss: 4.398780989646911, correct 92.7%\n",
      "Epoch 2605, training loss: 4.339748209714889, validation loss: 4.398823761940003, correct 92.7%\n",
      "Epoch 2610, training loss: 4.339735901355743, validation loss: 4.398866462707519, correct 92.7%\n",
      "Epoch 2615, training loss: 4.339723536372185, validation loss: 4.398909330368042, correct 92.7%\n",
      "Epoch 2620, training loss: 4.339711192250252, validation loss: 4.398952388763428, correct 92.7%\n",
      "Epoch 2625, training loss: 4.339698833227158, validation loss: 4.398995995521545, correct 92.7%\n",
      "Epoch 2630, training loss: 4.339686453342438, validation loss: 4.399038076400757, correct 92.7%\n",
      "Epoch 2635, training loss: 4.33967424929142, validation loss: 4.399081540107727, correct 92.7%\n",
      "Epoch 2640, training loss: 4.339661970734596, validation loss: 4.3991251468658445, correct 92.7%\n",
      "Epoch 2645, training loss: 4.339649859070778, validation loss: 4.3991687297821045, correct 92.7%\n",
      "Epoch 2650, training loss: 4.339637669920921, validation loss: 4.399212384223938, correct 92.7%\n",
      "Epoch 2655, training loss: 4.339625614881515, validation loss: 4.399256229400635, correct 92.7%\n",
      "Epoch 2660, training loss: 4.339613550901413, validation loss: 4.399299907684326, correct 92.7%\n",
      "Epoch 2665, training loss: 4.339601519703865, validation loss: 4.399343681335449, correct 92.7%\n",
      "Epoch 2670, training loss: 4.339589616656303, validation loss: 4.39938759803772, correct 92.7%\n",
      "Epoch 2675, training loss: 4.3395776629447935, validation loss: 4.399431610107422, correct 92.7%\n",
      "Epoch 2680, training loss: 4.339565715193748, validation loss: 4.3994752168655396, correct 92.7%\n",
      "Epoch 2685, training loss: 4.339553928375244, validation loss: 4.399518728256226, correct 92.7%\n",
      "Epoch 2690, training loss: 4.339542010426522, validation loss: 4.399562549591065, correct 92.7%\n",
      "Epoch 2695, training loss: 4.339530083537102, validation loss: 4.399606657028198, correct 92.7%\n",
      "Epoch 2700, training loss: 4.33951835334301, validation loss: 4.399649930000305, correct 92.7%\n",
      "Epoch 2705, training loss: 4.3395065635442736, validation loss: 4.399694323539734, correct 92.7%\n",
      "Epoch 2710, training loss: 4.339494904875755, validation loss: 4.3997392654418945, correct 92.7%\n",
      "Epoch 2715, training loss: 4.339483219385147, validation loss: 4.399783396720887, correct 92.7%\n",
      "Epoch 2720, training loss: 4.339471593499184, validation loss: 4.399827671051026, correct 92.7%\n",
      "Epoch 2725, training loss: 4.339460006356239, validation loss: 4.399871397018432, correct 92.7%\n",
      "Epoch 2730, training loss: 4.3394483655691145, validation loss: 4.399915552139282, correct 92.7%\n",
      "Epoch 2735, training loss: 4.339436703920365, validation loss: 4.399960517883301, correct 92.7%\n",
      "Epoch 2740, training loss: 4.339425182342529, validation loss: 4.400005412101746, correct 92.7%\n",
      "Epoch 2745, training loss: 4.339413857460022, validation loss: 4.400049257278442, correct 92.7%\n",
      "Epoch 2750, training loss: 4.339402362704277, validation loss: 4.400093817710877, correct 92.7%\n",
      "Epoch 2755, training loss: 4.339391002058983, validation loss: 4.400138878822327, correct 92.7%\n",
      "Epoch 2760, training loss: 4.339379587769509, validation loss: 4.4001846551895145, correct 92.7%\n",
      "Epoch 2765, training loss: 4.339368227124214, validation loss: 4.400229597091675, correct 92.7%\n",
      "Epoch 2770, training loss: 4.339357009530067, validation loss: 4.400274562835693, correct 92.7%\n",
      "Epoch 2775, training loss: 4.339345607161522, validation loss: 4.400318932533264, correct 92.7%\n",
      "Epoch 2780, training loss: 4.339334577322006, validation loss: 4.400364375114441, correct 92.7%\n",
      "Epoch 2785, training loss: 4.339323484897614, validation loss: 4.40040967464447, correct 92.7%\n",
      "Epoch 2790, training loss: 4.339312559366226, validation loss: 4.4004555463790895, correct 92.7%\n",
      "Epoch 2795, training loss: 4.339301490783692, validation loss: 4.400500822067261, correct 92.7%\n",
      "Epoch 2800, training loss: 4.339290669560432, validation loss: 4.400547194480896, correct 92.7%\n",
      "Epoch 2805, training loss: 4.339279702305793, validation loss: 4.400593543052674, correct 92.7%\n",
      "Epoch 2810, training loss: 4.33926899433136, validation loss: 4.400640392303467, correct 92.7%\n",
      "Epoch 2815, training loss: 4.339258369803429, validation loss: 4.400687217712402, correct 92.7%\n",
      "Epoch 2820, training loss: 4.339247637987137, validation loss: 4.400733757019043, correct 92.7%\n",
      "Epoch 2825, training loss: 4.339236870408058, validation loss: 4.400779867172242, correct 92.7%\n",
      "Epoch 2830, training loss: 4.339226466417313, validation loss: 4.400826907157898, correct 92.7%\n",
      "Epoch 2835, training loss: 4.339216020703316, validation loss: 4.400874233245849, correct 92.7%\n",
      "Epoch 2840, training loss: 4.339205819368362, validation loss: 4.400922465324402, correct 92.7%\n",
      "Epoch 2845, training loss: 4.339195424318314, validation loss: 4.400969982147217, correct 92.7%\n",
      "Epoch 2850, training loss: 4.339185214042663, validation loss: 4.401018214225769, correct 92.7%\n",
      "Epoch 2855, training loss: 4.339175045490265, validation loss: 4.401066255569458, correct 92.7%\n",
      "Epoch 2860, training loss: 4.3391650259494785, validation loss: 4.401114964485169, correct 92.7%\n",
      "Epoch 2865, training loss: 4.339155042171479, validation loss: 4.40116319656372, correct 92.7%\n",
      "Epoch 2870, training loss: 4.339145228266716, validation loss: 4.401211309432983, correct 92.7%\n",
      "Epoch 2875, training loss: 4.3391355007886885, validation loss: 4.401259446144104, correct 92.7%\n",
      "Epoch 2880, training loss: 4.339125710725784, validation loss: 4.401308631896972, correct 92.7%\n",
      "Epoch 2885, training loss: 4.339115923643112, validation loss: 4.401357746124267, correct 92.7%\n",
      "Epoch 2890, training loss: 4.339106327295303, validation loss: 4.40140655040741, correct 92.7%\n",
      "Epoch 2895, training loss: 4.339096772670746, validation loss: 4.401455807685852, correct 92.7%\n",
      "Epoch 2900, training loss: 4.339087089896202, validation loss: 4.401504993438721, correct 92.7%\n",
      "Epoch 2905, training loss: 4.3390776216983795, validation loss: 4.401554012298584, correct 92.7%\n",
      "Epoch 2910, training loss: 4.339068302512169, validation loss: 4.401603651046753, correct 92.7%\n",
      "Epoch 2915, training loss: 4.3390589445829395, validation loss: 4.401652908325195, correct 92.7%\n",
      "Epoch 2920, training loss: 4.339049535989761, validation loss: 4.401702547073365, correct 92.7%\n",
      "Epoch 2925, training loss: 4.339040288329125, validation loss: 4.401752281188965, correct 92.7%\n",
      "Epoch 2930, training loss: 4.339030987024307, validation loss: 4.401801824569702, correct 92.7%\n",
      "Epoch 2935, training loss: 4.339021700620651, validation loss: 4.4018505096435545, correct 92.7%\n",
      "Epoch 2940, training loss: 4.339012524485588, validation loss: 4.4019006252288815, correct 92.7%\n",
      "Epoch 2945, training loss: 4.339003494381904, validation loss: 4.401950025558472, correct 92.7%\n",
      "Epoch 2950, training loss: 4.33899444937706, validation loss: 4.40199990272522, correct 92.7%\n",
      "Epoch 2955, training loss: 4.338985475897789, validation loss: 4.402050232887268, correct 92.7%\n",
      "Epoch 2960, training loss: 4.338976487517357, validation loss: 4.402100443840027, correct 92.7%\n",
      "Epoch 2965, training loss: 4.338967546820641, validation loss: 4.4021509170532225, correct 92.7%\n",
      "Epoch 2970, training loss: 4.338958641886711, validation loss: 4.402201056480408, correct 92.7%\n",
      "Epoch 2975, training loss: 4.338949757814407, validation loss: 4.402250814437866, correct 92.7%\n",
      "Epoch 2980, training loss: 4.338940876722336, validation loss: 4.40230119228363, correct 92.7%\n",
      "Epoch 2985, training loss: 4.3389320999383925, validation loss: 4.402351188659668, correct 92.7%\n",
      "Epoch 2990, training loss: 4.338923254609108, validation loss: 4.402400851249695, correct 92.7%\n",
      "Epoch 2995, training loss: 4.338914468884468, validation loss: 4.402450513839722, correct 92.7%\n",
      "Epoch 3000, training loss: 4.338905769586563, validation loss: 4.402501106262207, correct 92.7%\n",
      "Epoch 3005, training loss: 4.338896998763085, validation loss: 4.402551507949829, correct 92.7%\n",
      "Epoch 3010, training loss: 4.33888832628727, validation loss: 4.402602028846741, correct 92.7%\n",
      "Epoch 3015, training loss: 4.338879597187042, validation loss: 4.402651834487915, correct 92.7%\n",
      "Epoch 3020, training loss: 4.33887095451355, validation loss: 4.402702045440674, correct 92.7%\n",
      "Epoch 3025, training loss: 4.338862469792366, validation loss: 4.402752208709717, correct 92.7%\n",
      "Epoch 3030, training loss: 4.338853827118873, validation loss: 4.402802419662476, correct 92.7%\n",
      "Epoch 3035, training loss: 4.338845211267471, validation loss: 4.40285210609436, correct 92.7%\n",
      "Epoch 3040, training loss: 4.338836717605591, validation loss: 4.402901840209961, correct 92.7%\n",
      "Epoch 3045, training loss: 4.338828238844871, validation loss: 4.402951788902283, correct 92.7%\n",
      "Epoch 3050, training loss: 4.338819766044617, validation loss: 4.403001880645752, correct 92.7%\n",
      "Epoch 3055, training loss: 4.338811275362969, validation loss: 4.403052568435669, correct 92.7%\n",
      "Epoch 3060, training loss: 4.338802775740623, validation loss: 4.403102946281433, correct 92.7%\n",
      "Epoch 3065, training loss: 4.338794445991516, validation loss: 4.403152751922607, correct 92.7%\n",
      "Epoch 3070, training loss: 4.338785916566849, validation loss: 4.403202486038208, correct 92.7%\n",
      "Epoch 3075, training loss: 4.338777500391006, validation loss: 4.403252220153808, correct 92.7%\n",
      "Epoch 3080, training loss: 4.338769203424453, validation loss: 4.403302526473999, correct 92.7%\n",
      "Epoch 3085, training loss: 4.338760703802109, validation loss: 4.403352975845337, correct 92.7%\n",
      "Epoch 3090, training loss: 4.338752326369286, validation loss: 4.403403043746948, correct 92.7%\n",
      "Epoch 3095, training loss: 4.338744068145752, validation loss: 4.403451728820801, correct 92.7%\n",
      "Epoch 3100, training loss: 4.338735643029213, validation loss: 4.403501844406128, correct 92.7%\n",
      "Epoch 3105, training loss: 4.338727313280105, validation loss: 4.40355110168457, correct 92.7%\n",
      "Epoch 3110, training loss: 4.338718968629837, validation loss: 4.403600883483887, correct 92.7%\n",
      "Epoch 3115, training loss: 4.338710623979568, validation loss: 4.403651142120362, correct 92.7%\n",
      "Epoch 3120, training loss: 4.338702303171158, validation loss: 4.403700137138367, correct 92.7%\n",
      "Epoch 3125, training loss: 4.338694015145302, validation loss: 4.403749179840088, correct 92.7%\n",
      "Epoch 3130, training loss: 4.338685819506646, validation loss: 4.403798675537109, correct 92.7%\n",
      "Epoch 3135, training loss: 4.338677379488945, validation loss: 4.403848600387573, correct 92.7%\n",
      "Epoch 3140, training loss: 4.3386691302061084, validation loss: 4.403898024559021, correct 92.7%\n",
      "Epoch 3145, training loss: 4.338660734891891, validation loss: 4.403946423530579, correct 92.7%\n",
      "Epoch 3150, training loss: 4.338652467727661, validation loss: 4.403994512557984, correct 92.7%\n",
      "Epoch 3155, training loss: 4.338644081354142, validation loss: 4.404043221473694, correct 92.7%\n",
      "Epoch 3160, training loss: 4.338635787367821, validation loss: 4.404091548919678, correct 92.7%\n",
      "Epoch 3165, training loss: 4.3386273890733715, validation loss: 4.404139280319214, correct 92.7%\n",
      "Epoch 3170, training loss: 4.338619032502175, validation loss: 4.40418860912323, correct 92.7%\n",
      "Epoch 3175, training loss: 4.338610580563545, validation loss: 4.404236531257629, correct 92.7%\n",
      "Epoch 3180, training loss: 4.338602292537689, validation loss: 4.40428466796875, correct 92.7%\n",
      "Epoch 3185, training loss: 4.33859381377697, validation loss: 4.404332852363586, correct 92.7%\n",
      "Epoch 3190, training loss: 4.338585075736046, validation loss: 4.404379487037659, correct 92.7%\n",
      "Epoch 3195, training loss: 4.33857638835907, validation loss: 4.404427480697632, correct 92.7%\n",
      "Epoch 3200, training loss: 4.33856770992279, validation loss: 4.404474878311158, correct 92.7%\n",
      "Epoch 3205, training loss: 4.338559108972549, validation loss: 4.404521489143372, correct 92.7%\n",
      "Epoch 3210, training loss: 4.338550627231598, validation loss: 4.404568600654602, correct 92.7%\n",
      "Epoch 3215, training loss: 4.338542321324349, validation loss: 4.404615092277527, correct 92.7%\n",
      "Epoch 3220, training loss: 4.3385339915752414, validation loss: 4.40466160774231, correct 92.7%\n",
      "Epoch 3225, training loss: 4.338525468111039, validation loss: 4.4047081708908085, correct 92.7%\n",
      "Epoch 3230, training loss: 4.338516342639923, validation loss: 4.404755258560181, correct 92.7%\n",
      "Epoch 3235, training loss: 4.338507911562919, validation loss: 4.404800486564636, correct 92.7%\n",
      "Epoch 3240, training loss: 4.338499319553375, validation loss: 4.404845666885376, correct 92.7%\n",
      "Epoch 3245, training loss: 4.33849081993103, validation loss: 4.404891061782837, correct 92.7%\n",
      "Epoch 3250, training loss: 4.338482430577278, validation loss: 4.404936695098877, correct 92.7%\n",
      "Epoch 3255, training loss: 4.3384738832712175, validation loss: 4.404981708526611, correct 92.7%\n",
      "Epoch 3260, training loss: 4.33846555352211, validation loss: 4.405026865005493, correct 92.7%\n",
      "Epoch 3265, training loss: 4.33845724761486, validation loss: 4.405072784423828, correct 92.7%\n",
      "Epoch 3270, training loss: 4.338448959589004, validation loss: 4.405117988586426, correct 92.7%\n",
      "Epoch 3275, training loss: 4.338440716266632, validation loss: 4.405162644386292, correct 92.7%\n",
      "Epoch 3280, training loss: 4.338432481884956, validation loss: 4.405207300186158, correct 92.7%\n",
      "Epoch 3285, training loss: 4.338424360752105, validation loss: 4.405252408981323, correct 92.7%\n",
      "Epoch 3290, training loss: 4.338416159152985, validation loss: 4.405296778678894, correct 92.7%\n",
      "Epoch 3295, training loss: 4.338408109545708, validation loss: 4.405340600013733, correct 92.7%\n",
      "Epoch 3300, training loss: 4.338399991393089, validation loss: 4.405383968353272, correct 92.7%\n",
      "Epoch 3305, training loss: 4.338391929864883, validation loss: 4.405427265167236, correct 92.7%\n",
      "Epoch 3310, training loss: 4.3383838653564455, validation loss: 4.40546989440918, correct 92.7%\n",
      "Epoch 3315, training loss: 4.338375931978225, validation loss: 4.405513048171997, correct 92.7%\n",
      "Epoch 3320, training loss: 4.338367873430252, validation loss: 4.405555486679077, correct 92.7%\n",
      "Epoch 3325, training loss: 4.338359838724136, validation loss: 4.4055983543396, correct 92.7%\n",
      "Epoch 3330, training loss: 4.338351970911026, validation loss: 4.405640578269958, correct 92.7%\n",
      "Epoch 3335, training loss: 4.338344016671181, validation loss: 4.405682754516602, correct 92.7%\n",
      "Epoch 3340, training loss: 4.3383362084627155, validation loss: 4.405724358558655, correct 92.7%\n",
      "Epoch 3345, training loss: 4.338328388333321, validation loss: 4.405766749382019, correct 92.7%\n",
      "Epoch 3350, training loss: 4.338320535421372, validation loss: 4.4058081150054935, correct 92.7%\n",
      "Epoch 3355, training loss: 4.338312780857086, validation loss: 4.4058492422103885, correct 92.7%\n",
      "Epoch 3360, training loss: 4.338305068016052, validation loss: 4.40588960647583, correct 92.7%\n",
      "Epoch 3365, training loss: 4.338297337293625, validation loss: 4.405929946899414, correct 92.7%\n",
      "Epoch 3370, training loss: 4.338289740681648, validation loss: 4.405969619750977, correct 92.7%\n",
      "Epoch 3375, training loss: 4.3382821589708325, validation loss: 4.406009864807129, correct 92.7%\n",
      "Epoch 3380, training loss: 4.338274532556534, validation loss: 4.406049656867981, correct 92.7%\n",
      "Epoch 3385, training loss: 4.338266918063164, validation loss: 4.406088495254517, correct 92.7%\n",
      "Epoch 3390, training loss: 4.338259369134903, validation loss: 4.406126832962036, correct 92.7%\n",
      "Epoch 3395, training loss: 4.338251847028732, validation loss: 4.406164622306823, correct 92.7%\n",
      "Epoch 3400, training loss: 4.338244274258614, validation loss: 4.4062021493911745, correct 92.7%\n",
      "Epoch 3405, training loss: 4.338236865401268, validation loss: 4.406239461898804, correct 92.7%\n",
      "Epoch 3410, training loss: 4.338229328393936, validation loss: 4.406275987625122, correct 92.7%\n",
      "Epoch 3415, training loss: 4.338221991062165, validation loss: 4.406312561035156, correct 92.7%\n",
      "Epoch 3420, training loss: 4.338214582204818, validation loss: 4.406348800659179, correct 92.7%\n",
      "Epoch 3425, training loss: 4.3382072150707245, validation loss: 4.406384801864624, correct 92.7%\n",
      "Epoch 3430, training loss: 4.3381999135017395, validation loss: 4.406419897079468, correct 92.7%\n",
      "Epoch 3435, training loss: 4.338192734122276, validation loss: 4.406455016136169, correct 92.7%\n",
      "Epoch 3440, training loss: 4.338185593485832, validation loss: 4.406489849090576, correct 92.7%\n",
      "Epoch 3445, training loss: 4.338178572058678, validation loss: 4.406523942947388, correct 92.7%\n",
      "Epoch 3450, training loss: 4.3381715446710585, validation loss: 4.406558299064637, correct 92.7%\n",
      "Epoch 3455, training loss: 4.338164612650871, validation loss: 4.406592082977295, correct 92.7%\n",
      "Epoch 3460, training loss: 4.338157799839974, validation loss: 4.4066253185272215, correct 92.7%\n",
      "Epoch 3465, training loss: 4.338150823116303, validation loss: 4.406658625602722, correct 92.7%\n",
      "Epoch 3470, training loss: 4.338144013285637, validation loss: 4.406691455841065, correct 92.7%\n",
      "Epoch 3475, training loss: 4.338137370347977, validation loss: 4.406724524497986, correct 92.7%\n",
      "Epoch 3480, training loss: 4.338130706548691, validation loss: 4.406757450103759, correct 92.7%\n",
      "Epoch 3485, training loss: 4.338124218583107, validation loss: 4.40678973197937, correct 92.7%\n",
      "Epoch 3490, training loss: 4.338117533922196, validation loss: 4.406821918487549, correct 92.7%\n",
      "Epoch 3495, training loss: 4.3381111234426495, validation loss: 4.406853485107422, correct 92.7%\n",
      "Epoch 3500, training loss: 4.338104677200318, validation loss: 4.406884574890137, correct 92.7%\n",
      "Epoch 3505, training loss: 4.338098356127739, validation loss: 4.406916165351868, correct 92.7%\n",
      "Epoch 3510, training loss: 4.338092133402824, validation loss: 4.406948256492615, correct 92.7%\n",
      "Epoch 3515, training loss: 4.338085752725601, validation loss: 4.406980061531067, correct 92.7%\n",
      "Epoch 3520, training loss: 4.338079658150673, validation loss: 4.407011413574219, correct 92.7%\n",
      "Epoch 3525, training loss: 4.338073468208313, validation loss: 4.40704345703125, correct 92.7%\n",
      "Epoch 3530, training loss: 4.3380673170089725, validation loss: 4.407074642181397, correct 92.7%\n",
      "Epoch 3535, training loss: 4.338061207532883, validation loss: 4.407106494903564, correct 92.7%\n",
      "Epoch 3540, training loss: 4.338055121898651, validation loss: 4.407137680053711, correct 92.7%\n",
      "Epoch 3545, training loss: 4.338049161434173, validation loss: 4.4071687936782835, correct 92.7%\n",
      "Epoch 3550, training loss: 4.338043063879013, validation loss: 4.4072003841400145, correct 92.7%\n",
      "Epoch 3555, training loss: 4.338037085533142, validation loss: 4.407232403755188, correct 92.7%\n",
      "Epoch 3560, training loss: 4.338031068444252, validation loss: 4.407263588905335, correct 92.7%\n",
      "Epoch 3565, training loss: 4.338025027513504, validation loss: 4.407294178009034, correct 92.7%\n",
      "Epoch 3570, training loss: 4.338019070029259, validation loss: 4.407325172424317, correct 92.7%\n",
      "Epoch 3575, training loss: 4.338013136386872, validation loss: 4.407356715202331, correct 92.7%\n",
      "Epoch 3580, training loss: 4.338007226586342, validation loss: 4.407388186454773, correct 92.7%\n",
      "Epoch 3585, training loss: 4.338001346588134, validation loss: 4.407419180870056, correct 92.7%\n",
      "Epoch 3590, training loss: 4.337995582818985, validation loss: 4.407450771331787, correct 92.7%\n",
      "Epoch 3595, training loss: 4.337989839911461, validation loss: 4.40748245716095, correct 92.7%\n",
      "Epoch 3600, training loss: 4.337984031438827, validation loss: 4.407514190673828, correct 92.7%\n",
      "Epoch 3605, training loss: 4.337978279590606, validation loss: 4.407544851303101, correct 92.7%\n",
      "Epoch 3610, training loss: 4.337972539663315, validation loss: 4.407576036453247, correct 92.7%\n",
      "Epoch 3615, training loss: 4.337966805696487, validation loss: 4.407607626914978, correct 92.7%\n",
      "Epoch 3620, training loss: 4.33796119093895, validation loss: 4.407638788223267, correct 92.7%\n",
      "Epoch 3625, training loss: 4.3379556149244305, validation loss: 4.407670164108277, correct 92.7%\n",
      "Epoch 3630, training loss: 4.337949851155281, validation loss: 4.40770161151886, correct 92.7%\n",
      "Epoch 3635, training loss: 4.337944182753563, validation loss: 4.407732701301574, correct 92.7%\n",
      "Epoch 3640, training loss: 4.3379386305809025, validation loss: 4.407763481140137, correct 92.7%\n",
      "Epoch 3645, training loss: 4.337933087348938, validation loss: 4.4077942848205565, correct 92.7%\n",
      "Epoch 3650, training loss: 4.337927493453026, validation loss: 4.407824921607971, correct 92.7%\n",
      "Epoch 3655, training loss: 4.337921911478043, validation loss: 4.407856416702271, correct 92.7%\n",
      "Epoch 3660, training loss: 4.337916451692581, validation loss: 4.407886958122253, correct 92.7%\n",
      "Epoch 3665, training loss: 4.337910944223404, validation loss: 4.407917714118957, correct 92.7%\n",
      "Epoch 3670, training loss: 4.337905490398407, validation loss: 4.407948994636536, correct 92.7%\n",
      "Epoch 3675, training loss: 4.337900051474572, validation loss: 4.407980155944824, correct 92.7%\n",
      "Epoch 3680, training loss: 4.33789459168911, validation loss: 4.4080112934112545, correct 92.7%\n",
      "Epoch 3685, training loss: 4.33788910806179, validation loss: 4.408042907714844, correct 92.7%\n",
      "Epoch 3690, training loss: 4.337883630394936, validation loss: 4.408073115348816, correct 92.7%\n",
      "Epoch 3695, training loss: 4.33787821829319, validation loss: 4.408103990554809, correct 92.7%\n",
      "Epoch 3700, training loss: 4.337873023748398, validation loss: 4.408135342597961, correct 92.7%\n",
      "Epoch 3705, training loss: 4.337867492437363, validation loss: 4.408166694641113, correct 92.7%\n",
      "Epoch 3710, training loss: 4.337862229347229, validation loss: 4.408197927474975, correct 92.7%\n",
      "Epoch 3715, training loss: 4.337856784462929, validation loss: 4.4082283735275265, correct 92.7%\n",
      "Epoch 3720, training loss: 4.337851485610008, validation loss: 4.408259153366089, correct 92.7%\n",
      "Epoch 3725, training loss: 4.337846124172211, validation loss: 4.408290529251099, correct 92.7%\n",
      "Epoch 3730, training loss: 4.337840813398361, validation loss: 4.408321237564087, correct 92.7%\n",
      "Epoch 3735, training loss: 4.337835559248925, validation loss: 4.408351755142212, correct 92.7%\n",
      "Epoch 3740, training loss: 4.337830263376236, validation loss: 4.408382272720337, correct 92.7%\n",
      "Epoch 3745, training loss: 4.932189106941223, validation loss: 4.408413338661194, correct 92.7%\n",
      "Epoch 3750, training loss: 4.337025770545006, validation loss: 4.399248957633972, correct 92.7%\n",
      "Epoch 3755, training loss: 4.3369469285011295, validation loss: 4.403175044059753, correct 92.7%\n",
      "Epoch 3760, training loss: 4.336976724863052, validation loss: 4.4049560785293576, correct 92.7%\n",
      "Epoch 3765, training loss: 4.337035658955574, validation loss: 4.40588607788086, correct 92.7%\n",
      "Epoch 3770, training loss: 4.337107664346695, validation loss: 4.40645923614502, correct 92.7%\n",
      "Epoch 3775, training loss: 4.337185630202294, validation loss: 4.406890821456909, correct 92.7%\n",
      "Epoch 3780, training loss: 4.337264427542687, validation loss: 4.407278084754944, correct 92.7%\n",
      "Epoch 3785, training loss: 4.337339636683464, validation loss: 4.4076598405838014, correct 92.7%\n",
      "Epoch 3790, training loss: 4.337408357858658, validation loss: 4.408039665222168, correct 92.7%\n",
      "Epoch 3795, training loss: 4.337468588352204, validation loss: 4.408402824401856, correct 92.7%\n",
      "Epoch 3800, training loss: 4.337519633769989, validation loss: 4.408727645874023, correct 92.7%\n",
      "Epoch 3805, training loss: 4.337562105059623, validation loss: 4.408992743492126, correct 92.7%\n",
      "Epoch 3810, training loss: 4.337596732378006, validation loss: 4.409188890457154, correct 92.7%\n",
      "Epoch 3815, training loss: 4.337624222040176, validation loss: 4.409322762489319, correct 92.7%\n",
      "Epoch 3820, training loss: 4.337645435333252, validation loss: 4.409408044815064, correct 92.7%\n",
      "Epoch 3825, training loss: 4.337661409378052, validation loss: 4.409459590911865, correct 92.7%\n",
      "Epoch 3830, training loss: 4.337672817707062, validation loss: 4.40949227809906, correct 92.7%\n",
      "Epoch 3835, training loss: 4.337681224942207, validation loss: 4.409517240524292, correct 92.7%\n",
      "Epoch 3840, training loss: 4.337686994671822, validation loss: 4.409540033340454, correct 92.7%\n",
      "Epoch 3845, training loss: 4.337690907716751, validation loss: 4.4095639944076535, correct 92.7%\n",
      "Epoch 3850, training loss: 4.337693679332733, validation loss: 4.409589147567749, correct 92.7%\n",
      "Epoch 3855, training loss: 4.337695354223252, validation loss: 4.409617829322815, correct 92.7%\n",
      "Epoch 3860, training loss: 4.337696346640587, validation loss: 4.409648013114929, correct 92.7%\n",
      "Epoch 3865, training loss: 4.337696844339371, validation loss: 4.409680795669556, correct 92.7%\n",
      "Epoch 3870, training loss: 4.337696787714958, validation loss: 4.409715414047241, correct 92.7%\n",
      "Epoch 3875, training loss: 4.337696459889412, validation loss: 4.409751319885254, correct 92.7%\n",
      "Epoch 3880, training loss: 4.337695935368538, validation loss: 4.409790015220642, correct 92.7%\n",
      "Epoch 3885, training loss: 4.337694865465164, validation loss: 4.409828996658325, correct 92.7%\n",
      "Epoch 3890, training loss: 4.33769343495369, validation loss: 4.409868764877319, correct 92.7%\n",
      "Epoch 3895, training loss: 4.337691375613213, validation loss: 4.409906697273255, correct 92.7%\n",
      "Epoch 3900, training loss: 4.337688905000687, validation loss: 4.409943056106568, correct 92.7%\n",
      "Epoch 3905, training loss: 4.337685897946358, validation loss: 4.409976005554199, correct 92.7%\n",
      "Epoch 3910, training loss: 4.337682375311852, validation loss: 4.410007739067078, correct 92.7%\n",
      "Epoch 3915, training loss: 4.337678670883179, validation loss: 4.410036230087281, correct 92.7%\n",
      "Epoch 3920, training loss: 4.337674486637115, validation loss: 4.410064172744751, correct 92.7%\n",
      "Epoch 3925, training loss: 4.337670266628265, validation loss: 4.410090780258178, correct 92.7%\n",
      "Epoch 3930, training loss: 4.33766559958458, validation loss: 4.410115885734558, correct 92.7%\n",
      "Epoch 3935, training loss: 4.337660729885101, validation loss: 4.410141253471375, correct 92.7%\n",
      "Epoch 3940, training loss: 4.337655609846115, validation loss: 4.410165071487427, correct 92.7%\n",
      "Epoch 3945, training loss: 4.337650388479233, validation loss: 4.410187673568726, correct 92.7%\n",
      "Epoch 3950, training loss: 4.337645038962364, validation loss: 4.41021077632904, correct 92.7%\n",
      "Epoch 3955, training loss: 4.337639737129211, validation loss: 4.410233378410339, correct 92.7%\n",
      "Epoch 3960, training loss: 4.33763464987278, validation loss: 4.4102555274963375, correct 92.7%\n",
      "Epoch 3965, training loss: 4.337629675865173, validation loss: 4.410278081893921, correct 92.7%\n",
      "Epoch 3970, training loss: 4.33762446641922, validation loss: 4.410300445556641, correct 92.7%\n",
      "Epoch 3975, training loss: 4.337619349360466, validation loss: 4.410322809219361, correct 92.7%\n",
      "Epoch 3980, training loss: 4.337614187598229, validation loss: 4.410345602035522, correct 92.7%\n",
      "Epoch 3985, training loss: 4.3376090109348295, validation loss: 4.410368037223816, correct 92.7%\n",
      "Epoch 3990, training loss: 4.3376038581132885, validation loss: 4.4103902816772464, correct 92.7%\n",
      "Epoch 3995, training loss: 4.337598726153374, validation loss: 4.410413122177124, correct 92.7%\n",
      "Epoch 4000, training loss: 4.337593537569046, validation loss: 4.410435605049133, correct 92.7%\n",
      "Epoch 4005, training loss: 4.337588438391686, validation loss: 4.410457611083984, correct 92.7%\n",
      "Epoch 4010, training loss: 4.3375831753015515, validation loss: 4.410479998588562, correct 92.7%\n",
      "Epoch 4015, training loss: 4.337578079104423, validation loss: 4.410502624511719, correct 92.7%\n",
      "Epoch 4020, training loss: 4.337572839856148, validation loss: 4.410525321960449, correct 92.7%\n",
      "Epoch 4025, training loss: 4.3375677287578585, validation loss: 4.410547876358033, correct 92.7%\n",
      "Epoch 4030, training loss: 4.337562450766564, validation loss: 4.4105712890625, correct 92.7%\n",
      "Epoch 4035, training loss: 4.337557315826416, validation loss: 4.410593891143799, correct 92.7%\n",
      "Epoch 4040, training loss: 4.337552270293235, validation loss: 4.4106158971786495, correct 92.7%\n",
      "Epoch 4045, training loss: 4.337547188997268, validation loss: 4.410638761520386, correct 92.7%\n",
      "Epoch 4050, training loss: 4.337542101740837, validation loss: 4.410661077499389, correct 92.7%\n",
      "Epoch 4055, training loss: 4.337536782026291, validation loss: 4.410683441162109, correct 92.7%\n",
      "Epoch 4060, training loss: 4.337531772255898, validation loss: 4.410705661773681, correct 92.7%\n",
      "Epoch 4065, training loss: 4.337526661157608, validation loss: 4.4107284307479855, correct 92.7%\n",
      "Epoch 4070, training loss: 4.337521576881409, validation loss: 4.410751247406006, correct 92.7%\n",
      "Epoch 4075, training loss: 4.337516435980797, validation loss: 4.410773086547851, correct 92.7%\n",
      "Epoch 4080, training loss: 4.337511315941811, validation loss: 4.410796689987182, correct 92.7%\n",
      "Epoch 4085, training loss: 4.33750633597374, validation loss: 4.4108196020126345, correct 92.7%\n",
      "Epoch 4090, training loss: 4.337501201033592, validation loss: 4.410842084884644, correct 92.7%\n",
      "Epoch 4095, training loss: 4.337496078014373, validation loss: 4.410864448547363, correct 92.7%\n",
      "Epoch 4100, training loss: 4.337491083145141, validation loss: 4.410887002944946, correct 92.7%\n",
      "Epoch 4105, training loss: 4.337485906481743, validation loss: 4.410909104347229, correct 92.7%\n",
      "Epoch 4110, training loss: 4.337480902671814, validation loss: 4.410931992530823, correct 92.7%\n",
      "Epoch 4115, training loss: 4.337475723028183, validation loss: 4.410954213142395, correct 92.7%\n",
      "Epoch 4120, training loss: 4.337470725178719, validation loss: 4.410976839065552, correct 92.7%\n",
      "Epoch 4125, training loss: 4.337465661764145, validation loss: 4.410999393463134, correct 92.7%\n",
      "Epoch 4130, training loss: 4.337460640072822, validation loss: 4.411021733283997, correct 92.7%\n",
      "Epoch 4135, training loss: 4.337455585598946, validation loss: 4.4110435247421265, correct 92.7%\n",
      "Epoch 4140, training loss: 4.337450584769249, validation loss: 4.411065149307251, correct 92.7%\n",
      "Epoch 4145, training loss: 4.3374455779790875, validation loss: 4.411087846755981, correct 92.7%\n",
      "Epoch 4150, training loss: 4.337440636754036, validation loss: 4.411110162734985, correct 92.7%\n",
      "Epoch 4155, training loss: 4.337435510754585, validation loss: 4.411131954193115, correct 92.7%\n",
      "Epoch 4160, training loss: 4.337430655956268, validation loss: 4.411154246330261, correct 92.7%\n",
      "Epoch 4165, training loss: 4.337425577640533, validation loss: 4.411176538467407, correct 92.7%\n",
      "Epoch 4170, training loss: 4.337420666217804, validation loss: 4.411197972297669, correct 92.7%\n",
      "Epoch 4175, training loss: 4.337415638566017, validation loss: 4.41121985912323, correct 92.7%\n",
      "Epoch 4180, training loss: 4.337410640716553, validation loss: 4.411241364479065, correct 92.7%\n",
      "Epoch 4185, training loss: 4.337405705451966, validation loss: 4.4112629175186155, correct 92.7%\n",
      "Epoch 4190, training loss: 4.3374007642269135, validation loss: 4.411285543441773, correct 92.7%\n",
      "Epoch 4195, training loss: 4.337395748496055, validation loss: 4.4113068103790285, correct 92.7%\n",
      "Epoch 4200, training loss: 4.3373908519744875, validation loss: 4.411329078674316, correct 92.7%\n",
      "Epoch 4205, training loss: 4.337385788559914, validation loss: 4.411350393295288, correct 92.7%\n",
      "Epoch 4210, training loss: 4.337380886077881, validation loss: 4.411371564865112, correct 92.7%\n",
      "Epoch 4215, training loss: 4.3373759329319, validation loss: 4.411393404006958, correct 92.7%\n",
      "Epoch 4220, training loss: 4.337370970845223, validation loss: 4.411413955688476, correct 92.7%\n",
      "Epoch 4225, training loss: 4.337366065382957, validation loss: 4.41143491268158, correct 92.7%\n",
      "Epoch 4230, training loss: 4.337361064553261, validation loss: 4.411455392837524, correct 92.7%\n",
      "Epoch 4235, training loss: 4.337356072664261, validation loss: 4.4114750623703, correct 92.7%\n",
      "Epoch 4240, training loss: 4.33735103905201, validation loss: 4.411495923995972, correct 92.7%\n",
      "Epoch 4245, training loss: 4.3373461097478865, validation loss: 4.411516237258911, correct 92.7%\n",
      "Epoch 4250, training loss: 4.337341246008873, validation loss: 4.41153621673584, correct 92.7%\n",
      "Epoch 4255, training loss: 4.337336182594299, validation loss: 4.411556673049927, correct 92.7%\n",
      "Epoch 4260, training loss: 4.33733129799366, validation loss: 4.411576652526856, correct 92.7%\n",
      "Epoch 4265, training loss: 4.33732622563839, validation loss: 4.411596488952637, correct 92.7%\n",
      "Epoch 4270, training loss: 4.337321263551712, validation loss: 4.411616373062134, correct 92.7%\n",
      "Epoch 4275, training loss: 4.337316301465035, validation loss: 4.41163592338562, correct 92.7%\n",
      "Epoch 4280, training loss: 4.337311202287674, validation loss: 4.411654615402222, correct 92.7%\n",
      "Epoch 4285, training loss: 4.33730608522892, validation loss: 4.411673974990845, correct 92.7%\n",
      "Epoch 4290, training loss: 4.3373010635375975, validation loss: 4.4116918563842775, correct 92.7%\n",
      "Epoch 4295, training loss: 4.3372959196567535, validation loss: 4.411710786819458, correct 92.7%\n",
      "Epoch 4300, training loss: 4.337290781736374, validation loss: 4.411728954315185, correct 92.7%\n",
      "Epoch 4305, training loss: 4.337285563349724, validation loss: 4.411746597290039, correct 92.7%\n",
      "Epoch 4310, training loss: 4.337280470132828, validation loss: 4.411764192581177, correct 92.7%\n",
      "Epoch 4315, training loss: 4.337275147438049, validation loss: 4.411781740188599, correct 92.7%\n",
      "Epoch 4320, training loss: 4.337269961833954, validation loss: 4.411798429489136, correct 92.7%\n",
      "Epoch 4325, training loss: 4.337264689803123, validation loss: 4.411815333366394, correct 92.7%\n",
      "Epoch 4330, training loss: 4.3372594058513645, validation loss: 4.411832857131958, correct 92.7%\n",
      "Epoch 4335, training loss: 4.337254109978676, validation loss: 4.411849117279052, correct 92.7%\n",
      "Epoch 4340, training loss: 4.337248599529266, validation loss: 4.411864805221557, correct 92.7%\n",
      "Epoch 4345, training loss: 4.337243151664734, validation loss: 4.411880326271057, correct 92.7%\n",
      "Epoch 4350, training loss: 4.337237527966499, validation loss: 4.411895847320556, correct 92.7%\n",
      "Epoch 4355, training loss: 4.337231528759003, validation loss: 4.411911392211914, correct 92.7%\n",
      "Epoch 4360, training loss: 4.337225925922394, validation loss: 4.411926698684693, correct 92.7%\n",
      "Epoch 4365, training loss: 4.337220168113708, validation loss: 4.411942172050476, correct 92.7%\n",
      "Epoch 4370, training loss: 4.337214383482933, validation loss: 4.411956453323365, correct 92.7%\n",
      "Epoch 4375, training loss: 4.337208506464958, validation loss: 4.411970257759094, correct 92.7%\n",
      "Epoch 4380, training loss: 4.3372025936841965, validation loss: 4.411983513832093, correct 92.7%\n",
      "Epoch 4385, training loss: 4.337196695804596, validation loss: 4.411995792388916, correct 92.7%\n",
      "Epoch 4390, training loss: 4.337190762162209, validation loss: 4.4120087146759035, correct 92.7%\n",
      "Epoch 4395, training loss: 4.337184807658195, validation loss: 4.412021040916443, correct 92.7%\n",
      "Epoch 4400, training loss: 4.337178742885589, validation loss: 4.412033271789551, correct 92.7%\n",
      "Epoch 4405, training loss: 4.3371726900339125, validation loss: 4.412045621871949, correct 92.7%\n",
      "Epoch 4410, training loss: 4.337166658043861, validation loss: 4.412057375907898, correct 92.7%\n",
      "Epoch 4415, training loss: 4.337160518765449, validation loss: 4.412068438529968, correct 92.7%\n",
      "Epoch 4420, training loss: 4.33715443611145, validation loss: 4.4120800495147705, correct 92.7%\n",
      "Epoch 4425, training loss: 4.337148284912109, validation loss: 4.412091779708862, correct 92.7%\n",
      "Epoch 4430, training loss: 4.337142273783684, validation loss: 4.412102198600769, correct 92.7%\n",
      "Epoch 4435, training loss: 4.337136068940163, validation loss: 4.412113857269287, correct 92.7%\n",
      "Epoch 4440, training loss: 4.337129828333855, validation loss: 4.4121258020401, correct 92.7%\n",
      "Epoch 4445, training loss: 4.3371237993240355, validation loss: 4.412136340141297, correct 92.7%\n",
      "Epoch 4450, training loss: 4.337117755413056, validation loss: 4.412146806716919, correct 92.7%\n",
      "Epoch 4455, training loss: 4.337111672759056, validation loss: 4.412158274650574, correct 92.7%\n",
      "Epoch 4460, training loss: 4.337105637788772, validation loss: 4.412170052528381, correct 92.7%\n",
      "Epoch 4465, training loss: 4.337099495530128, validation loss: 4.412181448936463, correct 92.7%\n",
      "Epoch 4470, training loss: 4.337093609571457, validation loss: 4.412192058563233, correct 92.7%\n",
      "Epoch 4475, training loss: 4.337087589502334, validation loss: 4.412204098701477, correct 92.7%\n",
      "Epoch 4480, training loss: 4.337081634998322, validation loss: 4.412216091156006, correct 92.7%\n",
      "Epoch 4485, training loss: 4.337075787782669, validation loss: 4.412228298187256, correct 92.7%\n",
      "Epoch 4490, training loss: 4.337069982290268, validation loss: 4.412240600585937, correct 92.7%\n",
      "Epoch 4495, training loss: 4.337064111232758, validation loss: 4.412252569198609, correct 92.7%\n",
      "Epoch 4500, training loss: 4.337058275938034, validation loss: 4.4122649192810055, correct 92.7%\n",
      "Epoch 4505, training loss: 4.337052589654922, validation loss: 4.412277746200561, correct 92.7%\n",
      "Epoch 4510, training loss: 4.337046945095063, validation loss: 4.412290000915528, correct 92.7%\n",
      "Epoch 4515, training loss: 4.337041184306145, validation loss: 4.412303161621094, correct 92.7%\n",
      "Epoch 4520, training loss: 4.3370354801416395, validation loss: 4.412315940856933, correct 92.7%\n",
      "Epoch 4525, training loss: 4.337029832601547, validation loss: 4.412329626083374, correct 92.7%\n",
      "Epoch 4530, training loss: 4.337024304270744, validation loss: 4.412342810630799, correct 92.7%\n",
      "Epoch 4535, training loss: 4.337018713355064, validation loss: 4.412356662750244, correct 92.7%\n",
      "Epoch 4540, training loss: 4.337013158202171, validation loss: 4.412370824813843, correct 92.7%\n",
      "Epoch 4545, training loss: 4.337007617950439, validation loss: 4.412383270263672, correct 92.7%\n",
      "Epoch 4550, training loss: 4.3370021134614944, validation loss: 4.412397289276123, correct 92.7%\n",
      "Epoch 4555, training loss: 4.336996704339981, validation loss: 4.412410807609558, correct 92.7%\n",
      "Epoch 4560, training loss: 4.336991226673126, validation loss: 4.412424802780151, correct 92.7%\n",
      "Epoch 4565, training loss: 4.336985728144645, validation loss: 4.412438249588012, correct 92.7%\n",
      "Epoch 4570, training loss: 4.336980253458023, validation loss: 4.412451505661011, correct 92.7%\n",
      "Epoch 4575, training loss: 4.336974778771401, validation loss: 4.41246531009674, correct 92.7%\n",
      "Epoch 4580, training loss: 4.336969405412674, validation loss: 4.412478351593018, correct 92.7%\n",
      "Epoch 4585, training loss: 4.3369639158248905, validation loss: 4.412492132186889, correct 92.7%\n",
      "Epoch 4590, training loss: 4.336958569288254, validation loss: 4.4125049114227295, correct 92.7%\n",
      "Epoch 4595, training loss: 4.3369531571865085, validation loss: 4.412518572807312, correct 92.7%\n",
      "Epoch 4600, training loss: 4.336947748064995, validation loss: 4.4125309705734255, correct 92.7%\n",
      "Epoch 4605, training loss: 4.336942321062088, validation loss: 4.41254403591156, correct 92.7%\n",
      "Epoch 4610, training loss: 4.336937096714974, validation loss: 4.412556791305542, correct 92.7%\n",
      "Epoch 4615, training loss: 4.336931791901589, validation loss: 4.412569952011109, correct 92.7%\n",
      "Epoch 4620, training loss: 4.336926358938217, validation loss: 4.412582015991211, correct 92.7%\n",
      "Epoch 4625, training loss: 4.336921161413192, validation loss: 4.412594938278199, correct 92.7%\n",
      "Epoch 4630, training loss: 4.33691576719284, validation loss: 4.412608242034912, correct 92.7%\n",
      "Epoch 4635, training loss: 4.336910471320152, validation loss: 4.41262104511261, correct 92.7%\n",
      "Epoch 4640, training loss: 4.336905089020729, validation loss: 4.412632989883423, correct 92.7%\n",
      "Epoch 4645, training loss: 4.33689975142479, validation loss: 4.412645936012268, correct 92.7%\n",
      "Epoch 4650, training loss: 4.336894455552101, validation loss: 4.412658762931824, correct 92.7%\n",
      "Epoch 4655, training loss: 4.336889228224754, validation loss: 4.412671327590942, correct 92.7%\n",
      "Epoch 4660, training loss: 4.336883714795112, validation loss: 4.412683320045471, correct 92.7%\n",
      "Epoch 4665, training loss: 4.336878472566605, validation loss: 4.412697267532349, correct 92.7%\n",
      "Epoch 4670, training loss: 4.336873060464859, validation loss: 4.412710380554199, correct 92.7%\n",
      "Epoch 4675, training loss: 4.336867919564247, validation loss: 4.412723803520203, correct 92.7%\n",
      "Epoch 4680, training loss: 4.336862498521805, validation loss: 4.412736821174621, correct 92.7%\n",
      "Epoch 4685, training loss: 4.336857253313065, validation loss: 4.41275053024292, correct 92.7%\n",
      "Epoch 4690, training loss: 4.336851981282234, validation loss: 4.412764716148376, correct 92.7%\n",
      "Epoch 4695, training loss: 4.3368467628955845, validation loss: 4.412778663635254, correct 92.7%\n",
      "Epoch 4700, training loss: 4.336841505765915, validation loss: 4.412792992591858, correct 92.7%\n",
      "Epoch 4705, training loss: 4.336836236715317, validation loss: 4.412807488441468, correct 92.7%\n",
      "Epoch 4710, training loss: 4.33683095574379, validation loss: 4.412822270393372, correct 92.7%\n",
      "Epoch 4715, training loss: 4.336825871467591, validation loss: 4.4128368377685545, correct 92.7%\n",
      "Epoch 4720, training loss: 4.336820644140244, validation loss: 4.412852334976196, correct 92.7%\n",
      "Epoch 4725, training loss: 4.336815598607063, validation loss: 4.412867188453674, correct 92.7%\n",
      "Epoch 4730, training loss: 4.336810368299484, validation loss: 4.412882947921753, correct 92.7%\n",
      "Epoch 4735, training loss: 4.3368052542209625, validation loss: 4.412898063659668, correct 92.7%\n",
      "Epoch 4740, training loss: 4.336800277233124, validation loss: 4.412912797927857, correct 92.7%\n",
      "Epoch 4745, training loss: 4.3367951840162275, validation loss: 4.412928414344788, correct 92.7%\n",
      "Epoch 4750, training loss: 4.336790177226066, validation loss: 4.412943291664123, correct 92.7%\n",
      "Epoch 4755, training loss: 4.33678514957428, validation loss: 4.412958741188049, correct 92.7%\n",
      "Epoch 4760, training loss: 4.3367801904678345, validation loss: 4.412974238395691, correct 92.7%\n",
      "Epoch 4765, training loss: 4.336775317788124, validation loss: 4.412990617752075, correct 92.7%\n",
      "Epoch 4770, training loss: 4.336770465970039, validation loss: 4.413005399703979, correct 92.7%\n",
      "Epoch 4775, training loss: 4.336765542626381, validation loss: 4.413020992279053, correct 92.7%\n",
      "Epoch 4780, training loss: 4.336760702729225, validation loss: 4.4130363941192625, correct 92.7%\n",
      "Epoch 4785, training loss: 4.336756008863449, validation loss: 4.413052725791931, correct 92.7%\n",
      "Epoch 4790, training loss: 4.336751183867454, validation loss: 4.413067722320557, correct 92.7%\n",
      "Epoch 4795, training loss: 4.336746349930763, validation loss: 4.41308274269104, correct 92.7%\n",
      "Epoch 4800, training loss: 4.3367416024208065, validation loss: 4.41309814453125, correct 92.7%\n",
      "Epoch 4805, training loss: 4.3367370694875715, validation loss: 4.413113832473755, correct 92.7%\n",
      "Epoch 4810, training loss: 4.336732414364815, validation loss: 4.413130092620849, correct 92.7%\n",
      "Epoch 4815, training loss: 4.336727663874626, validation loss: 4.413145685195923, correct 92.7%\n",
      "Epoch 4820, training loss: 4.33672300875187, validation loss: 4.4131614685058596, correct 92.7%\n",
      "Epoch 4825, training loss: 4.3367183148860935, validation loss: 4.413176822662353, correct 92.7%\n",
      "Epoch 4830, training loss: 4.336713781952858, validation loss: 4.413191843032837, correct 92.7%\n",
      "Epoch 4835, training loss: 4.336709105968476, validation loss: 4.413207459449768, correct 92.7%\n",
      "Epoch 4840, training loss: 4.336704540252685, validation loss: 4.413222980499268, correct 92.7%\n",
      "Epoch 4845, training loss: 4.3367001712322235, validation loss: 4.4132387161254885, correct 92.7%\n",
      "Epoch 4850, training loss: 4.336695513129234, validation loss: 4.413253974914551, correct 92.7%\n",
      "Epoch 4855, training loss: 4.33669094145298, validation loss: 4.413270401954651, correct 92.7%\n",
      "Epoch 4860, training loss: 4.336686325073242, validation loss: 4.413286018371582, correct 92.7%\n",
      "Epoch 4865, training loss: 4.336681878566742, validation loss: 4.413301801681518, correct 92.7%\n",
      "Epoch 4870, training loss: 4.336677458882332, validation loss: 4.413318347930908, correct 92.7%\n",
      "Epoch 4875, training loss: 4.336673051118851, validation loss: 4.413334345817566, correct 92.7%\n",
      "Epoch 4880, training loss: 4.336668568849563, validation loss: 4.413350534439087, correct 92.7%\n",
      "Epoch 4885, training loss: 4.336664021015167, validation loss: 4.4133668661117555, correct 92.7%\n",
      "Epoch 4890, training loss: 4.336659619212151, validation loss: 4.413383150100708, correct 92.7%\n",
      "Epoch 4895, training loss: 4.336655443906784, validation loss: 4.413400030136108, correct 92.7%\n",
      "Epoch 4900, training loss: 4.3366508185863495, validation loss: 4.413416600227356, correct 92.7%\n",
      "Epoch 4905, training loss: 4.336646455526352, validation loss: 4.413433408737182, correct 92.7%\n",
      "Epoch 4910, training loss: 4.336642235517502, validation loss: 4.413450717926025, correct 92.7%\n",
      "Epoch 4915, training loss: 4.3366379737854, validation loss: 4.41346755027771, correct 92.7%\n",
      "Epoch 4920, training loss: 4.33663370013237, validation loss: 4.413484215736389, correct 92.7%\n",
      "Epoch 4925, training loss: 4.3366296261549, validation loss: 4.413500547409058, correct 92.7%\n",
      "Epoch 4930, training loss: 4.336626952886581, validation loss: 4.413515996932984, correct 92.7%\n",
      "Epoch 4935, training loss: 4.336621108651161, validation loss: 4.413532328605652, correct 92.7%\n",
      "Epoch 4940, training loss: 4.336617043614387, validation loss: 4.413547873497009, correct 92.7%\n",
      "Epoch 4945, training loss: 4.336612808704376, validation loss: 4.413563990592957, correct 92.7%\n",
      "Epoch 4950, training loss: 4.3366087675094604, validation loss: 4.413579273223877, correct 92.7%\n",
      "Epoch 4955, training loss: 4.336604955792427, validation loss: 4.4135956287384035, correct 92.7%\n",
      "Epoch 4960, training loss: 4.336600553989411, validation loss: 4.413610982894897, correct 92.7%\n",
      "Epoch 4965, training loss: 4.336596468091011, validation loss: 4.413625860214234, correct 92.7%\n",
      "Epoch 4970, training loss: 4.336592942476273, validation loss: 4.413642382621765, correct 92.7%\n",
      "Epoch 4975, training loss: 4.3365890204906465, validation loss: 4.413657832145691, correct 92.7%\n",
      "Epoch 4980, training loss: 4.336584663391113, validation loss: 4.413673019409179, correct 92.7%\n",
      "Epoch 4985, training loss: 4.336580148339271, validation loss: 4.413687777519226, correct 92.7%\n",
      "Epoch 4990, training loss: 4.336576014757156, validation loss: 4.413703107833863, correct 92.7%\n",
      "Epoch 4995, training loss: 4.336572274565697, validation loss: 4.413717532157898, correct 92.7%\n"
     ]
    }
   ],
   "source": [
    "# Hyper-parameters\n",
    "num_epochs = 5000\n",
    "min_LSTM_loss = 10000\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# For each epoch\n",
    "m = nn.Sigmoid()\n",
    "\n",
    "model.train()\n",
    "for i in range(num_epochs):\n",
    "    # Track loss\n",
    "    epoch_training_loss = 0\n",
    "    epoch_validation_loss = 0\n",
    "    correct = total = 0\n",
    "\n",
    "    # For each sentence in validation set\n",
    "    for inputs, targets in validation_set:\n",
    "\n",
    "        # One-hot encode input and target sequence\n",
    "        inputs_one_hot = one_hot_encode_sequence(inputs, vocab_size, char_to_idx)\n",
    "\n",
    "        # Convert input to tensor\n",
    "        inputs_one_hot = torch.Tensor(inputs_one_hot).to(device)\n",
    "        inputs_one_hot = inputs_one_hot.permute(0, 2, 1)\n",
    "\n",
    "        # Convert target to tensor\n",
    "        one_hot_targets = one_hot_encode_sequence(targets, vocab_size, char_to_idx)\n",
    "        one_hot_targets = torch.FloatTensor(one_hot_targets).view(-1, vocab_size).to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model.forward(inputs_one_hot)\n",
    "                      \n",
    "        guesses = torch.argmax(outputs, axis=1)\n",
    "        targets_idx = [char_to_idx[char] for char in targets]\n",
    "        correct += torch.sum((guesses == torch.LongTensor(targets_idx).to(device)))\n",
    "        total += len(targets_idx)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(m(outputs), one_hot_targets)\n",
    "\n",
    "        # Update loss\n",
    "        epoch_validation_loss += loss.detach().cpu().numpy()\n",
    "\n",
    "\n",
    "    if epoch_validation_loss < min_LSTM_loss:\n",
    "        print(f\"Best {epoch_validation_loss}\")\n",
    "        torch.save(model, 'best_model.pt')\n",
    "        min_LSTM_loss=epoch_validation_loss\n",
    "\n",
    "\n",
    "    # For each sentence in training set\n",
    "    for inputs, targets in training_set:\n",
    "\n",
    "        # One-hot encode input and target sequence\n",
    "        inputs_one_hot = one_hot_encode_sequence(inputs, vocab_size, char_to_idx)\n",
    "\n",
    "        # Convert input to tensor\n",
    "        inputs_one_hot = torch.Tensor(inputs_one_hot).to(device)\n",
    "        inputs_one_hot = inputs_one_hot.permute(0, 2, 1)\n",
    "\n",
    "        # Convert target to tensor\n",
    "        one_hot_targets = one_hot_encode_sequence(targets, vocab_size, char_to_idx)\n",
    "        one_hot_targets = torch.FloatTensor(one_hot_targets).view(-1, vocab_size).to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model.forward(inputs_one_hot)\n",
    "\n",
    "        # Reset gradients\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(m(outputs), one_hot_targets)\n",
    "\n",
    "        # Update weights\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update learning rate (advanced technique, can be ignored)\n",
    "#        scheduler.step()\n",
    "\n",
    "        # Update loss\n",
    "        epoch_training_loss += loss.detach().cpu().numpy()\n",
    "        \n",
    "    # Save loss for plot\n",
    "    training_loss.append(epoch_training_loss / len(training_set))\n",
    "    validation_loss.append(epoch_validation_loss / len(validation_set))\n",
    "\n",
    "    # Print loss every 5 epochs\n",
    "    if i % 5 == 0:\n",
    "        print(f'Epoch {i:2d}, training loss: {training_loss[-1]}, validation loss: {validation_loss[-1]}, correct {100*correct/total:.1f}%')\n",
    "\n",
    "model.eval()\n",
    "# Plot training and validation loss\n",
    "epoch = np.arange(len(training_loss))\n",
    "\n",
    "# Get first sentence in training set\n",
    "inputs, targets = training_set[0]\n",
    "\n",
    "# One-hot encode input and target sequence\n",
    "inputs_one_hot = one_hot_encode_sequence(inputs, vocab_size, char_to_idx)\n",
    "targets_idx = [char_to_idx[char] for char in targets]\n",
    "\n",
    "# Convert input to tensor\n",
    "inputs_one_hot = torch.Tensor(inputs_one_hot)\n",
    "inputs_one_hot = inputs_one_hot.permute(0, 2, 1)\n",
    "\n",
    "# Convert target to tensor\n",
    "targets_idx = torch.LongTensor(targets_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the network\n",
    "**Exercise:** Finish the training code until you have at least 25% accuracy. If you're willing to train for a long time, feel free to use more data and bigger models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-19T13:00:44.919471300Z",
     "start_time": "2024-02-19T13:00:44.910471800Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sequence....: ['a', 'a', 'a', 'a', 'b', 'b', 'b', 'b']\n",
      "Target sequence...: ['a', 'a', 'a', 'b', 'b', 'b', 'b', 'EOS']\n",
      "Predicted sequence: ['a', 'a', 'a', 'a', 'b', 'b', 'b', 'EOS']\n"
     ]
    }
   ],
   "source": [
    "# Forward pass\n",
    "best_model = LSTM()\n",
    "best_model = torch.load('best_model.pt')\n",
    "best_model.cpu()\n",
    "outputs = best_model.forward(inputs_one_hot.cpu())\n",
    "print('Input sequence....:', inputs)\n",
    "print('Target sequence...:', targets)\n",
    "preds = [idx_to_char[int(torch.argmax(output))] for output in outputs]\n",
    "print('Predicted sequence:', preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-19T13:00:45.030998700Z",
     "start_time": "2024-02-19T13:00:44.966471100Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "plt.figure()\n",
    "plt.plot(epoch, training_loss, 'r', label='Training loss',)\n",
    "plt.plot(epoch, validation_loss, 'b', label='Validation loss')\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch'), plt.ylabel('NLL')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-19T13:00:45.125528600Z",
     "start_time": "2024-02-19T13:00:45.035000100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sequence....: ['a', 'a', 'a', 'b', 'b', 'b']\n",
      "Target sequence...: ['a', 'a', 'b', 'b', 'b', 'EOS']\n",
      "Predicted sequence: ['a', 'a', 'a', 'b', 'b', 'EOS']\n",
      "Test accuracy is 83.3%.\n",
      "Input sequence....: ['a', 'a', 'a', 'a', 'a', 'b', 'b', 'b', 'b', 'b']\n",
      "Target sequence...: ['a', 'a', 'a', 'a', 'b', 'b', 'b', 'b', 'b', 'EOS']\n",
      "Predicted sequence: ['a', 'a', 'a', 'a', 'a', 'b', 'b', 'b', 'b', 'EOS']\n",
      "Test accuracy is 90.0%.\n",
      "Input sequence....: ['a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b']\n",
      "Target sequence...: ['a', 'a', 'a', 'a', 'a', 'a', 'a', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'EOS']\n",
      "Predicted sequence: ['a', 'a', 'a', 'a', 'a', 'a', 'a', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'EOS']\n",
      "Test accuracy is 100.0%.\n",
      "Input sequence....: ['a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b']\n",
      "Target sequence...: ['a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'EOS']\n",
      "Predicted sequence: ['a', 'a', 'a', 'a', 'a', 'a', 'a', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'EOS']\n",
      "Test accuracy is 94.4%.\n",
      "Input sequence....: ['a', 'a', 'a', 'a', 'a', 'a', 'a', 'b', 'b', 'b', 'b', 'b', 'b', 'b']\n",
      "Target sequence...: ['a', 'a', 'a', 'a', 'a', 'a', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'EOS']\n",
      "Predicted sequence: ['a', 'a', 'a', 'a', 'a', 'a', 'a', 'b', 'b', 'b', 'b', 'b', 'b', 'EOS']\n",
      "Test accuracy is 92.9%.\n",
      "Input sequence....: ['a', 'a', 'a', 'a', 'b', 'b', 'b', 'b']\n",
      "Target sequence...: ['a', 'a', 'a', 'b', 'b', 'b', 'b', 'EOS']\n",
      "Predicted sequence: ['a', 'a', 'a', 'a', 'b', 'b', 'b', 'EOS']\n",
      "Test accuracy is 87.5%.\n",
      "Input sequence....: ['a', 'a', 'a', 'a', 'b', 'b', 'b', 'b']\n",
      "Target sequence...: ['a', 'a', 'a', 'b', 'b', 'b', 'b', 'EOS']\n",
      "Predicted sequence: ['a', 'a', 'a', 'a', 'b', 'b', 'b', 'EOS']\n",
      "Test accuracy is 87.5%.\n",
      "Input sequence....: ['a', 'a', 'a', 'b', 'b', 'b']\n",
      "Target sequence...: ['a', 'a', 'b', 'b', 'b', 'EOS']\n",
      "Predicted sequence: ['a', 'a', 'a', 'b', 'b', 'EOS']\n",
      "Test accuracy is 83.3%.\n",
      "Input sequence....: ['a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b']\n",
      "Target sequence...: ['a', 'a', 'a', 'a', 'a', 'a', 'a', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'EOS']\n",
      "Predicted sequence: ['a', 'a', 'a', 'a', 'a', 'a', 'a', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'EOS']\n",
      "Test accuracy is 100.0%.\n",
      "Input sequence....: ['a', 'a', 'a', 'a', 'a', 'a', 'b', 'b', 'b', 'b', 'b', 'b']\n",
      "Target sequence...: ['a', 'a', 'a', 'a', 'a', 'b', 'b', 'b', 'b', 'b', 'b', 'EOS']\n",
      "Predicted sequence: ['a', 'a', 'a', 'a', 'a', 'a', 'b', 'b', 'b', 'b', 'b', 'EOS']\n",
      "Test accuracy is 91.7%.\n"
     ]
    }
   ],
   "source": [
    "best_model = LSTM()\n",
    "best_model = torch.load('best_model.pt')\n",
    "best_model.cpu()\n",
    "\n",
    "# Get first sentence in test set\n",
    "for test_input_sequence, test_target_sequence in test_set:\n",
    "\n",
    "    # One-hot encode input and target sequence\n",
    "    inputs_one_hot = one_hot_encode_sequence(test_input_sequence, vocab_size, char_to_idx)\n",
    "    targets_idx = [char_to_idx[char] for char in test_target_sequence]\n",
    "    \n",
    "    # Convert input to tensor\n",
    "    inputs_one_hot = torch.Tensor(inputs_one_hot)\n",
    "    inputs_one_hot = inputs_one_hot.permute(0, 2, 1)\n",
    "    \n",
    "    # Convert target to tensor\n",
    "    targets_idx = torch.LongTensor(targets_idx)\n",
    "    \n",
    "    # Forward pass\n",
    "    # TODO:\n",
    "    outputs = best_model.forward(inputs_one_hot)\n",
    "    \n",
    "    print('Input sequence....:', test_input_sequence)\n",
    "    print('Target sequence...:', test_target_sequence)\n",
    "    preds = [idx_to_char[int(torch.argmax(output))] for output in outputs]\n",
    "    print('Predicted sequence:', preds)\n",
    "    \n",
    "    accuracy = 0\n",
    "    for target, pred in zip(test_target_sequence, preds):\n",
    "        accuracy += target == pred\n",
    "    accuracy /= len(test_target_sequence)/100\n",
    "    \n",
    "    print(f\"Test accuracy is {accuracy:.1f}%.\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final comments (extracurricular)\n",
    "Now you've seen how RNNs work and implemented a very simple causal language model. If you want to improve it, besides grabbing more data and building bigger models, you could also improve on character-based tokens to n_grams or subwords; make your word embeddings (hidden state) contextual, upgrade your model to a transformer or a state space model, and suddenly you've designed GPT, BERT, T5, LLaMA, or any other \"modern\" large language model. Though RNNs aren't the most popular due to the dominance of the transformer, models such as state space models, which exist inbetween RNNs and transformers, show that much innovation is still possible by merging these two frameworks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-02-17T22:44:49.989140200Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
